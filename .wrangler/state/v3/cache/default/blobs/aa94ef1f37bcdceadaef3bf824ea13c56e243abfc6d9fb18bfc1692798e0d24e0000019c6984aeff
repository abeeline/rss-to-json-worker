{"feed":{"items":[{"creator":"Manuel Olguín Muñoz","title":"Shedding old code with ecdysis: graceful restarts for Rust services at Cloudflare","link":"https://blog.cloudflare.com/ecdysis-rust-graceful-restarts/","pubDate":"Fri, 13 Feb 2026 14:00:00 GMT","content:encoded":" <blockquote><p>ecdysis | <i>ˈekdəsəs</i> |</p><p>noun</p><p>    the process of shedding the old skin (in reptiles) or casting off the outer \n    cuticle (in insects and other arthropods).  </p></blockquote><p>How do you upgrade a network service, handling millions of requests per second around the globe, without disrupting even a single connection?</p><p>One of our solutions at Cloudflare to this massive challenge has long been <a href=\"https://github.com/cloudflare/ecdysis\"><b><u>ecdysis</u></b></a>, a Rust library that implements graceful process restarts where no live connections are dropped, and no new connections are refused. </p><p>Last month, <b>we open-sourced ecdysis</b>, so now anyone can use it. After five years of production use at Cloudflare, ecdysis has proven itself by enabling zero-downtime upgrades across our critical Rust infrastructure, saving millions of requests with every restart across Cloudflare’s <a href=\"https://www.cloudflare.com/network/\"><u>global network</u></a>.</p><p>It’s hard to overstate the importance of getting these upgrades right, especially at the scale of Cloudflare’s network. Many of our services perform critical tasks such as traffic routing, <a href=\"https://www.cloudflare.com/application-services/solutions/certificate-lifecycle-management/\"><u>TLS lifecycle management</u></a>, or firewall rules enforcement, and must operate continuously. If one of these services goes down, even for an instant, the cascading impact can be catastrophic. Dropped connections and failed requests quickly lead to degraded customer performance and business impact.</p><p>When these services need updates, security patches can’t wait. Bug fixes need deployment and new features must roll out. </p><p>The naive approach involves waiting for the old process to be stopped before spinning up the new one, but this creates a window of time where connections are refused and requests are dropped. For a service handling thousands of requests per second in a single location, multiply that across hundreds of data centers, and a brief restart becomes millions of failed requests globally.</p><p>Let’s dig into the problem, and how ecdysis has been the solution for us — and maybe will be for you. </p><p><b>Links</b>: <a href=\"https://github.com/cloudflare/ecdysis\">GitHub</a> <b>|</b> <a href=\"https://crates.io/crates/ecdysis\">crates.io</a> <b>|</b> <a href=\"https://docs.rs/ecdysis\">docs.rs</a></p>\n    <div>\n      <h3>Why graceful restarts are hard</h3>\n      <a href=\"#why-graceful-restarts-are-hard\">\n        \n      </a>\n    </div>\n    <p>The naive approach to restarting a service, as we mentioned, is to stop the old process and start a new one. This works acceptably for simple services that don’t handle real-time requests, but for network services processing live connections, this approach has critical limitations.</p><p>First, the naive approach creates a window during which no process is listening for incoming connections. When the old process stops, it closes its listening sockets, which causes the OS to immediately refuse new connections with <code>ECONNREFUSED</code>. Even if the new process starts immediately, there will always be a gap where nothing is accepting connections, whether milliseconds or seconds. For a service handling thousands of requests per second, even a gap of 100ms means hundreds of dropped connections.</p><p>Second, stopping the old process kills all already-established connections. A client uploading a large file or streaming video gets abruptly disconnected. Long-lived connections like WebSockets or gRPC streams are terminated mid-operation. From the client’s perspective, the service simply vanishes.</p><p>Binding the new process before shutting down the old one appears to solve this, but also introduces additional issues. The kernel normally allows only one process to bind to an address:port combination, but <a href=\"https://man7.org/linux/man-pages/man7/socket.7.html\"><u>the SO_REUSEPORT socket option</u></a> permits multiple binds. However, this creates a problem during process transitions that makes it unsuitable for graceful restarts.</p><p>When <code>SO_REUSEPORT</code> is used, the kernel creates separate listening sockets for each process and <a href=\"https://lwn.net/Articles/542629/\"><u>load balances new connections across these sockets</u></a>. When the initial <code>SYN</code> packet for a connection is received, the kernel will assign it to one of the listening processes. Once the initial handshake is completed, the connection then sits in the <code>accept()</code> queue of the process until the process accepts it. If the process then exits before accepting this connection, it becomes orphaned and is terminated by the kernel. GitHub’s engineering team documented this issue extensively when <a href=\"https://github.blog/2020-10-07-glb-director-zero-downtime-load-balancer-updates/\"><u>building their GLB Director load balancer</u></a>.</p>\n    <div>\n      <h3>How ecdysis works</h3>\n      <a href=\"#how-ecdysis-works\">\n        \n      </a>\n    </div>\n    <p>When we set out to design and build ecdysis, we identified four key goals for the library:</p><ol><li><p><b>Old code can be completely shut down</b> post-upgrade.</p></li><li><p><b>The new process has a grace period</b> for initialization.</p></li><li><p><b>New code crashing during initialization is acceptable</b> and shouldn’t affect the running service.</p></li><li><p><b>Only a single upgrade runs in parallel</b> to avoid cascading failures.</p></li></ol><p>ecdysis satisfies these requirements following an approach pioneered by NGINX, which has supported graceful upgrades since its early days. The approach is straightforward: </p><ol><li><p>The parent process <code>fork()</code>s a new child process.</p></li><li><p>The child process replaces itself with a new version of the code with <code>execve()</code>.</p></li><li><p>The child process inherits the socket file descriptors via a named pipe shared with the parent.</p></li><li><p>The parent process waits for the child process to signal readiness before shutting down.</p></li></ol>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4QK8GY1s30C8RUovBQnqbD/525094478911eda96c7877a10753159f/image3.png\" />\n          </figure><p>Crucially, the socket remains open throughout the transition. The child process inherits the listening socket from the parent as a file descriptor shared via a named pipe. During the child's initialization, both processes share the same underlying kernel data structure, allowing the parent to continue accepting and processing new and existing connections. Once the child completes initialization, it notifies the parent and begins accepting connections. Upon receiving this ready notification, the parent immediately closes its copy of the listening socket and continues handling only existing connections. </p><p>This process eliminates coverage gaps while providing the child a safe initialization window. There is a brief window of time when both the parent and child may accept connections concurrently. This is intentional; any connections accepted by the parent are simply handled until completion as part of the draining process.</p><p>This model also provides the required crash safety. If the child process fails during initialization (e.g., due to a configuration error), it simply exits. Since the parent never stopped listening, no connections are dropped, and the upgrade can be retried once the problem is fixed.</p><p>ecdysis implements the forking model with first-class support for asynchronous programming through<a href=\"https://tokio.rs\"> <u>Tokio</u></a> and s<code>ystemd</code> integration:</p><ul><li><p><b>Tokio integration</b>: Native async stream wrappers for Tokio. Inherited sockets become listeners without additional glue code. For synchronous services, ecdysis supports operation without async runtime requirements.</p></li><li><p><b>systemd-notify support</b>: When the <code>systemd_notify</code> feature is enabled, ecdysis automatically integrates with systemd’s process lifecycle notifications. Setting <code>Type=notify-reload</code> in your service unit file allows systemd to track upgrades correctly.</p></li><li><p><b>systemd named sockets</b>: The <code>systemd_sockets</code> feature enables ecdysis to manage systemd-activated sockets. Your service can be socket-activated and support graceful restarts simultaneously.</p></li></ul><p>Platform note: ecdysis relies on Unix-specific syscalls for socket inheritance and process management. It does not work on Windows. This is a fundamental limitation of the forking approach.</p>\n    <div>\n      <h3>Security considerations</h3>\n      <a href=\"#security-considerations\">\n        \n      </a>\n    </div>\n    <p>Graceful restarts introduce security considerations. The forking model creates a brief window where two process generations coexist, both with access to the same listening sockets and potentially sensitive file descriptors.</p><p>ecdysis addresses these concerns through its design:</p><p><b>Fork-then-exec</b>: ecdysis follows the traditional Unix pattern of <code>fork()</code> followed immediately by <code>execve()</code>. This ensures the child process starts with a clean slate: new address space, fresh code, and no inherited memory. Only explicitly-passed file descriptors cross the boundary.</p><p><b>Explicit inheritance</b>: Only listening sockets and communication pipes are inherited. Other file descriptors are closed via <code>CLOEXEC</code> flags. This prevents accidental leakage of sensitive handles.</p><p><b>seccomp compatibility</b>: Services using seccomp filters must allow <code>fork()</code> and <code>execve()</code>. This is a tradeoff: graceful restarts require these syscalls, so they cannot be blocked.</p><p>For most network services, these tradeoffs are acceptable. The security of the fork-exec model is well understood and has been battle-tested for decades in software like NGINX and Apache.</p>\n    <div>\n      <h3>Code example</h3>\n      <a href=\"#code-example\">\n        \n      </a>\n    </div>\n    <p>Let’s look at a practical example. Here’s a simplified TCP echo server that supports graceful restarts:</p>\n            <pre><code>use ecdysis::tokio_ecdysis::{SignalKind, StopOnShutdown, TokioEcdysisBuilder};\nuse tokio::{net::TcpStream, task::JoinSet};\nuse futures::StreamExt;\nuse std::net::SocketAddr;\n\n#[tokio::main]\nasync fn main() {\n    // Create the ecdysis builder\n    let mut ecdysis_builder = TokioEcdysisBuilder::new(\n        SignalKind::hangup()  // Trigger upgrade/reload on SIGHUP\n    ).unwrap();\n\n    // Trigger stop on SIGUSR1\n    ecdysis_builder\n        .stop_on_signal(SignalKind::user_defined1())\n        .unwrap();\n\n    // Create listening socket - will be inherited by children\n    let addr: SocketAddr = \"0.0.0.0:8080\".parse().unwrap();\n    let stream = ecdysis_builder\n        .build_listen_tcp(StopOnShutdown::Yes, addr, |builder, addr| {\n            builder.set_reuse_address(true)?;\n            builder.bind(&amp;addr.into())?;\n            builder.listen(128)?;\n            Ok(builder.into())\n        })\n        .unwrap();\n\n    // Spawn task to handle connections\n    let server_handle = tokio::spawn(async move {\n        let mut stream = stream;\n        let mut set = JoinSet::new();\n        while let Some(Ok(socket)) = stream.next().await {\n            set.spawn(handle_connection(socket));\n        }\n        set.join_all().await;\n    });\n\n    // Signal readiness and wait for shutdown\n    let (_ecdysis, shutdown_fut) = ecdysis_builder.ready().unwrap();\n    let shutdown_reason = shutdown_fut.await;\n\n    log::info!(\"Shutting down: {:?}\", shutdown_reason);\n\n    // Gracefully drain connections\n    server_handle.await.unwrap();\n}\n\nasync fn handle_connection(mut socket: TcpStream) {\n    // Echo connection logic here\n}</code></pre>\n            <p>The key points:</p><ol><li><p><code><b>build_listen_tcp</b></code> creates a listener that will be inherited by child processes.</p></li><li><p><code><b>ready()</b></code> signals to the parent process that initialization is complete and that it can safely exit.</p></li><li><p><code><b>shutdown_fut.await</b></code> blocks until an upgrade or stop is requested. This future only yields once the process should be shut down, either because an upgrade/reload was executed successfully or because a shutdown signal was received.</p></li></ol><p>When you send <code>SIGHUP</code> to this process, here’s what ecdysis does…</p><p><i>…on the parent process:</i></p><ul><li><p>Forks and execs a new instance of your binary.</p></li><li><p>Passes the listening socket to the child.</p></li><li><p>Waits for the child to call <code>ready()</code>.</p></li><li><p>Drains existing connections, then exits.</p></li></ul><p><i>…on the child process:</i></p><ul><li><p>Initializes itself following the same execution flow as the parent, except any sockets owned by ecdysis are inherited and not bound by the child.</p></li><li><p>Signals readiness to the parent by calling <code>ready()</code>.</p></li><li><p>Blocks waiting for a shutdown or upgrade signal.</p></li></ul>\n    <div>\n      <h3>Production at scale</h3>\n      <a href=\"#production-at-scale\">\n        \n      </a>\n    </div>\n    <p>ecdysis has been running in production at Cloudflare since 2021. It powers critical Rust infrastructure services deployed across 330+ data centers in 120+ countries. These services handle billions of requests per day and require frequent updates for security patches, feature releases, and configuration changes.</p><p>Every restart using ecdysis saves hundreds of thousands of requests that would otherwise be dropped during a naive stop/start cycle. Across our global footprint, this translates to millions of preserved connections and improved reliability for customers.</p>\n    <div>\n      <h3>ecdysis vs alternatives</h3>\n      <a href=\"#ecdysis-vs-alternatives\">\n        \n      </a>\n    </div>\n    <p>Graceful restart libraries exist for several ecosystems. Understanding when to use ecdysis versus alternatives is critical to choosing the right tool.</p><p><a href=\"https://github.com/cloudflare/tableflip\"><b><u>tableflip</u></b></a> is our Go library that inspired ecdysis. It implements the same fork-and-inherit model for Go services. If you need Go, tableflip is a great option!</p><p><a href=\"https://github.com/cloudflare/shellflip\"><b><u>shellflip</u></b></a> is Cloudflare’s other Rust graceful restart library, designed specifically for Oxy, our Rust-based proxy. shellflip is more opinionated: it assumes systemd and Tokio, and focuses on transferring arbitrary application state between parent and child. This makes it excellent for complex stateful services, or services that want to apply such aggressive sandboxing that they can’t even open their own sockets, but adds overhead for simpler cases.</p>\n    <div>\n      <h3>Start building</h3>\n      <a href=\"#start-building\">\n        \n      </a>\n    </div>\n    <p>ecdysis brings five years of production-hardened graceful restart capabilities to the Rust ecosystem. It’s the same technology protecting millions of connections across Cloudflare’s global network, now open-sourced and available for anyone!</p><p>Full documentation is available at <a href=\"https://docs.rs/ecdysis\"><u>docs.rs/ecdysis</u></a>, including API reference, examples for common use cases, and steps for integrating with <code>systemd</code>.</p><p>The <a href=\"https://github.com/cloudflare/ecdysis/tree/main/examples\"><u>examples directory</u></a> in the repository contains working code demonstrating TCP listeners, Unix socket listeners, and systemd integration.</p><p>The library is actively maintained by the Argo Smart Routing &amp; Orpheus team, with contributions from teams across Cloudflare. We welcome contributions, bug reports, and feature requests on <a href=\"https://github.com/cloudflare/ecdysis\"><u>GitHub</u></a>.</p><p>Whether you’re building a high-performance proxy, a long-lived API server, or any network service where uptime matters, ecdysis can provide a foundation for zero-downtime operations.</p><p>Start building:<a href=\"https://github.com/cloudflare/ecdysis\"> <u>github.com/cloudflare/ecdysis</u></a></p> ","content:encodedSnippet":"ecdysis | ˈekdəsəs |\nnoun\n    the process of shedding the old skin (in reptiles) or casting off the outer \n    cuticle (in insects and other arthropods).  \n\nHow do you upgrade a network service, handling millions of requests per second around the globe, without disrupting even a single connection?\nOne of our solutions at Cloudflare to this massive challenge has long been ecdysis, a Rust library that implements graceful process restarts where no live connections are dropped, and no new connections are refused. \nLast month, we open-sourced ecdysis, so now anyone can use it. After five years of production use at Cloudflare, ecdysis has proven itself by enabling zero-downtime upgrades across our critical Rust infrastructure, saving millions of requests with every restart across Cloudflare’s global network.\nIt’s hard to overstate the importance of getting these upgrades right, especially at the scale of Cloudflare’s network. Many of our services perform critical tasks such as traffic routing, TLS lifecycle management, or firewall rules enforcement, and must operate continuously. If one of these services goes down, even for an instant, the cascading impact can be catastrophic. Dropped connections and failed requests quickly lead to degraded customer performance and business impact.\nWhen these services need updates, security patches can’t wait. Bug fixes need deployment and new features must roll out. \nThe naive approach involves waiting for the old process to be stopped before spinning up the new one, but this creates a window of time where connections are refused and requests are dropped. For a service handling thousands of requests per second in a single location, multiply that across hundreds of data centers, and a brief restart becomes millions of failed requests globally.\nLet’s dig into the problem, and how ecdysis has been the solution for us — and maybe will be for you. \nLinks: GitHub | crates.io | docs.rs\nWhy graceful restarts are hard\nThe naive approach to restarting a service, as we mentioned, is to stop the old process and start a new one. This works acceptably for simple services that don’t handle real-time requests, but for network services processing live connections, this approach has critical limitations.\nFirst, the naive approach creates a window during which no process is listening for incoming connections. When the old process stops, it closes its listening sockets, which causes the OS to immediately refuse new connections with ECONNREFUSED. Even if the new process starts immediately, there will always be a gap where nothing is accepting connections, whether milliseconds or seconds. For a service handling thousands of requests per second, even a gap of 100ms means hundreds of dropped connections.\nSecond, stopping the old process kills all already-established connections. A client uploading a large file or streaming video gets abruptly disconnected. Long-lived connections like WebSockets or gRPC streams are terminated mid-operation. From the client’s perspective, the service simply vanishes.\nBinding the new process before shutting down the old one appears to solve this, but also introduces additional issues. The kernel normally allows only one process to bind to an address:port combination, but the SO_REUSEPORT socket option permits multiple binds. However, this creates a problem during process transitions that makes it unsuitable for graceful restarts.\nWhen SO_REUSEPORT is used, the kernel creates separate listening sockets for each process and load balances new connections across these sockets. When the initial SYN packet for a connection is received, the kernel will assign it to one of the listening processes. Once the initial handshake is completed, the connection then sits in the accept() queue of the process until the process accepts it. If the process then exits before accepting this connection, it becomes orphaned and is terminated by the kernel. GitHub’s engineering team documented this issue extensively when building their GLB Director load balancer.\nHow ecdysis works\nWhen we set out to design and build ecdysis, we identified four key goals for the library:\n\nOld code can be completely shut down post-upgrade.\n\nThe new process has a grace period for initialization.\n\nNew code crashing during initialization is acceptable and shouldn’t affect the running service.\n\nOnly a single upgrade runs in parallel to avoid cascading failures.\n\necdysis satisfies these requirements following an approach pioneered by NGINX, which has supported graceful upgrades since its early days. The approach is straightforward: \n\nThe parent process fork()s a new child process.\n\nThe child process replaces itself with a new version of the code with execve().\n\nThe child process inherits the socket file descriptors via a named pipe shared with the parent.\n\nThe parent process waits for the child process to signal readiness before shutting down.\n\nCrucially, the socket remains open throughout the transition. The child process inherits the listening socket from the parent as a file descriptor shared via a named pipe. During the child's initialization, both processes share the same underlying kernel data structure, allowing the parent to continue accepting and processing new and existing connections. Once the child completes initialization, it notifies the parent and begins accepting connections. Upon receiving this ready notification, the parent immediately closes its copy of the listening socket and continues handling only existing connections. \nThis process eliminates coverage gaps while providing the child a safe initialization window. There is a brief window of time when both the parent and child may accept connections concurrently. This is intentional; any connections accepted by the parent are simply handled until completion as part of the draining process.\nThis model also provides the required crash safety. If the child process fails during initialization (e.g., due to a configuration error), it simply exits. Since the parent never stopped listening, no connections are dropped, and the upgrade can be retried once the problem is fixed.\necdysis implements the forking model with first-class support for asynchronous programming through Tokio and systemd integration:\n\nTokio integration: Native async stream wrappers for Tokio. Inherited sockets become listeners without additional glue code. For synchronous services, ecdysis supports operation without async runtime requirements.\n\nsystemd-notify support: When the systemd_notify feature is enabled, ecdysis automatically integrates with systemd’s process lifecycle notifications. Setting Type=notify-reload in your service unit file allows systemd to track upgrades correctly.\n\nsystemd named sockets: The systemd_sockets feature enables ecdysis to manage systemd-activated sockets. Your service can be socket-activated and support graceful restarts simultaneously.\n\nPlatform note: ecdysis relies on Unix-specific syscalls for socket inheritance and process management. It does not work on Windows. This is a fundamental limitation of the forking approach.\nSecurity considerations\nGraceful restarts introduce security considerations. The forking model creates a brief window where two process generations coexist, both with access to the same listening sockets and potentially sensitive file descriptors.\necdysis addresses these concerns through its design:\nFork-then-exec: ecdysis follows the traditional Unix pattern of fork() followed immediately by execve(). This ensures the child process starts with a clean slate: new address space, fresh code, and no inherited memory. Only explicitly-passed file descriptors cross the boundary.\nExplicit inheritance: Only listening sockets and communication pipes are inherited. Other file descriptors are closed via CLOEXEC flags. This prevents accidental leakage of sensitive handles.\nseccomp compatibility: Services using seccomp filters must allow fork() and execve(). This is a tradeoff: graceful restarts require these syscalls, so they cannot be blocked.\nFor most network services, these tradeoffs are acceptable. The security of the fork-exec model is well understood and has been battle-tested for decades in software like NGINX and Apache.\nCode example\nLet’s look at a practical example. Here’s a simplified TCP echo server that supports graceful restarts:\nuse ecdysis::tokio_ecdysis::{SignalKind, StopOnShutdown, TokioEcdysisBuilder};\nuse tokio::{net::TcpStream, task::JoinSet};\nuse futures::StreamExt;\nuse std::net::SocketAddr;\n\n#[tokio::main]\nasync fn main() {\n    // Create the ecdysis builder\n    let mut ecdysis_builder = TokioEcdysisBuilder::new(\n        SignalKind::hangup()  // Trigger upgrade/reload on SIGHUP\n    ).unwrap();\n\n    // Trigger stop on SIGUSR1\n    ecdysis_builder\n        .stop_on_signal(SignalKind::user_defined1())\n        .unwrap();\n\n    // Create listening socket - will be inherited by children\n    let addr: SocketAddr = \"0.0.0.0:8080\".parse().unwrap();\n    let stream = ecdysis_builder\n        .build_listen_tcp(StopOnShutdown::Yes, addr, |builder, addr| {\n            builder.set_reuse_address(true)?;\n            builder.bind(&addr.into())?;\n            builder.listen(128)?;\n            Ok(builder.into())\n        })\n        .unwrap();\n\n    // Spawn task to handle connections\n    let server_handle = tokio::spawn(async move {\n        let mut stream = stream;\n        let mut set = JoinSet::new();\n        while let Some(Ok(socket)) = stream.next().await {\n            set.spawn(handle_connection(socket));\n        }\n        set.join_all().await;\n    });\n\n    // Signal readiness and wait for shutdown\n    let (_ecdysis, shutdown_fut) = ecdysis_builder.ready().unwrap();\n    let shutdown_reason = shutdown_fut.await;\n\n    log::info!(\"Shutting down: {:?}\", shutdown_reason);\n\n    // Gracefully drain connections\n    server_handle.await.unwrap();\n}\n\nasync fn handle_connection(mut socket: TcpStream) {\n    // Echo connection logic here\n}\nThe key points:\n\nbuild_listen_tcp creates a listener that will be inherited by child processes.\n\nready() signals to the parent process that initialization is complete and that it can safely exit.\n\nshutdown_fut.await blocks until an upgrade or stop is requested. This future only yields once the process should be shut down, either because an upgrade/reload was executed successfully or because a shutdown signal was received.\n\nWhen you send SIGHUP to this process, here’s what ecdysis does…\n…on the parent process:\n\nForks and execs a new instance of your binary.\n\nPasses the listening socket to the child.\n\nWaits for the child to call ready().\n\nDrains existing connections, then exits.\n\n…on the child process:\n\nInitializes itself following the same execution flow as the parent, except any sockets owned by ecdysis are inherited and not bound by the child.\n\nSignals readiness to the parent by calling ready().\n\nBlocks waiting for a shutdown or upgrade signal.\n\nProduction at scale\necdysis has been running in production at Cloudflare since 2021. It powers critical Rust infrastructure services deployed across 330+ data centers in 120+ countries. These services handle billions of requests per day and require frequent updates for security patches, feature releases, and configuration changes.\nEvery restart using ecdysis saves hundreds of thousands of requests that would otherwise be dropped during a naive stop/start cycle. Across our global footprint, this translates to millions of preserved connections and improved reliability for customers.\necdysis vs alternatives\nGraceful restart libraries exist for several ecosystems. Understanding when to use ecdysis versus alternatives is critical to choosing the right tool.\ntableflip is our Go library that inspired ecdysis. It implements the same fork-and-inherit model for Go services. If you need Go, tableflip is a great option!\nshellflip is Cloudflare’s other Rust graceful restart library, designed specifically for Oxy, our Rust-based proxy. shellflip is more opinionated: it assumes systemd and Tokio, and focuses on transferring arbitrary application state between parent and child. This makes it excellent for complex stateful services, or services that want to apply such aggressive sandboxing that they can’t even open their own sockets, but adds overhead for simpler cases.\nStart building\necdysis brings five years of production-hardened graceful restart capabilities to the Rust ecosystem. It’s the same technology protecting millions of connections across Cloudflare’s global network, now open-sourced and available for anyone!\nFull documentation is available at docs.rs/ecdysis, including API reference, examples for common use cases, and steps for integrating with systemd.\nThe examples directory in the repository contains working code demonstrating TCP listeners, Unix socket listeners, and systemd integration.\nThe library is actively maintained by the Argo Smart Routing & Orpheus team, with contributions from teams across Cloudflare. We welcome contributions, bug reports, and feature requests on GitHub.\nWhether you’re building a high-performance proxy, a long-lived API server, or any network service where uptime matters, ecdysis can provide a foundation for zero-downtime operations.\nStart building: github.com/cloudflare/ecdysis","dc:creator":"Manuel Olguín Muñoz","content":" ecdysis is a Rust library enabling zero-downtime upgrades for network services. After five years protecting millions of connections at Cloudflare, it’s now open source. ","contentSnippet":"ecdysis is a Rust library enabling zero-downtime upgrades for network services. After five years protecting millions of connections at Cloudflare, it’s now open source.","guid":"GMarF75NkFuiwVuyFJk77","categories":["Rust","Open Source","Infrastructure","Engineering","Edge","Developers","Developer Platform","Application Services","Rust"],"isoDate":"2026-02-13T14:00:00.000Z"},{"creator":"Celso Martinho","title":"Introducing Markdown for Agents","link":"https://blog.cloudflare.com/markdown-for-agents/","pubDate":"Thu, 12 Feb 2026 14:03:00 GMT","content:encoded":" <p>The way content and businesses are discovered online is changing rapidly. In the past, traffic originated from traditional search engines, and SEO determined who got found first. Now the traffic is increasingly coming from AI crawlers and agents that demand structured data within the often-unstructured Web that was built for humans.</p><p>As a business, to continue to stay ahead, now is the time to consider not just human visitors, or traditional wisdom for SEO-optimization, but start to treat agents as first-class citizens. </p>\n    <div>\n      <h2>Why markdown is important</h2>\n      <a href=\"#why-markdown-is-important\">\n        \n      </a>\n    </div>\n    <p>Feeding raw HTML to an AI is like paying by the word to read packaging instead of the letter inside. A simple <code>## About Us</code> on a page in markdown costs roughly 3 tokens; its HTML equivalent – <code>&lt;h2 class=\"section-title\" id=\"about\"&gt;About Us&lt;/h2&gt;</code> – burns 12-15, and that's before you account for the <code>&lt;div&gt;</code> wrappers, nav bars, and script tags that pad every real web page and have zero semantic value.</p><p>This blog post you’re reading takes 16,180 tokens in HTML and 3,150 tokens when converted to markdown. <b>That’s a 80% reduction in token usage</b>.</p><p><a href=\"https://en.wikipedia.org/wiki/Markdown\"><u>Markdown</u></a> has quickly become the <i>lingua franca</i> for agents and AI systems as a whole. The format’s explicit structure makes it ideal for AI processing, ultimately resulting in better results while minimizing token waste.</p><p>The problem is that the Web is made of HTML, not markdown, and page weight has been <a href=\"https://almanac.httparchive.org/en/2025/page-weight#page-weight-over-time\"><u>steadily increasing</u></a> over the years, making pages hard to parse. For agents, their goal is to filter out all non-essential elements and scan the relevant content.</p><p>The conversion of HTML to markdown is now a common step for any AI pipeline. Still, this process is far from ideal: it wastes computation, adds costs and processing complexity, and above all, it may not be how the content creator intended their content to be used in the first place.</p><p>What if AI agents could bypass the complexities of intent analysis and document conversion, and instead receive structured markdown directly from the source?</p>\n    <div>\n      <h2>Convert HTML to markdown, automatically</h2>\n      <a href=\"#convert-html-to-markdown-automatically\">\n        \n      </a>\n    </div>\n    <p>Cloudflare's network now supports real-time content conversion at the source, for <a href=\"https://developers.cloudflare.com/fundamentals/reference/markdown-for-agents/\"><u>enabled zones</u></a> using <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Content_negotiation\"><u>content negotiation</u></a> headers. Now when AI systems request pages from any website that uses Cloudflare and has Markdown for Agents enabled, they can express the preference for text/markdown in the request. Our network will automatically and efficiently convert the HTML to markdown, when possible, on the fly.</p><p>Here’s how it works. To fetch the markdown version of any page from a zone with Markdown for Agents enabled, the client needs to add the <b>Accept</b> negotiation header with <code>text/markdown</code><b> </b>as one of the options. Cloudflare will detect this, fetch the original HTML version from the origin, and convert it to markdown before serving it to the client.</p><p>Here's a curl example with the Accept negotiation header requesting a page from our developer documentation:</p>\n            <pre><code>curl https://developers.cloudflare.com/fundamentals/reference/markdown-for-agents/ \\\n  -H \"Accept: text/markdown\"\n</code></pre>\n            <p>Or if you’re building an AI Agent using Workers, you can use TypeScript:</p>\n            <pre><code>const r = await fetch(\n  `https://developers.cloudflare.com/fundamentals/reference/markdown-for-agents/`,\n  {\n    headers: {\n      Accept: \"text/markdown, text/html\",\n    },\n  },\n);\nconst tokenCount = r.headers.get(\"x-markdown-tokens\");\nconst markdown = await r.text();\n</code></pre>\n            <p>We already see some of the most popular coding agents today – like Claude Code and OpenCode – send these accept headers with their requests for content. Now, the response to this request is formatted  in markdown. It's that simple.  </p>\n            <pre><code>HTTP/2 200\ndate: Wed, 11 Feb 2026 11:44:48 GMT\ncontent-type: text/markdown; charset=utf-8\ncontent-length: 2899\nvary: accept\nx-markdown-tokens: 725\ncontent-signal: ai-train=yes, search=yes, ai-input=yes\n\n---\ntitle: Markdown for Agents · Cloudflare Agents docs\n---\n\n## What is Markdown for Agents\n\nThe ability to parse and convert HTML to Markdown has become foundational for AI.\n...\n</code></pre>\n            <p>Note that we include an <code>x-markdown-tokens</code> header with the converted response that indicates the estimated number of tokens in the markdown document. You can use this value in your flow, for example to calculate the size of a context window or to decide on your chunking strategy.</p><p>Here’s a diagram of how it works:</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6Zw1Q5kBBqTrouN1362H5I/3080d74a2a971be1f1e7e0ba79611998/BLOG-3162_2.png\" />\n          </figure>\n    <div>\n      <h3>Content Signals Policy</h3>\n      <a href=\"#content-signals-policy\">\n        \n      </a>\n    </div>\n    <p>During our last Birthday Week, Cloudflare <a href=\"https://blog.cloudflare.com/content-signals-policy/\"><u>announced</u></a> Content Signals — <a href=\"http://contentsignals.org\"><u>a framework</u></a> that allows anyone to express their preferences for how their content can be used after it has been accessed. </p><p>When you return markdown, you want to make sure your content is being used by the Agent or AI crawler. That’s why Markdown for Agents converted responses include the <code>Content-Signal: ai-train=yes, search=yes, ai-input=yes</code> header signaling that indicates content can be used for AI Training, Search results and AI Input, which includes agentic use. Markdown for Agents will provide options to define custom Content Signal policies in the future.</p><p>Check our dedicated <a href=\"https://contentsignals.org/\"><u>Content Signals</u></a> page for more information on this framework.</p>\n    <div>\n      <h3>Try it with the Cloudflare Blog &amp; Developer Documentation </h3>\n      <a href=\"#try-it-with-the-cloudflare-blog-developer-documentation\">\n        \n      </a>\n    </div>\n    <p>We enabled this feature in our <a href=\"https://developers.cloudflare.com/\"><u>Developer Documentation</u></a> and our <a href=\"https://blog.cloudflare.com/\"><u>Blog</u></a>, inviting all AI crawlers and agents to consume our content using markdown instead of HTML.</p><p>Try it out now by requesting this blog with <code>Accept: text/markdown</code>.</p>\n            <pre><code>curl https://blog.cloudflare.com/markdown-for-agents/ \\\n  -H \"Accept: text/markdown\"</code></pre>\n            <p>The result is:</p>\n            <pre><code>---\ndescription: The way content is discovered online is shifting, from traditional search engines to AI agents that need structured data from a Web built for humans. It’s time to consider not just human visitors, but start to treat agents as first-class citizens. Markdown for Agents automatically converts any HTML page requested from our network to markdown.\ntitle: Introducing Markdown for Agents\nimage: https://blog.cloudflare.com/images/markdown-for-agents.png\n---\n\n# Introducing Markdown for Agents\n\nThe way content and businesses are discovered online is changing rapidly. In the past, traffic originated from traditional search engines and SEO determined who got found first. Now the traffic is increasingly coming from AI crawlers and agents that demand structured data within the often-unstructured Web that was built for humans.\n\n...</code></pre>\n            \n    <div>\n      <h3>Other ways to convert to Markdown</h3>\n      <a href=\"#other-ways-to-convert-to-markdown\">\n        \n      </a>\n    </div>\n    <p>If you’re building AI systems that require arbitrary document conversion from outside Cloudflare or Markdown for Agents is not available from the content source, we provide other ways to convert documents to Markdown for your applications:</p><ul><li><p>Workers AI <a href=\"https://developers.cloudflare.com/workers-ai/features/markdown-conversion/\"><u>AI.toMarkdown()</u></a> supports multiple document types, not just HTML, and summarization.</p></li><li><p>Browser Rendering <a href=\"https://developers.cloudflare.com/browser-rendering/rest-api/markdown-endpoint/\"><u>/markdown</u></a> REST API supports markdown conversion if you need to render a dynamic page or application in a real browser before converting it.</p></li></ul>\n    <div>\n      <h2>Tracking markdown usage</h2>\n      <a href=\"#tracking-markdown-usage\">\n        \n      </a>\n    </div>\n    <p>Anticipating a shift in how AI systems browse the Web, Cloudflare Radar now includes content type insights for AI bot and crawler traffic, both globally on the <a href=\"https://radar.cloudflare.com/ai-insights#content-type\"><u>AI Insights</u></a> page and in the <a href=\"https://radar.cloudflare.com/bots/directory/gptbot\"><u>individual bot</u></a> information pages.</p><p>The new <code>content_type</code> dimension and filter shows the distribution of content types returned to AI agents and crawlers, grouped by <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/MIME_types\"><u>MIME type</u></a> category.  </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7vQzvzHsTLPXGhoQK0Xbr5/183129a8947990bc4ee5bb5ca7ba71b5/BLOG-3162_3.png\" />\n          </figure><p>You can also see the requests for markdown filtered by a specific agent or crawler. Here are the requests that return markdown to OAI-Searchbot, the crawler used by OpenAI to power ChatGPT’s search: </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7Ah99DWLxnYjadW6xJhAXg/afef4a29ae504d4fe69df4f9823dd103/BLOG-3162_4.png\" />\n          </figure><p>This new data will allow us to track the evolution of how AI bots, crawlers, and agents are consuming Web content over time. As always, everything on Radar is freely accessible via the <a href=\"https://developers.cloudflare.com/api/resources/radar/\"><u>public APIs</u></a> and the <a href=\"https://radar.cloudflare.com/explorer?dataSet=ai.bots&amp;groupBy=content_type&amp;filters=userAgent%253DGPTBot&amp;timeCompare=1\"><u>Data Explorer</u></a>. </p>\n    <div>\n      <h2>Start using today</h2>\n      <a href=\"#start-using-today\">\n        \n      </a>\n    </div>\n    <p>To enable Markdown for Agents for your zone, log into the Cloudflare <a href=\"https://dash.cloudflare.com/\"><u>dashboard</u></a>, select your account, select the zone, look for Quick Actions and toggle the Markdown for Agents button to enable. This feature is available today in Beta at no cost for Pro, Business and Enterprise plans, as well as SSL for SaaS customers.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1UqzmHrNa1UdCCI6eXIfmn/3da0ff51dd94219d8af87c172d83fc72/BLOG-3162_5.png\" />\n          </figure><p>You can find more information about Markdown for Agents on our<a href=\"https://developers.cloudflare.com/fundamentals/reference/markdown-for-agents/\"> Developer Docs</a>. We welcome your feedback as we continue to refine and enhance this feature. We’re curious to see how AI crawlers and agents navigate and adapt to the unstructured nature of the Web as it evolves.</p> ","content:encodedSnippet":"The way content and businesses are discovered online is changing rapidly. In the past, traffic originated from traditional search engines, and SEO determined who got found first. Now the traffic is increasingly coming from AI crawlers and agents that demand structured data within the often-unstructured Web that was built for humans.\nAs a business, to continue to stay ahead, now is the time to consider not just human visitors, or traditional wisdom for SEO-optimization, but start to treat agents as first-class citizens. \nWhy markdown is important\nFeeding raw HTML to an AI is like paying by the word to read packaging instead of the letter inside. A simple ## About Us on a page in markdown costs roughly 3 tokens; its HTML equivalent – <h2 class=\"section-title\" id=\"about\">About Us</h2> – burns 12-15, and that's before you account for the <div> wrappers, nav bars, and script tags that pad every real web page and have zero semantic value.\nThis blog post you’re reading takes 16,180 tokens in HTML and 3,150 tokens when converted to markdown. That’s a 80% reduction in token usage.\nMarkdown has quickly become the lingua franca for agents and AI systems as a whole. The format’s explicit structure makes it ideal for AI processing, ultimately resulting in better results while minimizing token waste.\nThe problem is that the Web is made of HTML, not markdown, and page weight has been steadily increasing over the years, making pages hard to parse. For agents, their goal is to filter out all non-essential elements and scan the relevant content.\nThe conversion of HTML to markdown is now a common step for any AI pipeline. Still, this process is far from ideal: it wastes computation, adds costs and processing complexity, and above all, it may not be how the content creator intended their content to be used in the first place.\nWhat if AI agents could bypass the complexities of intent analysis and document conversion, and instead receive structured markdown directly from the source?\nConvert HTML to markdown, automatically\nCloudflare's network now supports real-time content conversion at the source, for enabled zones using content negotiation headers. Now when AI systems request pages from any website that uses Cloudflare and has Markdown for Agents enabled, they can express the preference for text/markdown in the request. Our network will automatically and efficiently convert the HTML to markdown, when possible, on the fly.\nHere’s how it works. To fetch the markdown version of any page from a zone with Markdown for Agents enabled, the client needs to add the Accept negotiation header with text/markdown as one of the options. Cloudflare will detect this, fetch the original HTML version from the origin, and convert it to markdown before serving it to the client.\nHere's a curl example with the Accept negotiation header requesting a page from our developer documentation:\ncurl https://developers.cloudflare.com/fundamentals/reference/markdown-for-agents/ \\\n  -H \"Accept: text/markdown\"\n\nOr if you’re building an AI Agent using Workers, you can use TypeScript:\nconst r = await fetch(\n  `https://developers.cloudflare.com/fundamentals/reference/markdown-for-agents/`,\n  {\n    headers: {\n      Accept: \"text/markdown, text/html\",\n    },\n  },\n);\nconst tokenCount = r.headers.get(\"x-markdown-tokens\");\nconst markdown = await r.text();\n\nWe already see some of the most popular coding agents today – like Claude Code and OpenCode – send these accept headers with their requests for content. Now, the response to this request is formatted  in markdown. It's that simple.  \nHTTP/2 200\ndate: Wed, 11 Feb 2026 11:44:48 GMT\ncontent-type: text/markdown; charset=utf-8\ncontent-length: 2899\nvary: accept\nx-markdown-tokens: 725\ncontent-signal: ai-train=yes, search=yes, ai-input=yes\n\n---\ntitle: Markdown for Agents · Cloudflare Agents docs\n---\n\n## What is Markdown for Agents\n\nThe ability to parse and convert HTML to Markdown has become foundational for AI.\n...\n\nNote that we include an x-markdown-tokens header with the converted response that indicates the estimated number of tokens in the markdown document. You can use this value in your flow, for example to calculate the size of a context window or to decide on your chunking strategy.\nHere’s a diagram of how it works:\nContent Signals Policy\nDuring our last Birthday Week, Cloudflare announced Content Signals — a framework that allows anyone to express their preferences for how their content can be used after it has been accessed. \nWhen you return markdown, you want to make sure your content is being used by the Agent or AI crawler. That’s why Markdown for Agents converted responses include the Content-Signal: ai-train=yes, search=yes, ai-input=yes header signaling that indicates content can be used for AI Training, Search results and AI Input, which includes agentic use. Markdown for Agents will provide options to define custom Content Signal policies in the future.\nCheck our dedicated Content Signals page for more information on this framework.\nTry it with the Cloudflare Blog & Developer Documentation \nWe enabled this feature in our Developer Documentation and our Blog, inviting all AI crawlers and agents to consume our content using markdown instead of HTML.\nTry it out now by requesting this blog with Accept: text/markdown.\ncurl https://blog.cloudflare.com/markdown-for-agents/ \\\n  -H \"Accept: text/markdown\"\nThe result is:\n---\ndescription: The way content is discovered online is shifting, from traditional search engines to AI agents that need structured data from a Web built for humans. It’s time to consider not just human visitors, but start to treat agents as first-class citizens. Markdown for Agents automatically converts any HTML page requested from our network to markdown.\ntitle: Introducing Markdown for Agents\nimage: https://blog.cloudflare.com/images/markdown-for-agents.png\n---\n\n# Introducing Markdown for Agents\n\nThe way content and businesses are discovered online is changing rapidly. In the past, traffic originated from traditional search engines and SEO determined who got found first. Now the traffic is increasingly coming from AI crawlers and agents that demand structured data within the often-unstructured Web that was built for humans.\n\n...\nOther ways to convert to Markdown\nIf you’re building AI systems that require arbitrary document conversion from outside Cloudflare or Markdown for Agents is not available from the content source, we provide other ways to convert documents to Markdown for your applications:\n\nWorkers AI AI.toMarkdown() supports multiple document types, not just HTML, and summarization.\n\nBrowser Rendering /markdown REST API supports markdown conversion if you need to render a dynamic page or application in a real browser before converting it.\n\nTracking markdown usage\nAnticipating a shift in how AI systems browse the Web, Cloudflare Radar now includes content type insights for AI bot and crawler traffic, both globally on the AI Insights page and in the individual bot information pages.\nThe new content_type dimension and filter shows the distribution of content types returned to AI agents and crawlers, grouped by MIME type category.  \nYou can also see the requests for markdown filtered by a specific agent or crawler. Here are the requests that return markdown to OAI-Searchbot, the crawler used by OpenAI to power ChatGPT’s search: \nThis new data will allow us to track the evolution of how AI bots, crawlers, and agents are consuming Web content over time. As always, everything on Radar is freely accessible via the public APIs and the Data Explorer. \nStart using today\nTo enable Markdown for Agents for your zone, log into the Cloudflare dashboard, select your account, select the zone, look for Quick Actions and toggle the Markdown for Agents button to enable. This feature is available today in Beta at no cost for Pro, Business and Enterprise plans, as well as SSL for SaaS customers.\nYou can find more information about Markdown for Agents on our Developer Docs. We welcome your feedback as we continue to refine and enhance this feature. We’re curious to see how AI crawlers and agents navigate and adapt to the unstructured nature of the Web as it evolves.","dc:creator":"Celso Martinho","content":" The way content is discovered online is shifting, from traditional search engines to AI agents that need structured data from a Web built for humans. It’s time to consider not just human visitors, but start to treat agents as first-class citizens. Markdown for Agents automatically converts any HTML page requested from our network to markdown. ","contentSnippet":"The way content is discovered online is shifting, from traditional search engines to AI agents that need structured data from a Web built for humans. It’s time to consider not just human visitors, but start to treat agents as first-class citizens. Markdown for Agents automatically converts any HTML page requested from our network to markdown.","guid":"5uEb99xvnHVk3QfN0KMjb6","categories":["AI","Agents","Developers","Developer Platform"],"isoDate":"2026-02-12T14:03:00.000Z"},{"creator":"Omer Yoachimik","title":"2025 Q4 DDoS threat report: A record-setting 31.4 Tbps attack caps a year of massive DDoS assaults","link":"https://blog.cloudflare.com/ddos-threat-report-2025-q4/","pubDate":"Thu, 05 Feb 2026 14:00:00 GMT","content:encoded":" <p>Welcome to the 24th edition of Cloudflare’s Quarterly DDoS Threat Report. In this report, <a href=\"https://www.cloudflare.com/cloudforce-one/\"><u>Cloudforce One</u></a> offers a comprehensive analysis of the evolving threat landscape of <a href=\"https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/\"><u>Distributed Denial of Service (DDoS) attacks</u></a> based on data from the <a href=\"https://www.cloudflare.com/network/\"><u>Cloudflare network</u></a>. In this edition, we focus on the fourth quarter of 2025, as well as share overall 2025 data.</p><p>The fourth quarter of 2025 was characterized by an unprecedented bombardment launched by the <a href=\"https://www.cloudflare.com/learning/ddos/glossary/aisuru-kimwolf-botnet/\"><u>Aisuru-Kimwolf botnet</u></a>, dubbed “The Night Before Christmas\" DDoS attack campaign. The campaign targeted Cloudflare customers as well as Cloudflare’s dashboard and infrastructure with hyper-volumetric HTTP DDoS attacks exceeding rates of 200 million requests per second (rps), just weeks after a record-breaking 31.4 Terabits per second (Tbps) attack.</p>\n    <div>\n      <h2>Key insights</h2>\n      <a href=\"#key-insights\">\n        \n      </a>\n    </div>\n    <ol><li><p>DDoS attacks surged by 121% in 2025, reaching an average of 5,376 attacks automatically mitigated every hour.</p></li><li><p>In the final quarter of 2025, Hong Kong jumped 12 places, making it the second most DDoS’d place on earth. The United Kingdom also leapt by an astonishing 36 places, making it the sixth most-attacked place.</p></li><li><p>Infected Android TVs — part of the Aisuru-Kimwolf botnet — bombarded Cloudflare’s network with hyper-volumetric HTTP DDoS attacks, while Telcos emerged as the most-attacked industry.</p></li></ol>\n    <div>\n      <h2>2025 saw a huge spike in DDoS attacks</h2>\n      <a href=\"#2025-saw-a-huge-spike-in-ddos-attacks\">\n        \n      </a>\n    </div>\n    <p>In 2025, the total number of DDoS attacks more than doubled to an incredible 47.1 million. Such attacks have soared in recent years: The number of DDoS attacks spiked 236% between 2023 and 2025.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7gWz9fvMGvTVL30YfnFL55/57749a329c2be23e45f87227221aa440/BLOG-3098_2.png\" />\n          </figure><p>In 2025, Cloudflare mitigated an average of 5,376 DDoS attacks every hour — of these, 3,925 were network-layer DDoS attacks and 1,451 were HTTP DDoS attacks. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6cANr8wDVOOMNIb9IPvPYb/56f75509048fcd68c188fdd87f68e883/.png\" />\n          </figure>\n    <div>\n      <h3>Network-layer DDoS attacks more than tripled in 2025</h3>\n      <a href=\"#network-layer-ddos-attacks-more-than-tripled-in-2025\">\n        \n      </a>\n    </div>\n    <p>The most substantial growth was in network-layer DDoS attacks, which more than tripled year over year. Cloudflare mitigated 34.4 million network-layer DDoS attacks in 2025, compared to 11.4 million in 2024.</p><p>A substantial portion of the network-layer attacks — approximately 13.5 million — targeted global Internet infrastructure protected by <a href=\"https://www.cloudflare.com/en-gb/network-services/products/magic-transit/\"><u>Cloudflare Magic Transit</u></a> and Cloudflare’s infrastructure directly, as part of an 18-day DDoS campaign in the first quarter of 2025. Of these attacks, 6.9 million targeted Magic Transit customers while the remaining 6.6 million targeted Cloudflare directly. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6jomtSPOraOer8LPDxJ3Aw/603db470ecbde1362579624193807e43/BLOG-3098_4.png\" />\n          </figure><p>This assault was a multi-vector DDoS campaign comprising <a href=\"https://www.cloudflare.com/learning/ddos/syn-flood-ddos-attack/\"><u>SYN flood attacks</u></a>, <a href=\"https://www.cloudflare.com/learning/ddos/glossary/mirai-botnet/\"><u>Mirai-generated DDoS attacks</u></a>, and <a href=\"https://www.cloudflare.com/learning/ddos/ssdp-ddos-attack/\"><u>SSDP amplification attacks</u></a> to name a few. Our systems detected and mitigated these attacks automatically. In fact, we only discovered the campaign while preparing our <a href=\"https://blog.cloudflare.com/ddos-threat-report-for-2025-q1/\"><u>DDoS threat report for 2025 Q1</u></a> — an example of how effective Cloudflare’s DDoS mitigation is!</p><p>In the final quarter of 2025, the number of DDoS attacks grew by 31% over the previous quarter and 58% over 2024. Network-layer DDoS attacks fueled that growth. In 2025 Q4, network-layer DDoS attacks accounted for 78% of all DDoS attacks. The amount of HTTP DDoS attacks remained the same, but surged in their size to rates that we haven’t seen since the <a href=\"https://blog.cloudflare.com/technical-breakdown-http2-rapid-reset-ddos-attack/\"><u>HTTP/2 Rapid Reset DDoS campaign</u></a> in 2023. These recent surges were launched by the <a href=\"https://www.cloudflare.com/learning/ddos/glossary/aisuru-kimwolf-botnet/\"><u>Aisuru-Kimwolf botnet</u></a>, which we will cover in the next section. </p>\n    <div>\n      <h3>“The Night Before Christmas” DDoS campaign</h3>\n      <a href=\"#the-night-before-christmas-ddos-campaign\">\n        \n      </a>\n    </div>\n    <p>On Friday, December 19, 2025, the <a href=\"https://www.cloudflare.com/learning/ddos/glossary/aisuru-kimwolf-botnet/\"><u>Aisuru-Kimwolf botnet</u></a> began bombarding Cloudflare infrastructure and Cloudflare customers with hyper-volumetric DDoS attacks. What was new in this campaign was its size: The botnet used hyper-volumetric HTTP DDoS attacks exceeding rates of 20 million requests per second (Mrps).\n\n</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6CMbEWh6TwRcld7gccwE81/dbe9877483861026d2fec6c0112ca8bb/BLOG-3098_5.png\" />\n          </figure><p>The Aisuru-Kimwolf botnet is a massive collection of <a href=\"https://www.cloudflare.com/learning/ddos/glossary/malware/\"><u>malware</u></a>-infected devices, primarily Android TVs. The botnet comprises an estimated 1-4 million infected hosts. It is capable of launching DDoS attacks that can cripple critical infrastructure, crash most legacy cloud-based DDoS protection solutions, and even disrupt the connectivity of entire nations.</p><p>Throughout the campaign, Cloudflare’s autonomous DDoS defense systems detected and mitigated all of the attacks: 384 packet-intensive attacks, 329 bit-intensive attacks, and 189 request-intensive attacks, for a total of 902 hyper-volumetric DDoS attacks, averaging 53 attacks a day.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3GDQWNNnHac5Ovwm4z5Bug/052d194716063d069e4ccd2ff49e4228/BLOG-3098_6.png\" />\n          </figure><p>The average size of the hyper-volumetric DDoS attacks during the campaign were 3 Bpps, 4 Tbps, and 54 Mrps. The maximum rates recorded during the campaign were 9 Bpps, 24 Tbps, and 205 Mrps.</p><p>To put that in context, the scale of a 205 Mrps DDoS attack is comparable to the combined populations of the UK, Germany, and Spain all simultaneously typing a website address and then hitting 'enter’ at the same second.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7N0ruuQdsq6ncG7sQOMvv2/eb092b6380378031003760697d123f9d/BLOG-3098_7.png\" />\n          </figure><p>While highly dramatic, The Night Before Christmas campaign accounted for only a small portion of the hyper-volumetric DDoS attacks we saw throughout the year.</p>\n    <div>\n      <h3>Hyper-volumetric DDoS attacks</h3>\n      <a href=\"#hyper-volumetric-ddos-attacks\">\n        \n      </a>\n    </div>\n    <p>Throughout 2025, Cloudflare observed a continuous increase in hyper-volumetric DDoS attacks. In 2025 Q4, hyper-volumetric attacks increased by 40% compared to the previous quarter.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3ZZAyBKHY8TST9or2kXc7b/a5927b87b686c50aa7137847cd204b74/BLOG-3098_8.png\" />\n          </figure><p>As the number of attacks increased over the course of 2025, the size of the attacks increased as well, growing by over 700% compared to the large attacks seen in late 2024, with one reaching 31.4 Tbps in a DDoS attack that lasted just 35 seconds. The graph below portrays the rapid growth in DDoS attack sizes as seen and blocked by Cloudflare — each one a world record, i.e. the largest ever disclosed publicly by any company at the time.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5fqqJ2VBvAlhnv0vIpoGZF/bd260c5a7ab673b35865e94b9e86a6d7/BLOG-3098_9.png\" />\n          </figure><p>Like all of the other attacks, the 31.4 Tbps DDoS attack was detected and mitigated automatically by Cloudflare’s autonomous DDoS defense, which was able to adapt and quickly lock on to botnets such as Aisuru-Kimwolf.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3piM1qb6UGpxBXExV0adHn/8f1cfbb2841dce9d6b462fb71704bca2/BLOG-3098_10.png\" />\n          </figure><p>Most of the hyper-volumetric DDoS attacks targeted Cloudflare customers in the Telecommunications, Service Providers and Carriers industry. Cloudflare customers in the Gaming industry and customers providing Generative AI services were also heavily targeted. Lastly, Cloudflare’s own infrastructure itself was targeted by multiple attack vectors such as <a href=\"https://www.cloudflare.com/learning/ddos/http-flood-ddos-attack/\"><u>HTTP floods</u></a>, <a href=\"https://www.cloudflare.com/learning/ddos/dns-amplification-ddos-attack/\"><u>DNS attacks</u></a> and <a href=\"https://www.cloudflare.com/learning/ddos/udp-flood-ddos-attack/\"><u>UDP flood</u></a>.</p>\n    <div>\n      <h3>Most-attacked industries</h3>\n      <a href=\"#most-attacked-industries\">\n        \n      </a>\n    </div>\n    <p>When analyzing DDoS attacks of all sizes, the Telecommunications, Service Providers and Carriers industry was also the most targeted. Previously, the Information Technology &amp; Services industry held that unlucky title.</p><p>The Gambling &amp; Casinos and Gaming industries ranked third and fourth, respectively. The quarter’s biggest changes in the top 10 were the Computer Software and Business Services industries, which both climbed several spots. </p><p>The most-attacked industries are defined by their role as critical infrastructure, a central backbone for other businesses, or their immediate, high-stakes financial sensitivity to service interruption and latency.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2zmtrvUq0cXCEKlprLopWg/80e622f255fa6466f5facfa1288d571b/image8.png\" />\n          </figure>\n    <div>\n      <h3>Most-attacked locations</h3>\n      <a href=\"#most-attacked-locations\">\n        \n      </a>\n    </div>\n    <p>The DDoS landscape saw both predictable stability and dramatic shifts among the world's most-attacked locations. Targets like China, Germany, Brazil, and the United States were the top five, demonstrating persistent appeal for attackers. </p><p>Hong Kong made a significant move, jumping twelve spots to land at number two. However, the bigger story was the meteoric rise of the United Kingdom, which surged an astonishing 36 places this quarter, making it the sixth most-attacked location.  </p><p>Vietnam held its place as the seventh most-attacked location, followed by Azerbaijan in eighth, India in ninth, and Singapore as number ten.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1fbfabacHT9WNKaZLhShlP/465f20da2e2f728692d5c22fc788a0a3/image10.png\" />\n          </figure>\n    <div>\n      <h3>Top attack sources</h3>\n      <a href=\"#top-attack-sources\">\n        \n      </a>\n    </div>\n    <p>Bangladesh dethroned Indonesia as the largest source of DDoS attacks in the fourth quarter of 2025. Indonesia dropped to the third spot, after spending a year as the top source of DDoS attacks. Ecuador also jumped two spots, making it the second-largest source.</p><p>Notably, Argentina soared an incredible twenty places, making it the fourth-largest source of DDoS attacks. Hong Kong rose three places, taking fifth place. Ukraine came in sixth place, followed by Vietnam, Taiwan, Singapore, and Peru.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/67THFzBjHYivQwttU61U9a/f8f5fe3afcca9495cb7d5fb7f61220fa/image5.png\" />\n          </figure>\n    <div>\n      <h2>Top source networks</h2>\n      <a href=\"#top-source-networks\">\n        \n      </a>\n    </div>\n    <p>The top 10 list of attack source networks reads like a list of Internet giants, revealing a fascinating story about the anatomy of modern DDoS attacks. The common thread is clear: Threat actors are leveraging the world's most accessible and powerful network infrastructure — primarily large, public-facing services. </p><p>We see most DDoS attacks coming from IP addresses associated with Cloud Computing Platforms and Cloud Infrastructure Providers, including<a href=\"https://radar.cloudflare.com/as14061\"> <u>DigitalOcean (AS 14061)</u></a>,<a href=\"https://radar.cloudflare.com/as8075\"> <u>Microsoft (AS 8075)</u></a>,<a href=\"https://radar.cloudflare.com/as132203\"> <u>Tencent (AS 132203)</u></a>, <a href=\"https://radar.cloudflare.com/as31898\"><u>Oracle (AS 31898)</u></a>, and<a href=\"https://radar.cloudflare.com/as24940\"> <u>Hetzner (AS 24940)</u></a>. This demonstrates the strong link between easily-provisioned virtual machines and high-volume attacks. These cloud sources, heavily concentrated in the United States, are closely followed by a significant presence of attacks coming from IP addresses associated with traditional Telecommunications Providers (Telcos). These Telcos, primarily from the Asia-Pacific region (including Vietnam, China, Malaysia, and Taiwan), round out the rest of the top 10.</p><p>This geographic and organizational diversity confirms a two-pronged attack reality: While the sheer scale of the highest-ranking sources often originates from global cloud hubs, the problem is truly worldwide, routed through the Internet's most critical pathways from across the globe. In many DDoS attacks, we see thousands of various source ASNs, highlighting the truly global distribution of botnet nodes.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7ga5hHIgrc1pTwosbpx9di/458a87c028e8d51e10c7c56b416d3b64/BLOG-3098_14.png\" />\n          </figure><p>To help hosting providers, cloud computing platforms and Internet service providers identify and take down the abusive IP addresses/accounts that launch these attacks, we leverage Cloudflare’s unique vantage point on DDoS attacks to provide a <a href=\"https://developers.cloudflare.com/ddos-protection/botnet-threat-feed/\"><u>free DDoS Botnet Threat Feed for Service Providers</u></a>. </p><p>Over 800 networks worldwide have signed up for this feed, and we’ve already seen great collaboration across the community to take down botnet nodes.</p>\n    <div>\n      <h3>Helping defend the Internet</h3>\n      <a href=\"#helping-defend-the-internet\">\n        \n      </a>\n    </div>\n    <p>DDoS attacks are rapidly growing in sophistication and size, surpassing what was previously imaginable. This evolving threat landscape presents a significant challenge for many organizations to keep pace. Organizations currently relying on on-premise mitigation appliances or on-demand scrubbing centers may benefit from re-evaluating their defense strategy.</p><p>Cloudflare is dedicated to offering<a href=\"https://www.cloudflare.com/ddos/\"> <u>free, unmetered DDoS protection</u></a> to all its customers, regardless of the size, duration, or volume of attacks, leveraging its<a href=\"https://www.cloudflare.com/network/\"> <u>vast global network</u></a> and<a href=\"https://developers.cloudflare.com/ddos-protection/about/\"> <u>autonomous DDoS mitigation systems</u></a>.</p>\n    <div>\n      <h3>About Cloudforce One</h3>\n      <a href=\"#about-cloudforce-one\">\n        \n      </a>\n    </div>\n    <p>Driven by a mission to help defend the Internet, <a href=\"https://www.cloudflare.com/cloudforce-one/\"><u>Cloudforce One</u></a> leverages telemetry from Cloudflare’s global network — which protects approximately 20% of the web — to drive threat research and operational response, protecting critical systems for millions of organizations worldwide.</p> ","content:encodedSnippet":"Welcome to the 24th edition of Cloudflare’s Quarterly DDoS Threat Report. In this report, Cloudforce One offers a comprehensive analysis of the evolving threat landscape of Distributed Denial of Service (DDoS) attacks based on data from the Cloudflare network. In this edition, we focus on the fourth quarter of 2025, as well as share overall 2025 data.\nThe fourth quarter of 2025 was characterized by an unprecedented bombardment launched by the Aisuru-Kimwolf botnet, dubbed “The Night Before Christmas\" DDoS attack campaign. The campaign targeted Cloudflare customers as well as Cloudflare’s dashboard and infrastructure with hyper-volumetric HTTP DDoS attacks exceeding rates of 200 million requests per second (rps), just weeks after a record-breaking 31.4 Terabits per second (Tbps) attack.\nKey insights\n\nDDoS attacks surged by 121% in 2025, reaching an average of 5,376 attacks automatically mitigated every hour.\n\nIn the final quarter of 2025, Hong Kong jumped 12 places, making it the second most DDoS’d place on earth. The United Kingdom also leapt by an astonishing 36 places, making it the sixth most-attacked place.\n\nInfected Android TVs — part of the Aisuru-Kimwolf botnet — bombarded Cloudflare’s network with hyper-volumetric HTTP DDoS attacks, while Telcos emerged as the most-attacked industry.\n\n2025 saw a huge spike in DDoS attacks\nIn 2025, the total number of DDoS attacks more than doubled to an incredible 47.1 million. Such attacks have soared in recent years: The number of DDoS attacks spiked 236% between 2023 and 2025.\nIn 2025, Cloudflare mitigated an average of 5,376 DDoS attacks every hour — of these, 3,925 were network-layer DDoS attacks and 1,451 were HTTP DDoS attacks. \nNetwork-layer DDoS attacks more than tripled in 2025\nThe most substantial growth was in network-layer DDoS attacks, which more than tripled year over year. Cloudflare mitigated 34.4 million network-layer DDoS attacks in 2025, compared to 11.4 million in 2024.\nA substantial portion of the network-layer attacks — approximately 13.5 million — targeted global Internet infrastructure protected by Cloudflare Magic Transit and Cloudflare’s infrastructure directly, as part of an 18-day DDoS campaign in the first quarter of 2025. Of these attacks, 6.9 million targeted Magic Transit customers while the remaining 6.6 million targeted Cloudflare directly. \nThis assault was a multi-vector DDoS campaign comprising SYN flood attacks, Mirai-generated DDoS attacks, and SSDP amplification attacks to name a few. Our systems detected and mitigated these attacks automatically. In fact, we only discovered the campaign while preparing our DDoS threat report for 2025 Q1 — an example of how effective Cloudflare’s DDoS mitigation is!\nIn the final quarter of 2025, the number of DDoS attacks grew by 31% over the previous quarter and 58% over 2024. Network-layer DDoS attacks fueled that growth. In 2025 Q4, network-layer DDoS attacks accounted for 78% of all DDoS attacks. The amount of HTTP DDoS attacks remained the same, but surged in their size to rates that we haven’t seen since the HTTP/2 Rapid Reset DDoS campaign in 2023. These recent surges were launched by the Aisuru-Kimwolf botnet, which we will cover in the next section. \n“The Night Before Christmas” DDoS campaign\nOn Friday, December 19, 2025, the Aisuru-Kimwolf botnet began bombarding Cloudflare infrastructure and Cloudflare customers with hyper-volumetric DDoS attacks. What was new in this campaign was its size: The botnet used hyper-volumetric HTTP DDoS attacks exceeding rates of 20 million requests per second (Mrps).\n\n\n          \n          \n          \nThe Aisuru-Kimwolf botnet is a massive collection of malware-infected devices, primarily Android TVs. The botnet comprises an estimated 1-4 million infected hosts. It is capable of launching DDoS attacks that can cripple critical infrastructure, crash most legacy cloud-based DDoS protection solutions, and even disrupt the connectivity of entire nations.\nThroughout the campaign, Cloudflare’s autonomous DDoS defense systems detected and mitigated all of the attacks: 384 packet-intensive attacks, 329 bit-intensive attacks, and 189 request-intensive attacks, for a total of 902 hyper-volumetric DDoS attacks, averaging 53 attacks a day.\nThe average size of the hyper-volumetric DDoS attacks during the campaign were 3 Bpps, 4 Tbps, and 54 Mrps. The maximum rates recorded during the campaign were 9 Bpps, 24 Tbps, and 205 Mrps.\nTo put that in context, the scale of a 205 Mrps DDoS attack is comparable to the combined populations of the UK, Germany, and Spain all simultaneously typing a website address and then hitting 'enter’ at the same second.\nWhile highly dramatic, The Night Before Christmas campaign accounted for only a small portion of the hyper-volumetric DDoS attacks we saw throughout the year.\nHyper-volumetric DDoS attacks\nThroughout 2025, Cloudflare observed a continuous increase in hyper-volumetric DDoS attacks. In 2025 Q4, hyper-volumetric attacks increased by 40% compared to the previous quarter.\nAs the number of attacks increased over the course of 2025, the size of the attacks increased as well, growing by over 700% compared to the large attacks seen in late 2024, with one reaching 31.4 Tbps in a DDoS attack that lasted just 35 seconds. The graph below portrays the rapid growth in DDoS attack sizes as seen and blocked by Cloudflare — each one a world record, i.e. the largest ever disclosed publicly by any company at the time.\nLike all of the other attacks, the 31.4 Tbps DDoS attack was detected and mitigated automatically by Cloudflare’s autonomous DDoS defense, which was able to adapt and quickly lock on to botnets such as Aisuru-Kimwolf.\nMost of the hyper-volumetric DDoS attacks targeted Cloudflare customers in the Telecommunications, Service Providers and Carriers industry. Cloudflare customers in the Gaming industry and customers providing Generative AI services were also heavily targeted. Lastly, Cloudflare’s own infrastructure itself was targeted by multiple attack vectors such as HTTP floods, DNS attacks and UDP flood.\nMost-attacked industries\nWhen analyzing DDoS attacks of all sizes, the Telecommunications, Service Providers and Carriers industry was also the most targeted. Previously, the Information Technology & Services industry held that unlucky title.\nThe Gambling & Casinos and Gaming industries ranked third and fourth, respectively. The quarter’s biggest changes in the top 10 were the Computer Software and Business Services industries, which both climbed several spots. \nThe most-attacked industries are defined by their role as critical infrastructure, a central backbone for other businesses, or their immediate, high-stakes financial sensitivity to service interruption and latency.\nMost-attacked locations\nThe DDoS landscape saw both predictable stability and dramatic shifts among the world's most-attacked locations. Targets like China, Germany, Brazil, and the United States were the top five, demonstrating persistent appeal for attackers. \nHong Kong made a significant move, jumping twelve spots to land at number two. However, the bigger story was the meteoric rise of the United Kingdom, which surged an astonishing 36 places this quarter, making it the sixth most-attacked location.  \nVietnam held its place as the seventh most-attacked location, followed by Azerbaijan in eighth, India in ninth, and Singapore as number ten.\nTop attack sources\nBangladesh dethroned Indonesia as the largest source of DDoS attacks in the fourth quarter of 2025. Indonesia dropped to the third spot, after spending a year as the top source of DDoS attacks. Ecuador also jumped two spots, making it the second-largest source.\nNotably, Argentina soared an incredible twenty places, making it the fourth-largest source of DDoS attacks. Hong Kong rose three places, taking fifth place. Ukraine came in sixth place, followed by Vietnam, Taiwan, Singapore, and Peru.\nTop source networks\nThe top 10 list of attack source networks reads like a list of Internet giants, revealing a fascinating story about the anatomy of modern DDoS attacks. The common thread is clear: Threat actors are leveraging the world's most accessible and powerful network infrastructure — primarily large, public-facing services. \nWe see most DDoS attacks coming from IP addresses associated with Cloud Computing Platforms and Cloud Infrastructure Providers, including DigitalOcean (AS 14061), Microsoft (AS 8075), Tencent (AS 132203), Oracle (AS 31898), and Hetzner (AS 24940). This demonstrates the strong link between easily-provisioned virtual machines and high-volume attacks. These cloud sources, heavily concentrated in the United States, are closely followed by a significant presence of attacks coming from IP addresses associated with traditional Telecommunications Providers (Telcos). These Telcos, primarily from the Asia-Pacific region (including Vietnam, China, Malaysia, and Taiwan), round out the rest of the top 10.\nThis geographic and organizational diversity confirms a two-pronged attack reality: While the sheer scale of the highest-ranking sources often originates from global cloud hubs, the problem is truly worldwide, routed through the Internet's most critical pathways from across the globe. In many DDoS attacks, we see thousands of various source ASNs, highlighting the truly global distribution of botnet nodes.\nTo help hosting providers, cloud computing platforms and Internet service providers identify and take down the abusive IP addresses/accounts that launch these attacks, we leverage Cloudflare’s unique vantage point on DDoS attacks to provide a free DDoS Botnet Threat Feed for Service Providers. \nOver 800 networks worldwide have signed up for this feed, and we’ve already seen great collaboration across the community to take down botnet nodes.\nHelping defend the Internet\nDDoS attacks are rapidly growing in sophistication and size, surpassing what was previously imaginable. This evolving threat landscape presents a significant challenge for many organizations to keep pace. Organizations currently relying on on-premise mitigation appliances or on-demand scrubbing centers may benefit from re-evaluating their defense strategy.\nCloudflare is dedicated to offering free, unmetered DDoS protection to all its customers, regardless of the size, duration, or volume of attacks, leveraging its vast global network and autonomous DDoS mitigation systems.\nAbout Cloudforce One\nDriven by a mission to help defend the Internet, Cloudforce One leverages telemetry from Cloudflare’s global network — which protects approximately 20% of the web — to drive threat research and operational response, protecting critical systems for millions of organizations worldwide.","dc:creator":"Omer Yoachimik","content":" The number of DDoS attacks more than doubled in 2025. The network layer is under particular threat as hyper-volumetric attacks grew 700%. ","contentSnippet":"The number of DDoS attacks more than doubled in 2025. The network layer is under particular threat as hyper-volumetric attacks grew 700%.","guid":"4RtH1xA4p0tuaD6gFL46Pf","categories":["DDoS Reports","DDoS","Cloudforce One","Security","Advanced DDoS","AI"],"isoDate":"2026-02-05T14:00:00.000Z"},{"creator":"Frank Chen","title":"Improve global upload performance with R2 Local Uploads","link":"https://blog.cloudflare.com/r2-local-uploads/","pubDate":"Tue, 03 Feb 2026 14:00:00 GMT","content:encoded":" <p>Today, we are launching<b> Local Uploads</b> for R2 in <b>open beta</b>. With <a href=\"https://developers.cloudflare.com/r2/buckets/local-uploads/\"><u>Local Uploads</u></a> enabled, object data is automatically written to a storage location close to the client first, then asynchronously copied to where the bucket lives. The data is immediately accessible and stays <a href=\"https://developers.cloudflare.com/r2/reference/consistency/\"><u>strongly consistent</u></a>. Uploads get faster, and data feels global.</p><p>For many applications, performance needs to be global. Users uploading media content from different regions, for example, or devices sending logs and telemetry from all around the world. But your data has to live somewhere, and that means uploads from far away have to travel the full distance to reach your bucket. </p><p><a href=\"https://workers.cloudflare.com/product/r2/\"><u>R2</u></a> is <a href=\"https://developers.cloudflare.com/r2/\"><u>object storage</u></a> built on Cloudflare's global network. Out of the box, it automatically caches object data globally for fast reads anywhere — all while retaining strong consistency and zero <a href=\"https://www.cloudflare.com/learning/cloud/what-are-data-egress-fees/\"><u>egress fees</u></a>. This happens behind the scenes whether you're using the S3 API, Workers Bindings, or plain HTTP. And now with Local Uploads, both reads and writes can be fast from anywhere in the world.</p><p>Try it yourself <a href=\"https://local-uploads.r2-demo.workers.dev/\"><u>in this demo</u></a> to see the benefits of Local Uploads.</p><p>Ready to try it? Enable Local Uploads in the <a href=\"https://dash.cloudflare.com/?to=/:account/r2/overview\"><u>Cloudflare Dashboard</u></a> under your bucket's settings, or with a single Wrangler command on an existing bucket.</p>\n            <pre><code>npx wrangler r2 bucket local-uploads enable [BUCKET]</code></pre>\n            \n    <div>\n      <h2>75% lower total request duration for global uploads</h2>\n      <a href=\"#75-lower-total-request-duration-for-global-uploads\">\n        \n      </a>\n    </div>\n    <p><a href=\"https://developers.cloudflare.com/r2/buckets/local-uploads\"><u>Local Uploads</u></a> makes upload requests (i.e. PutObject, UploadPart) faster. In both our private beta tests with customers and our synthetic benchmarks, we saw up to 75% reduction in Time to Last Byte (TTLB) when upload requests are made in a different region than the bucket. In these results, TTLB is measured from when R2 receives the upload request to when R2 returns a 200 response.</p><p>In our synthetic tests, we measured the impact of Local Uploads by using a synthetic workload to simulate a cross-region upload workflow. We deployed a test client in Western North America and configured an R2 bucket with a <a href=\"https://developers.cloudflare.com/r2/reference/data-location/\"><u>location hint</u></a> for Asia-Pacific. The client performed around 20 PutObject requests per second over 30 minutes to upload objects of 5 MB size. </p><p>The following graph compares the p50 (or median) TTLB metrics for these requests, showing the difference in upload request duration — first without Local Uploads (TTLB around 2s), and then with Local Uploads enabled (TTLB around 500ms): </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4uvSdPwflyjHohwLQvOKsu/4b82637a5ac29ceee0fc37e04ab0107f/image1.png\" />\n          </figure>\n    <div>\n      <h2>How it works: The distance problem</h2>\n      <a href=\"#how-it-works-the-distance-problem\">\n        \n      </a>\n    </div>\n    <p>To understand how Local Uploads can improve upload requests, let’s first take a look at <a href=\"https://developers.cloudflare.com/r2/how-r2-works/\"><u>how R2 works</u></a>. R2's architecture is composed of multiple components including:</p><ul><li><p><b>R2 Gateway Worker: </b>The entry point for all API requests that handles authentication and routing logic. It is deployed across Cloudflare's global network via <a href=\"https://developers.cloudflare.com/workers/\"><u>Cloudflare Workers</u></a>.</p></li><li><p><b>Durable Object Metadata Service: </b>A distributed layer built on <a href=\"https://developers.cloudflare.com/durable-objects/\"><u>Durable Objects</u></a> used to store and manage object metadata (e.g. object key, checksum).</p></li><li><p><b>Distributed Storage Infrastructure: </b>The underlying infrastructure that persistently stores encrypted object data.</p></li></ul><p>Without Local Uploads, here’s what happens when you upload objects to your bucket: The request is first received by the R2 Gateway, close to the user, where it is authenticated. Then, as the client streams bytes of the object data, the data is encrypted and written into the storage infrastructure in the region where the bucket is placed. When this is completed, the Gateway reaches out to the Metadata Service to publish the object metadata, and it returns a success response back to the client after it is committed.</p><p>If the client and the bucket are in separate regions, more variability can be introduced in the process of uploading bytes of the object data, due to the longer distance that the request must travel. This could result in slower or less reliable uploads. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6toAZ6JSHPv2jgntdyCOvr/704f6837d2705f18a0e5b8554994cb7a/image9.png\" />\n          </figure><p><sup>A client uploading from Eastern North America to a bucket in Eastern Europe without Local Uploads enabled. </sup></p><p>Now, when you make an upload request to a bucket with Local Uploads enabled, there are two cases that are handled: </p><ol><li><p>The client and the bucket region are in the <b>same</b> region</p></li><li><p>The client and the bucket region are in <b>different</b> regions</p></li></ol><p>In the first case, R2 follows the regular flow, where object data is written to the storage infrastructure for your bucket. In the second case, R2 writes to the storage infrastructure located in the client region while still publishing to the object metadata to the region of the bucket.</p><p>Importantly, the object is immediately accessible after the initial write completes. It remains accessible throughout the entire replication process — there's <b>no</b> <b>waiting period</b> for background replication to finish before the object can be read.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/33oUAdlGF8cWOeQhha6Ocy/68537e503f1ec8d1dd080db363f97dc3/image3.png\" />\n          </figure><p><sup>A client uploading from Eastern North America to a bucket in Eastern Europe with Local Uploads enabled. </sup></p><p>Note that this is for non-jurisdiction restricted buckets, and Local Uploads are not available for buckets with jurisdiction restriction (e.g. EU, FedRAMP) enabled.</p>\n    <div>\n      <h2>When to use Local Uploads</h2>\n      <a href=\"#when-to-use-local-uploads\">\n        \n      </a>\n    </div>\n    <p>Local uploads are built for workloads that receive a lot of upload requests originating from different geographic regions than where your bucket is located. This feature is ideal when:</p><ul><li><p>Your users are globally distributed</p></li><li><p>Upload performance and reliability is critical to your application</p></li><li><p>You want to optimize write performance without changing your bucket's primary location</p></li></ul><p>To understand the geographic distribution of where your read and write requests are initiated, you can visit the <a href=\"https://dash.cloudflare.com/?to=/:account/r2/overview\"><u>Cloudflare Dashboard</u></a>, and go to your R2 bucket’s Metrics page and view the Request Distribution by Region graph. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6SJ9UYY3RADryXmnT0J3Vq/9b26c948e925a705387a64c24a1dd7e3/image7.png\" />\n          </figure>\n    <div>\n      <h2>How we built Local Uploads</h2>\n      <a href=\"#how-we-built-local-uploads\">\n        \n      </a>\n    </div>\n    <p>With Local Uploads, object data is written close to the client and then copied to the bucket's region in the background. We call this copy job a replication task.</p><p>Given these replication tasks, we needed an asynchronous processing component for them, which tends to be a great use case for <a href=\"https://developers.cloudflare.com/queues/\"><u>Cloudflare Queues</u></a>. Queues allow us to control the rate at which we process replication tasks, and it provides built-in failure handling capabilities like <a href=\"https://developers.cloudflare.com/queues/configuration/batching-retries/\"><u>retries</u></a> and <a href=\"https://developers.cloudflare.com/queues/configuration/dead-letter-queues/\"><u>dead letter queues</u></a>. In this case, R2 shards replication tasks across multiple queues per storage region.</p>\n    <div>\n      <h3>Publishing metadata and scheduling replication</h3>\n      <a href=\"#publishing-metadata-and-scheduling-replication\">\n        \n      </a>\n    </div>\n    <p>When publishing the metadata of an object with Local Uploads enabled, we perform three operations atomically:</p><ol><li><p>Store the object metadata</p></li><li><p>Create a pending replica key that tracks which replications still need to happen</p></li><li><p>Create a replication task marker keyed by timestamp, which controls when the task should be sent to the queue</p></li></ol><p>The pending replica key contains the full replication plan: the number of replication tasks, which source location to read from, which destination location to write to, the replication mode and priority, and whether the source should be deleted after successful replication.</p><p>This gives us flexibility in how we move an object's data. For example, moving data across long geographical distances is expensive. We could try to move all the replicas as fast as possible by processing them in parallel, but this would incur greater cost and pressure the network infrastructure. Instead, we minimize the number of cross-regional data movements by first creating one replica in the target bucket region, and then use this local copy to create additional replicas within the bucket region.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2rCuA2zXR4ltZJsiNDBHd7/ae388f13ea27922158b27f429080c69c/image6.png\" />\n          </figure><p>A background process periodically scans the replication task markers and sends them to one of the queues associated with the destination storage region. The markers guarantee at-least-once delivery to the queue — if enqueueing fails or the process crashes, the marker persists and the task will be retried on the next scan. This also allows us to process replications at different times and enqueue only valid tasks. Once a replication task reaches a queue, it is ready to be processed.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5G4STZSp67TnKhehzCqFMv/445a0c74ba7f4bc5dd3de04eb7aa1257/image4.png\" />\n          </figure>\n    <div>\n      <h3>Asynchronous replication: Pull model</h3>\n      <a href=\"#asynchronous-replication-pull-model\">\n        \n      </a>\n    </div>\n    <p>For the queue consumer, we chose a pull model where a centralized polling service consumes tasks from the regional queues and dispatches them to the Gateway Worker for execution.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2p6SHkqO1tT7wxdhPJCFCr/86f219af85e332813ede2eb95a3810d8/image2.png\" />\n          </figure><p>Here's how it works:</p><ol><li><p>Polling service pulls from a regional queue: The consumer service polls the regional queue for replication tasks. It then batches the tasks to create uniform batch sizes based on the amount of data to be moved.</p></li><li><p>Polling service dispatches to Gateway Worker: The consumer service sends the replication job to the Gateway Worker.</p></li><li><p>Gateway Worker executes replication: The worker reads object data from the source location, writes it to the destination, and updates metadata in the Durable Object, optionally marking the source location to be garbage collected.</p></li><li><p>Gateway Worker reports result: On completion, the worker returns the result to the poller, which acknowledges the task to the queue as completed or failed.</p></li></ol><p>By using this pull model approach, we ensure that the replication process remains stable and efficient. The service can dynamically adjust its pace based on real-time system health, guaranteeing that data is safely replicated across regions.</p>\n    <div>\n      <h2>Try it out</h2>\n      <a href=\"#try-it-out\">\n        \n      </a>\n    </div>\n    <p>Local Uploads is available now in open beta. There is <b>no additional cost</b> to enable Local Uploads. Upload requests made with this feature enabled incur the standard <a href=\"https://developers.cloudflare.com/r2/pricing/\"><u>Class A operation costs</u></a>, same as upload requests made without Local Uploads.</p><p>To get started, visit the <a href=\"https://dash.cloudflare.com/?to=/:account/r2/overview\"><u>Cloudflare Dashboard</u></a> under your bucket's settings and look for the Local Uploads card to enable, or simply run the following command using Wrangler to enable Local Uploads on a bucket.</p>\n            <pre><code>npx wrangler r2 bucket local-uploads enable [BUCKET]</code></pre>\n            <p>Enabling Local Uploads on a bucket is seamless: existing uploads will complete as expected and there’s no interruption to traffic.</p><p>For more information, refer to the <a href=\"https://developers.cloudflare.com/r2/buckets/local-uploads/\"><u>Local Uploads documentation</u></a>. If you have questions or want to share feedback, join the discussion on our <a href=\"https://discord.gg/cloudflaredev\"><u>Developer Discord</u></a>.</p> ","content:encodedSnippet":"Today, we are launching Local Uploads for R2 in open beta. With Local Uploads enabled, object data is automatically written to a storage location close to the client first, then asynchronously copied to where the bucket lives. The data is immediately accessible and stays strongly consistent. Uploads get faster, and data feels global.\nFor many applications, performance needs to be global. Users uploading media content from different regions, for example, or devices sending logs and telemetry from all around the world. But your data has to live somewhere, and that means uploads from far away have to travel the full distance to reach your bucket. \nR2 is object storage built on Cloudflare's global network. Out of the box, it automatically caches object data globally for fast reads anywhere — all while retaining strong consistency and zero egress fees. This happens behind the scenes whether you're using the S3 API, Workers Bindings, or plain HTTP. And now with Local Uploads, both reads and writes can be fast from anywhere in the world.\nTry it yourself in this demo to see the benefits of Local Uploads.\nReady to try it? Enable Local Uploads in the Cloudflare Dashboard under your bucket's settings, or with a single Wrangler command on an existing bucket.\nnpx wrangler r2 bucket local-uploads enable [BUCKET]\n75% lower total request duration for global uploads\nLocal Uploads makes upload requests (i.e. PutObject, UploadPart) faster. In both our private beta tests with customers and our synthetic benchmarks, we saw up to 75% reduction in Time to Last Byte (TTLB) when upload requests are made in a different region than the bucket. In these results, TTLB is measured from when R2 receives the upload request to when R2 returns a 200 response.\nIn our synthetic tests, we measured the impact of Local Uploads by using a synthetic workload to simulate a cross-region upload workflow. We deployed a test client in Western North America and configured an R2 bucket with a location hint for Asia-Pacific. The client performed around 20 PutObject requests per second over 30 minutes to upload objects of 5 MB size. \nThe following graph compares the p50 (or median) TTLB metrics for these requests, showing the difference in upload request duration — first without Local Uploads (TTLB around 2s), and then with Local Uploads enabled (TTLB around 500ms): \nHow it works: The distance problem\nTo understand how Local Uploads can improve upload requests, let’s first take a look at how R2 works. R2's architecture is composed of multiple components including:\n\nR2 Gateway Worker: The entry point for all API requests that handles authentication and routing logic. It is deployed across Cloudflare's global network via Cloudflare Workers.\n\nDurable Object Metadata Service: A distributed layer built on Durable Objects used to store and manage object metadata (e.g. object key, checksum).\n\nDistributed Storage Infrastructure: The underlying infrastructure that persistently stores encrypted object data.\n\nWithout Local Uploads, here’s what happens when you upload objects to your bucket: The request is first received by the R2 Gateway, close to the user, where it is authenticated. Then, as the client streams bytes of the object data, the data is encrypted and written into the storage infrastructure in the region where the bucket is placed. When this is completed, the Gateway reaches out to the Metadata Service to publish the object metadata, and it returns a success response back to the client after it is committed.\nIf the client and the bucket are in separate regions, more variability can be introduced in the process of uploading bytes of the object data, due to the longer distance that the request must travel. This could result in slower or less reliable uploads. \nA client uploading from Eastern North America to a bucket in Eastern Europe without Local Uploads enabled. \nNow, when you make an upload request to a bucket with Local Uploads enabled, there are two cases that are handled: \n\nThe client and the bucket region are in the same region\n\nThe client and the bucket region are in different regions\n\nIn the first case, R2 follows the regular flow, where object data is written to the storage infrastructure for your bucket. In the second case, R2 writes to the storage infrastructure located in the client region while still publishing to the object metadata to the region of the bucket.\nImportantly, the object is immediately accessible after the initial write completes. It remains accessible throughout the entire replication process — there's no waiting period for background replication to finish before the object can be read.\nA client uploading from Eastern North America to a bucket in Eastern Europe with Local Uploads enabled. \nNote that this is for non-jurisdiction restricted buckets, and Local Uploads are not available for buckets with jurisdiction restriction (e.g. EU, FedRAMP) enabled.\nWhen to use Local Uploads\nLocal uploads are built for workloads that receive a lot of upload requests originating from different geographic regions than where your bucket is located. This feature is ideal when:\n\nYour users are globally distributed\n\nUpload performance and reliability is critical to your application\n\nYou want to optimize write performance without changing your bucket's primary location\n\nTo understand the geographic distribution of where your read and write requests are initiated, you can visit the Cloudflare Dashboard, and go to your R2 bucket’s Metrics page and view the Request Distribution by Region graph. \nHow we built Local Uploads\nWith Local Uploads, object data is written close to the client and then copied to the bucket's region in the background. We call this copy job a replication task.\nGiven these replication tasks, we needed an asynchronous processing component for them, which tends to be a great use case for Cloudflare Queues. Queues allow us to control the rate at which we process replication tasks, and it provides built-in failure handling capabilities like retries and dead letter queues. In this case, R2 shards replication tasks across multiple queues per storage region.\nPublishing metadata and scheduling replication\nWhen publishing the metadata of an object with Local Uploads enabled, we perform three operations atomically:\n\nStore the object metadata\n\nCreate a pending replica key that tracks which replications still need to happen\n\nCreate a replication task marker keyed by timestamp, which controls when the task should be sent to the queue\n\nThe pending replica key contains the full replication plan: the number of replication tasks, which source location to read from, which destination location to write to, the replication mode and priority, and whether the source should be deleted after successful replication.\nThis gives us flexibility in how we move an object's data. For example, moving data across long geographical distances is expensive. We could try to move all the replicas as fast as possible by processing them in parallel, but this would incur greater cost and pressure the network infrastructure. Instead, we minimize the number of cross-regional data movements by first creating one replica in the target bucket region, and then use this local copy to create additional replicas within the bucket region.\nA background process periodically scans the replication task markers and sends them to one of the queues associated with the destination storage region. The markers guarantee at-least-once delivery to the queue — if enqueueing fails or the process crashes, the marker persists and the task will be retried on the next scan. This also allows us to process replications at different times and enqueue only valid tasks. Once a replication task reaches a queue, it is ready to be processed.\nAsynchronous replication: Pull model\nFor the queue consumer, we chose a pull model where a centralized polling service consumes tasks from the regional queues and dispatches them to the Gateway Worker for execution.\nHere's how it works:\n\nPolling service pulls from a regional queue: The consumer service polls the regional queue for replication tasks. It then batches the tasks to create uniform batch sizes based on the amount of data to be moved.\n\nPolling service dispatches to Gateway Worker: The consumer service sends the replication job to the Gateway Worker.\n\nGateway Worker executes replication: The worker reads object data from the source location, writes it to the destination, and updates metadata in the Durable Object, optionally marking the source location to be garbage collected.\n\nGateway Worker reports result: On completion, the worker returns the result to the poller, which acknowledges the task to the queue as completed or failed.\n\nBy using this pull model approach, we ensure that the replication process remains stable and efficient. The service can dynamically adjust its pace based on real-time system health, guaranteeing that data is safely replicated across regions.\nTry it out\nLocal Uploads is available now in open beta. There is no additional cost to enable Local Uploads. Upload requests made with this feature enabled incur the standard Class A operation costs, same as upload requests made without Local Uploads.\nTo get started, visit the Cloudflare Dashboard under your bucket's settings and look for the Local Uploads card to enable, or simply run the following command using Wrangler to enable Local Uploads on a bucket.\nnpx wrangler r2 bucket local-uploads enable [BUCKET]\nEnabling Local Uploads on a bucket is seamless: existing uploads will complete as expected and there’s no interruption to traffic.\nFor more information, refer to the Local Uploads documentation. If you have questions or want to share feedback, join the discussion on our Developer Discord.","dc:creator":"Frank Chen","content":" Local Uploads on R2 reduces request duration for uploads by up to 75%. It writes object data to a nearby location and asynchronously copies it to your bucket, all while data is available immediately.  ","contentSnippet":"Local Uploads on R2 reduces request duration for uploads by up to 75%. It writes object data to a nearby location and asynchronously copies it to your bucket, all while data is available immediately.","guid":"453lZMuYluqGqfRKADhf9K","categories":["R2","Performance","Storage"],"isoDate":"2026-02-03T14:00:00.000Z"},{"creator":"Maria Palmieri","title":"Google’s AI advantage: why crawler separation is the only path to a fair Internet","link":"https://blog.cloudflare.com/uk-google-ai-crawler-policy/","pubDate":"Fri, 30 Jan 2026 17:01:04 GMT","content:encoded":" <p>Earlier this week, the UK’s Competition and Markets Authority (CMA) <a href=\"https://www.gov.uk/government/news/cma-proposes-package-of-measures-to-improve-google-search-services-in-uk\"><u>opened its consultation</u></a> on a package of proposed conduct requirements for Google. The consultation invites comments on the proposed requirements before the CMA imposes any final measures. These new rules aim to address the lack of choice and transparency that publishers (broadly defined as “any party that makes content available on the web”) face over how Google uses search to fuel its generative AI services and features. These are the first consultations on conduct requirements launched under the digital markets competition regime in the UK. </p><p>We welcome the CMA’s recognition that publishers need a fairer deal and believe the proposed rules are a step into the right direction. Publishers should be entitled to have access to tools that enable them to control the inclusion of their content in generative AI services, and AI companies should have a level playing field on which to compete. </p><p>But we believe the CMA has not gone far enough and should do more to safeguard the UK’s creative sector and foster healthy competition in the market for generative and agentic AI. </p>\n    <div>\n      <h2>CMA designation of Google as having Strategic Market Status </h2>\n      <a href=\"#cma-designation-of-google-as-having-strategic-market-status\">\n        \n      </a>\n    </div>\n    <p>In January 2025, the UK’s regulatory landscape underwent a significant legal shift with the implementation of the Digital Markets, Competition and Consumers Act 2024 (DMCC). Rather than relying on antitrust investigations to address risks to competition, the CMA can now designate firms as having Strategic Market Status (SMS) when they hold substantial, entrenched market power. This designation allows for targeted CMA interventions in digital markets, such as imposing detailed conduct requirements, to improve competition. </p><p>In October 2025, the CMA <a href=\"https://assets.publishing.service.gov.uk/media/68e8b643cf65bd04bad76724/Final_decision_-_strategic_market_status_investigation_into_Google_s_general_search_services.pdf\"><u>designated Google</u></a> as having SMS in general search and search advertising, given its 90 percent share of the search market in the UK. Crucially, this designation encompasses AI Overviews and AI Mode, with the CMA now having the authority to impose conduct requirements on Google’s search ecosystem. Final requirements imposed by the CMA are not merely suggestions but legally enforceable rules that can relate specifically to AI crawling with significant sanctions to ensure Google operates fairly. </p>\n    <div>\n      <h2>Publishers need a meaningful way to opt out of Google’s use of their content for generative AI</h2>\n      <a href=\"#publishers-need-a-meaningful-way-to-opt-out-of-googles-use-of-their-content-for-generative-ai\">\n        \n      </a>\n    </div>\n    <p>The CMA’s designation could not be more timely. As we’ve <a href=\"https://blog.cloudflare.com/building-a-better-internet-with-responsible-ai-bot-principles/\"><u>said before</u></a>, we are indisputably in a time when the Internet needs clear “rules of the road” for AI crawling behavior. </p><p>As the CMA rightly <a href=\"https://assets.publishing.service.gov.uk/media/6979d0bf75d44370965520a0/Publisher_conduct_requirement.pdf\"><u>states</u></a>, “publishers have no realistic option but to allow their content to be crawled for Google’s general search because of the market power Google holds in general search. However, Google currently uses that content in both its search generative AI features and in its broader generative AI services.” </p><p>In other words: the same content that Google scrapes for search indexing is also used for inference/grounding purposes, like AI Overviews and AI Mode, which rely on fetching live information from the Internet in response to real-time user queries. And that creates a big problem for publishers—and for competition.</p><p>Because publishers cannot afford to disallow or block Googlebot, Google’s search crawler, on their website, they have to accept that their content will be used in generative AI applications such as AI Overviews and AI Mode within Google Search that <a href=\"https://blog.cloudflare.com/crawlers-click-ai-bots-training/\"><u>return very little, if any, traffic to their websites</u></a>. This undermines the ad-supported business models that have sustained digital publishing for decades, given the critical role of Google Search in driving human traffic to online advertising. It also means that Google’s generative AI applications enter into direct competition with publishers by reproducing their content, most often without attribution or compensation. </p><p>Publishers’ reluctance to block Google because of its dominance in search gives Google an unfair competitive advantage in the market for generative and agentic AI. Unlike other AI bot operators, Google can use its search crawler to gather data for a variety of AI functions with little fear that its access will be restricted. It has minimal incentive to pay publishers for that data, which it is already getting for free. </p><p>This prevents the emergence of a well-functioning marketplace where AI developers negotiate fair value for content. Instead, other AI companies are disincentivized from coming to the table, as they are structurally disadvantaged by a system that allows one dominant player to bypass compensation entirely. As the CMA itself <a href=\"https://assets.publishing.service.gov.uk/media/6979d05275d443709655209f/Introduction_to_the_consultation.pdf\"><u>recognizes</u></a>, \"[b]y not providing sufficient control over how this content is used, Google can limit the ability of publishers to monetise their content, while accessing content for AI-generated results in a way that its competitors cannot match”. </p>\n    <div>\n      <h2>Google’s advantage</h2>\n      <a href=\"#googles-advantage\">\n        \n      </a>\n    </div>\n    <p>Cloudflare data validates the concern about Google’s competitive advantage. Based on our data, Googlebot sees significantly more Internet content than its closest peers. </p><p>Over an observed period of two months, Googlebot successfully accessed individual pages almost two times more than ClaudeBot and GPTBot, three times more than Meta-ExternalAgent, and more than three times more than Bingbot. The difference was even more extreme for other popular AI crawlers: for instance, Googlebot saw 167 times more unique pages than PerplexityBot. Out of the sampled unique URLs using our network that we observed over the last two months, Googlebot crawled roughly 8%.</p><p><b>In rounded multiple terms, Googlebot sees:</b></p><ul><li><p>vs. ~1.70x the amount of unique URLs seen by ClaudeBot;</p></li><li><p>vs. ~1.76x the amount of unique URLs seen by GPTBot;</p></li><li><p>vs. ~2.99x the amount of unique URLs by Meta-ExternalAgent;</p></li><li><p>vs. ~3.26x the amount of unique URLs seen by Bingbot;</p></li><li><p>vs. ~5.09x the amount of unique URLs seen by Amazonbot;</p></li><li><p>vs. ~14.87x the amount of unique URLs seen by Applebot;</p></li><li><p>vs. ~23.73x the amount of unique URLs seen by Bytespider;</p></li><li><p>vs. ~166.98x the amount of unique URLs seen by PerplexityBot;</p></li><li><p>vs. ~714.48x the amount of unique URLs seen by CCBot; and</p></li><li><p>vs: ~1801.97x the amount of unique URLs seen by archive.org_bot.</p></li></ul><p>Googlebot also stands out in other Cloudflare datasets.  </p><p>Even though it ranks as the most active bot by overall traffic, publishers are far less likely to disallow or block Googlebot in their <a href=\"https://www.cloudflare.com/learning/bots/what-is-robots-txt/\"><u>robots.txt file</u></a> compared to other crawlers. This is likely due to its importance in driving human traffic to their content—and, as a result, ad revenue—through search. </p><p>As shown below, almost no website explicitly disallows the dual-purpose Googlebot in full, reflecting how important this bot is to driving traffic via search referrals. (Note that partial disallows often impact certain parts of a website that are irrelevant for search engine optimization, or SEO, such as login endpoints.)</p>\n<p>\nRobots.txt merely allows the expression of crawling preferences; it is not an enforcement mechanism. Publishers rely on “good bots” to comply. To manage crawler access to their sites more effectively—and independently of a given bot’s compliance—publishers can set up a Web Application Firewall (WAF) with specific rules, technically preventing undesired crawlers from accessing their sites. Following the same logic as with robots.txt above, we would expect websites to block mostly other AI crawlers but not Googlebot. </p><p>Indeed, when comparing the numbers for customers using <a href=\"https://www.cloudflare.com/lp/pg-ai-crawl-control/\"><u>AI Crawl Control</u></a>, Cloudflare’s own <a href=\"https://developers.cloudflare.com/ai-crawl-control/configuration/ai-crawl-control-with-waf/\"><u>AI crawler blocking tool</u></a> that is integrated in our Application Security suite, between July 2025 and January 2026, one can see that the number of websites actively blocking other popular AI crawlers (e.g., GPTBot, Claudebot), was nearly seven times as high as the number of websites that blocked Googlebot and Bingbot. (Like Googlebot, Bingbot combines search and AI crawling and drives traffic to these sites, but given its small market share in search, its impact is less significant.)</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/344ATKpYmJHsSRlEtxQen5/2fc5da1211b4fd0189e026f0ec19548f/BLOG-3170_3.png\" />\n          </figure><p>So we agree with the CMA on the problem statement. But how can publishers be enabled to effectively opt out of Google using their content for its generative AI applications? We share the CMA’s conclusion that “in order to be able to make meaningful decisions about how Google uses their Search Content, (...) publishers need the ability effectively to opt their Search Content out of both Google’s search generative AI features and Google’s broader generative AI services.” </p><p>But we’re concerned that the CMA’s proposal is insufficient.</p>\n    <div>\n      <h2>CMA’s proposed publisher conduct requirements</h2>\n      <a href=\"#cmas-proposed-publisher-conduct-requirements\">\n        \n      </a>\n    </div>\n    <p>On January 28, 2026, the CMA published four sets of proposed conduct requirements for Google, including <a href=\"https://assets.publishing.service.gov.uk/media/6979ceae75d443709655209c/Publisher_conduct_requirement.pdf\"><u>conduct requirements related to publishers</u></a>. According to the CMA, the proposed publisher rules are designed to address concerns that publishers (1) lack sufficient choice over how Google uses their content in its AI-generated responses, (2) have limited transparency into Google’s use of that content, and (3) do not get effective attribution for Google’s use of their content. The CMA recognized the importance of these concerns because of the role that Google search plays in finding content online. </p><p>The conduct requirements would mandate Google grant publishers <a href=\"https://assets.publishing.service.gov.uk/media/6979d05275d443709655209f/Introduction_to_the_consultation.pdf\"><u>\"meaningful and effective\" </u></a>control over whether their content is used for AI features, like AI Overviews. Google would be prohibited from taking any action that negatively impacts the effectiveness of those control options, such as intentionally downranking the content in search. </p><p>To support informed decisionmaking, the CMA proposal also requires Google to increase transparency, by publishing clear documentation on how it uses crawled content for generative AI and on exactly what its various publisher controls cover in practice. Finally, the proposal would require Google to ensure effective attribution of publisher content and to provide publishers with detailed, disaggregated engagement data—including specific metrics for impressions, clicks, and \"click quality\"—to help them evaluate the commercial value of allowing their content to be used in AI-generated search summaries.</p>\n    <div>\n      <h2>The CMA’s proposed remedies are insufficient</h2>\n      <a href=\"#the-cmas-proposed-remedies-are-insufficient\">\n        \n      </a>\n    </div>\n    <p>Although we support the CMA’s efforts to improve options for publishers, we are concerned that the proposed requirements do not solve the underlying issue of promoting fair, transparent choice over how their content is used by Google. Publishers are effectively forced to use Google’s proprietary opt-out mechanisms, tied specifically to the Google platform and under the conditions set by Google, rather than granting them direct, autonomous control. <b>A framework where the platform dictates the rules, manages the technical controls, and defines the scope of application does not offer “effective control” to content creators or encourage competitive innovation in the market. Instead, it reinforces a state of permanent dependency.</b>  </p><p>Such a framework also reduces choice for publishers. Creating new opt-out controls makes it impossible for publishers to choose to use external tools to block Googlebot from accessing their content without jeopardizing their appearance in Search results. Instead, under the current proposal, content creators will still have to allow Googlebot to scrape their websites, with no enforcement mechanisms to deploy and limited visibility available if Google does not respect their signalled preferences. Enforcement of these requirements by the CMA, if done properly, will be very onerous, without guarantee that publishers will trust the solution.</p><p>In fact, Cloudflare has received feedback from its customers that Google’s current proprietary opt-out mechanisms, including Google-Extended and ‘nosnippet’, have failed to prevent content from being utilized in ways that publishers cannot control. These opt-out tools also do not enable mechanisms for fair compensation for publishers. </p><p>More broadly, as reflected in our proposed <a href=\"https://blog.cloudflare.com/building-a-better-internet-with-responsible-ai-bot-principles/\"><u>responsible AI bot principles</u></a>, we believe that all AI bots should have one distinct purpose and declare it, so that website owners can make clear decisions over who can access their content and why. Unlike its leading competitors, such as OpenAI and Anthropic, Google does not comply with this principle for Googlebot, which is used for multiple purposes (search indexing, AI training, and inference/grounding). Simply requiring Google to develop a new opt-out mechanism would not allow publishers to achieve meaningful control over the use of their content. </p><p>The most effective way to give publishers that necessary control is to require Googlebot to be split up into separate crawlers. That way, publishers could allow crawling for traditional search indexing, which they need to attract traffic to their sites, but block access for unwanted use of their content in generative AI services and features. </p>\n    <div>\n      <h2>Requiring crawler separation is the only effective solution </h2>\n      <a href=\"#requiring-crawler-separation-is-the-only-effective-solution\">\n        \n      </a>\n    </div>\n    <p>To ensure a fair digital ecosystem, the CMA must instead empower content owners to prevent Google from accessing their data for particular purposes in the first place, rather than relying on Google-managed workarounds after the crawler has already accessed the content for other purposes. That approach also enables creators to set conditions for access to their content. </p><p>Although the CMA described crawler separation as an “equally effective intervention”, it ultimately rejected mandating separation based on Google’s input that it would be too onerous. We disagree.</p><p>Requiring Google to split up Googlebot by purpose — just like Google already does for its <a href=\"https://developers.google.com/crawling/docs/crawlers-fetchers/overview-google-crawlers\"><u>nearly 20 other crawlers</u></a> — is not only technically feasible, but also a necessary and proportionate remedy that empowers website operators to have the granular control they currently lack, without increasing traffic load from crawlers to their websites (and in fact, perhaps even decreasing it, should they choose to block AI crawling).</p><p>To be clear, a crawler separation remedy benefits AI companies, by leveling the playing field between them and Google, in addition to giving UK-based publishers more control over their content. (There has been widespread public support for a crawler separation remedy by Daily Mail Group, the Guardian and the News Media Association.) Mandatory crawler separation is not a disadvantage to Google, nor does it undermine investment in AI. On the contrary, it is a pro-competitive safeguard that prevents Google from leveraging its search monopoly to gain an unfair advantage in the AI market. By decoupling these functions, we ensure that AI development is driven by fair-market competition rather than the exploitation of a single hyperscaler’s dominance.</p><p>******</p><p>The UK has a unique chance to lead the world in protecting the value of original and high-quality content on the Internet. However, we worry that the current proposals fall short. We would encourage rules that ensure that Google operates under the same conditions for content access as other AI developers, meaningfully restoring agency to publishers and paving the way for new business models promoting content monetization.</p><p>Cloudflare remains committed to engaging with the CMA and other partners during upcoming consultations to provide evidence-based data to help shape a final decision on conduct requirements that are targeted, proportional, and effective. The CMA still has an opportunity to ensure that the Internet becomes a fair marketplace for content creators and smaller AI players—not just a select few tech giants.</p> ","content:encodedSnippet":"Earlier this week, the UK’s Competition and Markets Authority (CMA) opened its consultation on a package of proposed conduct requirements for Google. The consultation invites comments on the proposed requirements before the CMA imposes any final measures. These new rules aim to address the lack of choice and transparency that publishers (broadly defined as “any party that makes content available on the web”) face over how Google uses search to fuel its generative AI services and features. These are the first consultations on conduct requirements launched under the digital markets competition regime in the UK. \nWe welcome the CMA’s recognition that publishers need a fairer deal and believe the proposed rules are a step into the right direction. Publishers should be entitled to have access to tools that enable them to control the inclusion of their content in generative AI services, and AI companies should have a level playing field on which to compete. \nBut we believe the CMA has not gone far enough and should do more to safeguard the UK’s creative sector and foster healthy competition in the market for generative and agentic AI. \nCMA designation of Google as having Strategic Market Status \nIn January 2025, the UK’s regulatory landscape underwent a significant legal shift with the implementation of the Digital Markets, Competition and Consumers Act 2024 (DMCC). Rather than relying on antitrust investigations to address risks to competition, the CMA can now designate firms as having Strategic Market Status (SMS) when they hold substantial, entrenched market power. This designation allows for targeted CMA interventions in digital markets, such as imposing detailed conduct requirements, to improve competition. \nIn October 2025, the CMA designated Google as having SMS in general search and search advertising, given its 90 percent share of the search market in the UK. Crucially, this designation encompasses AI Overviews and AI Mode, with the CMA now having the authority to impose conduct requirements on Google’s search ecosystem. Final requirements imposed by the CMA are not merely suggestions but legally enforceable rules that can relate specifically to AI crawling with significant sanctions to ensure Google operates fairly. \nPublishers need a meaningful way to opt out of Google’s use of their content for generative AI\nThe CMA’s designation could not be more timely. As we’ve said before, we are indisputably in a time when the Internet needs clear “rules of the road” for AI crawling behavior. \nAs the CMA rightly states, “publishers have no realistic option but to allow their content to be crawled for Google’s general search because of the market power Google holds in general search. However, Google currently uses that content in both its search generative AI features and in its broader generative AI services.” \nIn other words: the same content that Google scrapes for search indexing is also used for inference/grounding purposes, like AI Overviews and AI Mode, which rely on fetching live information from the Internet in response to real-time user queries. And that creates a big problem for publishers—and for competition.\nBecause publishers cannot afford to disallow or block Googlebot, Google’s search crawler, on their website, they have to accept that their content will be used in generative AI applications such as AI Overviews and AI Mode within Google Search that return very little, if any, traffic to their websites. This undermines the ad-supported business models that have sustained digital publishing for decades, given the critical role of Google Search in driving human traffic to online advertising. It also means that Google’s generative AI applications enter into direct competition with publishers by reproducing their content, most often without attribution or compensation. \nPublishers’ reluctance to block Google because of its dominance in search gives Google an unfair competitive advantage in the market for generative and agentic AI. Unlike other AI bot operators, Google can use its search crawler to gather data for a variety of AI functions with little fear that its access will be restricted. It has minimal incentive to pay publishers for that data, which it is already getting for free. \nThis prevents the emergence of a well-functioning marketplace where AI developers negotiate fair value for content. Instead, other AI companies are disincentivized from coming to the table, as they are structurally disadvantaged by a system that allows one dominant player to bypass compensation entirely. As the CMA itself recognizes, \"[b]y not providing sufficient control over how this content is used, Google can limit the ability of publishers to monetise their content, while accessing content for AI-generated results in a way that its competitors cannot match”. \nGoogle’s advantage\nCloudflare data validates the concern about Google’s competitive advantage. Based on our data, Googlebot sees significantly more Internet content than its closest peers. \nOver an observed period of two months, Googlebot successfully accessed individual pages almost two times more than ClaudeBot and GPTBot, three times more than Meta-ExternalAgent, and more than three times more than Bingbot. The difference was even more extreme for other popular AI crawlers: for instance, Googlebot saw 167 times more unique pages than PerplexityBot. Out of the sampled unique URLs using our network that we observed over the last two months, Googlebot crawled roughly 8%.\nIn rounded multiple terms, Googlebot sees:\n\nvs. ~1.70x the amount of unique URLs seen by ClaudeBot;\n\nvs. ~1.76x the amount of unique URLs seen by GPTBot;\n\nvs. ~2.99x the amount of unique URLs by Meta-ExternalAgent;\n\nvs. ~3.26x the amount of unique URLs seen by Bingbot;\n\nvs. ~5.09x the amount of unique URLs seen by Amazonbot;\n\nvs. ~14.87x the amount of unique URLs seen by Applebot;\n\nvs. ~23.73x the amount of unique URLs seen by Bytespider;\n\nvs. ~166.98x the amount of unique URLs seen by PerplexityBot;\n\nvs. ~714.48x the amount of unique URLs seen by CCBot; and\n\nvs: ~1801.97x the amount of unique URLs seen by archive.org_bot.\n\nGooglebot also stands out in other Cloudflare datasets.  \nEven though it ranks as the most active bot by overall traffic, publishers are far less likely to disallow or block Googlebot in their robots.txt file compared to other crawlers. This is likely due to its importance in driving human traffic to their content—and, as a result, ad revenue—through search. \nAs shown below, almost no website explicitly disallows the dual-purpose Googlebot in full, reflecting how important this bot is to driving traffic via search referrals. (Note that partial disallows often impact certain parts of a website that are irrelevant for search engine optimization, or SEO, such as login endpoints.)\nIndeed, when comparing the numbers for customers using AI Crawl Control, Cloudflare’s own AI crawler blocking tool that is integrated in our Application Security suite, between July 2025 and January 2026, one can see that the number of websites actively blocking other popular AI crawlers (e.g., GPTBot, Claudebot), was nearly seven times as high as the number of websites that blocked Googlebot and Bingbot. (Like Googlebot, Bingbot combines search and AI crawling and drives traffic to these sites, but given its small market share in search, its impact is less significant.)\nSo we agree with the CMA on the problem statement. But how can publishers be enabled to effectively opt out of Google using their content for its generative AI applications? We share the CMA’s conclusion that “in order to be able to make meaningful decisions about how Google uses their Search Content, (...) publishers need the ability effectively to opt their Search Content out of both Google’s search generative AI features and Google’s broader generative AI services.” \nBut we’re concerned that the CMA’s proposal is insufficient.\nCMA’s proposed publisher conduct requirements\nOn January 28, 2026, the CMA published four sets of proposed conduct requirements for Google, including conduct requirements related to publishers. According to the CMA, the proposed publisher rules are designed to address concerns that publishers (1) lack sufficient choice over how Google uses their content in its AI-generated responses, (2) have limited transparency into Google’s use of that content, and (3) do not get effective attribution for Google’s use of their content. The CMA recognized the importance of these concerns because of the role that Google search plays in finding content online. \nThe conduct requirements would mandate Google grant publishers \"meaningful and effective\" control over whether their content is used for AI features, like AI Overviews. Google would be prohibited from taking any action that negatively impacts the effectiveness of those control options, such as intentionally downranking the content in search. \nTo support informed decisionmaking, the CMA proposal also requires Google to increase transparency, by publishing clear documentation on how it uses crawled content for generative AI and on exactly what its various publisher controls cover in practice. Finally, the proposal would require Google to ensure effective attribution of publisher content and to provide publishers with detailed, disaggregated engagement data—including specific metrics for impressions, clicks, and \"click quality\"—to help them evaluate the commercial value of allowing their content to be used in AI-generated search summaries.\nThe CMA’s proposed remedies are insufficient\nAlthough we support the CMA’s efforts to improve options for publishers, we are concerned that the proposed requirements do not solve the underlying issue of promoting fair, transparent choice over how their content is used by Google. Publishers are effectively forced to use Google’s proprietary opt-out mechanisms, tied specifically to the Google platform and under the conditions set by Google, rather than granting them direct, autonomous control. A framework where the platform dictates the rules, manages the technical controls, and defines the scope of application does not offer “effective control” to content creators or encourage competitive innovation in the market. Instead, it reinforces a state of permanent dependency.  \nSuch a framework also reduces choice for publishers. Creating new opt-out controls makes it impossible for publishers to choose to use external tools to block Googlebot from accessing their content without jeopardizing their appearance in Search results. Instead, under the current proposal, content creators will still have to allow Googlebot to scrape their websites, with no enforcement mechanisms to deploy and limited visibility available if Google does not respect their signalled preferences. Enforcement of these requirements by the CMA, if done properly, will be very onerous, without guarantee that publishers will trust the solution.\nIn fact, Cloudflare has received feedback from its customers that Google’s current proprietary opt-out mechanisms, including Google-Extended and ‘nosnippet’, have failed to prevent content from being utilized in ways that publishers cannot control. These opt-out tools also do not enable mechanisms for fair compensation for publishers. \nMore broadly, as reflected in our proposed responsible AI bot principles, we believe that all AI bots should have one distinct purpose and declare it, so that website owners can make clear decisions over who can access their content and why. Unlike its leading competitors, such as OpenAI and Anthropic, Google does not comply with this principle for Googlebot, which is used for multiple purposes (search indexing, AI training, and inference/grounding). Simply requiring Google to develop a new opt-out mechanism would not allow publishers to achieve meaningful control over the use of their content. \nThe most effective way to give publishers that necessary control is to require Googlebot to be split up into separate crawlers. That way, publishers could allow crawling for traditional search indexing, which they need to attract traffic to their sites, but block access for unwanted use of their content in generative AI services and features. \nRequiring crawler separation is the only effective solution \nTo ensure a fair digital ecosystem, the CMA must instead empower content owners to prevent Google from accessing their data for particular purposes in the first place, rather than relying on Google-managed workarounds after the crawler has already accessed the content for other purposes. That approach also enables creators to set conditions for access to their content. \nAlthough the CMA described crawler separation as an “equally effective intervention”, it ultimately rejected mandating separation based on Google’s input that it would be too onerous. We disagree.\nRequiring Google to split up Googlebot by purpose — just like Google already does for its nearly 20 other crawlers — is not only technically feasible, but also a necessary and proportionate remedy that empowers website operators to have the granular control they currently lack, without increasing traffic load from crawlers to their websites (and in fact, perhaps even decreasing it, should they choose to block AI crawling).\nTo be clear, a crawler separation remedy benefits AI companies, by leveling the playing field between them and Google, in addition to giving UK-based publishers more control over their content. (There has been widespread public support for a crawler separation remedy by Daily Mail Group, the Guardian and the News Media Association.) Mandatory crawler separation is not a disadvantage to Google, nor does it undermine investment in AI. On the contrary, it is a pro-competitive safeguard that prevents Google from leveraging its search monopoly to gain an unfair advantage in the AI market. By decoupling these functions, we ensure that AI development is driven by fair-market competition rather than the exploitation of a single hyperscaler’s dominance.\n******\nThe UK has a unique chance to lead the world in protecting the value of original and high-quality content on the Internet. However, we worry that the current proposals fall short. We would encourage rules that ensure that Google operates under the same conditions for content access as other AI developers, meaningfully restoring agency to publishers and paving the way for new business models promoting content monetization.\nCloudflare remains committed to engaging with the CMA and other partners during upcoming consultations to provide evidence-based data to help shape a final decision on conduct requirements that are targeted, proportional, and effective. The CMA still has an opportunity to ensure that the Internet becomes a fair marketplace for content creators and smaller AI players—not just a select few tech giants.","dc:creator":"Maria Palmieri","content":" Google's dual-purpose crawler creates an unfair AI advantage. To protect publishers and foster competition, the UK’s Competition and Markets Authority must mandate crawler separation for search and AI. ","contentSnippet":"Google's dual-purpose crawler creates an unfair AI advantage. To protect publishers and foster competition, the UK’s Competition and Markets Authority must mandate crawler separation for search and AI.","guid":"1csdasmGFE5gWnYFDBbN9j","categories":["AI","AI Bots","Google","Legal","Policy & Legal"],"isoDate":"2026-01-30T17:01:04.000Z"},{"creator":"Brayden Wilmoth","title":"Building vertical microfrontends on Cloudflare’s platform","link":"https://blog.cloudflare.com/vertical-microfrontends/","pubDate":"Fri, 30 Jan 2026 14:00:00 GMT","content:encoded":" <p><i>Updated at 6:55 a.m. PT</i></p><p>Today, we’re introducing a new Worker template for Vertical Microfrontends (VMFE). <a href=\"https://dash.cloudflare.com/?to=/:account/workers-and-pages/create?type=vmfe\"><u>This template</u></a> allows you to map multiple independent <a href=\"https://workers.cloudflare.com/\"><u>Cloudflare Workers</u></a> to a single domain, enabling teams to work in complete silos — shipping marketing, docs, and dashboards independently — while presenting a single, seamless application to the user.</p><a href=\"https://dash.cloudflare.com/?to=/:account/workers-and-pages/create?type=vmfe\"><img src=\"https://deploy.workers.cloudflare.com/button\" /></a>\n<p></p><p>Most microfrontend architectures are \"horizontal\", meaning different <i>parts</i> of a single page are fetched from different services. Vertical microfrontends take a different approach by splitting the application by URL path. In this model, a team owning the `/blog` path doesn't <i>just</i> own a component; they own the entire vertical stack for that route – framework, library choice, CI/CD and more. Owning the entire stack of a path, or set of paths, allows teams to have true ownership of their work and ship with confidence.</p><p>Teams face problems as they grow, where different frameworks serve varying use cases. A marketing website could be better utilized with Astro, for example, while a dashboard might be better with React. Or say you have a monolithic code base where many teams ship as a collective. An update to add new features from several teams can get frustratingly rolled back because a single team introduced a regression. How do we solve the problem of obscuring the technical implementation details away from the user and letting teams ship a cohesive user experience with full autonomy and control of their domains?</p><p>Vertical microfrontends can be the answer. Let’s dive in and explore how they solve developer pain points together.</p>\n    <div>\n      <h2>What are vertical microfrontends?</h2>\n      <a href=\"#what-are-vertical-microfrontends\">\n        \n      </a>\n    </div>\n    <p>A vertical microfrontend is an architectural pattern where a single independent team owns an entire slice of the application’s functionality, from the user interface all the way down to the <a href=\"https://www.cloudflare.com/learning/serverless/glossary/what-is-ci-cd/\">CI/CD pipeline</a>. These slices are defined by paths on a domain where you can associate individual Workers with specific paths:</p>\n            <pre><code>/      = Marketing\n/docs  = Documentation\n/blog  = Blog\n/dash  = Dashboard</code></pre>\n            <p>We could take it a step further and focus on more granular sub-path Worker associations, too, such as a dashboard. Within a dashboard, you likely segment out various features or products by adding depth to your URL path (e.g. <code>/dash/product-a</code>) and navigating between two products could mean two entirely different code bases. </p><p>Now with vertical microfrontends, we could also have the following:</p>\n            <pre><code>/dash/product-a  = WorkerA\n/dash/product-b  = WorkerB</code></pre>\n            <p>Each of the above paths are their own frontend project with zero shared code between them. The <code>product-a</code> and <code>product-b</code> routes map to separately deployed frontend applications that have their own frameworks, libraries, CI/CD pipelines defined and owned by their own teams. FINALLY.</p><p>You can now own your own code from end to end. But now we need to find a way to stitch these separate projects together, and even more so, make them feel as if they are a unified experience.</p><p>We experience this pain point ourselves here at Cloudflare, as the dashboard has many individual teams owning their own products. Teams must contend with the fact that changes made outside their control impact how users experience their product. </p><p>Internally, we are now using a similar strategy for our own dashboard. When users navigate from the core dashboard into our ZeroTrust product, in reality they are two entirely separate projects and the user is simply being routed to that project by its path <code>/:accountId/one</code>.</p>\n    <div>\n      <h2>Visually unified experiences</h2>\n      <a href=\"#visually-unified-experiences\">\n        \n      </a>\n    </div>\n    <p>Stitching these individual projects together to make them feel like a unified experience isn’t as difficult as you might think: It only takes a few lines of CSS magic. What we <i>absolutely do not want</i> to happen is to leak our implementation details and internal decisions to our users. If we fail to make this user experience feel like one cohesive frontend, then we’ve done a grave injustice to our users. </p><p>To accomplish this sleight of hand, let us take a little trip in understanding how view transitions and document preloading come into play.</p>\n    <div>\n      <h3>View transitions</h3>\n      <a href=\"#view-transitions\">\n        \n      </a>\n    </div>\n    <p>When we want to seamlessly navigate between two distinct pages while making it feel smooth to the end user, <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/View_Transition_API\"><u>view transitions</u></a> are quite useful. Defining specific <a href=\"https://www.w3schools.com/jsref/dom_obj_all.asp\"><u>DOM elements</u></a> on our page to stick around until the next page is visible, and defining how any changes are handled, make for quite the powerful quilt-stitching tool for multi-page applications.</p><p>There may be, however, instances where making the various vertical microfrontends feel different is more than acceptable. Perhaps our marketing website, documentation, and dashboard are each uniquely defined, for instance. A user would not expect all three of those to feel cohesive as you navigate between the three parts. But… if you decide to introduce vertical slices to an individual experience such as the dashboard (e.g. <code>/dash/product-a</code> &amp; <code>/dash/product-b</code>), then users should <b>never</b> know they are two different repositories/workers/projects underneath.</p><p>Okay, enough talk — let’s get to work. I mentioned it was low-effort to make two separate projects feel as if they were one to a user, and if you have yet to hear about <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/View_Transition_API\"><u>CSS View Transitions</u></a> then I’m about to blow your mind.</p><p>What if I told you that you could make animated transitions between different views  — single-page app (SPA) or multi-page app (MPA) — feel as if they were one? Before any view transitions are added, if we navigate between pages owned by two different Workers, the interstitial loading state would be the white blank screen in our browser for some few hundred milliseconds until the full next page began rendering. Pages would not feel cohesive, and it certainly would not feel like a single-page application.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4vw1Am7gYUQPtmFFCRcsu1/774b881dff7ce1c26db88f30623dfc13/image3.png\" />\n          </figure><p><sup>Appears as multiple navigation elements between each site.</sup></p><p>If we want elements to stick around, rather than seeing a white blank page, we can achieve that by defining CSS View Transitions. With the code below, we’re telling our current document page that when a view transition event is about to happen, keep the <code>nav</code> DOM element on the screen, and if any delta in appearance exists between our existing page and our destination page, then we’ll animate that with an <code>ease-in-out</code> transition.</p><p>All of a sudden, two different Workers feel like one.</p>\n            <pre><code>@supports (view-transition-name: none) {\n  ::view-transition-old(root),\n  ::view-transition-new(root) {\n    animation-duration: 0.3s;\n    animation-timing-function: ease-in-out;\n  }\n  nav { view-transition-name: navigation; }\n}</code></pre>\n            \n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4h6Eh5LSX4552QJDvV1l7o/a5a43ee0e6e011bca58ecc2d74902744/image1.png\" />\n          </figure><p><sup>Appears as a single navigation element between three distinct sites.</sup></p>\n    <div>\n      <h3>Preloading</h3>\n      <a href=\"#preloading\">\n        \n      </a>\n    </div>\n    <p>Transitioning between two pages makes it <i>look</i> seamless — and we also want it to <i>feel</i> as instant as a client-side SPA. While currently Firefox and Safari do not support <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Speculation_Rules_API\"><u>Speculation Rules</u></a>, Chrome/Edge/Opera do support the more recent newcomer. The speculation rules API is designed to improve performance for future navigations, particularly for document URLs, making multi-page applications feel more like single-page applications.</p><p>Breaking it down into code, what we need to define is a script rule in a specific format that tells the supporting browsers how to prefetch the other vertical slices that are connected to our web application — likely linked through some shared navigation.</p>\n            <pre><code>&lt;script type=\"speculationrules\"&gt;\n  {\n    \"prefetch\": [\n      {\n        \"urls\": [\"https://product-a.com\", \"https://product-b.com\"],\n        \"requires\": [\"anonymous-client-ip-when-cross-origin\"],\n        \"referrer_policy\": \"no-referrer\"\n      }\n    ]\n  }\n&lt;/script&gt;</code></pre>\n            <p>With that, our application prefetches our other microfrontends and holds them in our in-memory cache, so if we were to navigate to those pages it would feel nearly instant.</p><p>You likely won’t require this for clearly discernible vertical slices (marketing, docs, dashboard) because users would expect a slight load between them. However, it is highly encouraged to use when vertical slices are defined within a specific visible experience (e.g. within dashboard pages).</p><p>Between <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/View_Transition_API\"><u>View Transitions</u></a> and <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Speculation_Rules_API\"><u>Speculation Rules</u></a>, we are able to tie together entirely different code repositories to feel as if they were served from a single-page application. Wild if you ask me.</p>\n    <div>\n      <h2>Zero-config request routing</h2>\n      <a href=\"#zero-config-request-routing\">\n        \n      </a>\n    </div>\n    <p>Now we need a mechanism to host multiple applications, and a method to stitch them together as requests stream in. Defining a single Cloudflare Worker as the “Router” allows a single logical point (at the edge) to handle network requests and then forward them to whichever vertical microfrontend is responsible for that URL path. Plus it doesn’t hurt that then we can map a single domain to that router Worker and the rest “just works.”</p>\n    <div>\n      <h3>Service bindings</h3>\n      <a href=\"#service-bindings\">\n        \n      </a>\n    </div>\n    <p>If you have yet to explore Cloudflare Worker <a href=\"https://developers.cloudflare.com/workers/runtime-apis/bindings/service-bindings/\"><u>service bindings</u></a>, then it is worth taking a moment to do so. </p><p>Service bindings allow one Worker to call into another, without going through a publicly-accessible URL. A Service binding allows Worker A to call a method on Worker B, or to forward a request from Worker A to Worker. Breaking it down further, the Router Worker can call into each vertical microfrontend Worker that has been defined (e.g. marketing, docs, dashboard), assuming each of them were Cloudflare Workers.</p><p>Why is this important? This is precisely the mechanism that “stitches” these vertical slices together. We’ll dig into how the request routing is handling the traffic split in the next section. But to define each of these microfrontends, we’ll need to update our Router Worker’s wrangler definition, so it knows which frontends it’s allowed to call into.\n</p>\n            <pre><code>{\n  \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n  \"name\": \"router\",\n  \"main\": \"./src/router.js\",\n  \"services\": [\n    {\n      \"binding\": \"HOME\",\n      \"service\": \"worker_marketing\"\n    },\n    {\n      \"binding\": \"DOCS\",\n      \"service\": \"worker_docs\"\n    },\n    {\n      \"binding\": \"DASH\",\n      \"service\": \"worker_dash\"\n    },\n  ]\n}</code></pre>\n            <p>Our above sample definition is defined in our Router Worker, which then tells us that we are permitted to make requests into three separate additional Workers (marketing, docs, and dash). Granting permissions is as simple as that, but let’s tumble into some of the more complex logic with request routing and HTML rewriting network responses.</p>\n    <div>\n      <h3>Request routing</h3>\n      <a href=\"#request-routing\">\n        \n      </a>\n    </div>\n    <p>With knowledge of the various other Workers we are able to call into if needed, now we need some logic in place to know where to direct network requests when. Since the Router Worker is assigned to our custom domain, all incoming requests hit it first at the network edge. It then determines which Worker should handle the request and manages the resulting response. </p><p>The first step is to map URL paths to associated Workers. When a certain request URL is received, we need to know where it needs to be forwarded. We do this by defining rules. While we support wildcard routes, dynamic paths, and parameter constraints, we are going to stay focused on the basics — literal path prefixes — as it illustrates the point more clearly. </p><p> In this example, we have three microfrontends:</p>\n            <pre><code>/      = Marketing\n/docs  = Documentation\n/dash  = Dashboard</code></pre>\n            <p>Each of the above paths need to be mapped to an actual Worker (see our wrangler definition for services in the section above). For our Router Worker, we define an additional variable with the following data, so we can know which paths should map to which service bindings. We now know where to route users as requests come in! Define a wrangler variable with the name ROUTES and the following contents:</p>\n            <pre><code>{\n  \"routes\":[\n    {\"binding\": \"HOME\", \"path\": \"/\"},\n    {\"binding\": \"DOCS\", \"path\": \"/docs\"},\n    {\"binding\": \"DASH\", \"path\": \"/dash\"}\n  ]\n}</code></pre>\n            <p>Let’s envision a user visiting our website path <code>/docs/installation</code>. Under the hood, what happens is the request first reaches our Router Worker which is in charge of understanding what URL paths map to which individual Workers. It understands that the <code>/docs</code> path prefix is mapped to our <code>DOCS</code> service binding which referencing our wrangler file points us at our <code>worker_docs</code> project. Our Router Worker, knowing that <code>/docs</code> is defined as a vertical microfrontend route, removes the <code>/docs</code> prefix from the path and forwards the request to our <code>worker_docs</code> Worker to handle the request and then finally returns whatever response we get.</p><p>Why does it drop the <code>/docs</code> path, though? This was an implementation detail choice that was made so that when the Worker is accessed via the Router Worker, it can clean up the URL to handle the request <i>as if </i>it were called from outside our Router Worker. Like any Cloudflare Worker, our <code>worker_docs</code> service might have its own individual URL where it can be accessed. We decided we wanted that service URL to continue to work independently. When it’s attached to our new Router Worker, it would automatically handle removing the prefix, so the service could be accessible from its own defined URL or through our Router Worker… either place, doesn’t matter.</p>\n    <div>\n      <h3>HTMLRewriter</h3>\n      <a href=\"#htmlrewriter\">\n        \n      </a>\n    </div>\n    <p>Splitting our various frontend services with URL paths (e.g. <code>/docs</code> or <code>/dash</code>) makes it easy for us to forward a request, but when our response contains HTML that doesn’t know it’s being reverse proxied through a path component… well, that causes problems. </p><p>Say our documentation website has an image tag in the response <code>&lt;img src=\"./logo.png\" /&gt;</code>. If our user was visiting this page at <code>https://website.com/docs/</code>, then loading the <code>logo.png</code> file would likely fail because our <code>/docs</code> path is somewhat artificially defined only by our Router Worker.</p><p>Only when our services are accessed through our Router Worker do we need to do some HTML rewriting of absolute paths so our returned browser response references valid assets. In practice what happens is that when a request passes through our Router Worker, we pass the request to the correct Service Binding, and we receive the response from that. Before we pass that back to the client, we have an opportunity to rewrite the DOM — so where we see absolute paths, we go ahead and prepend that with the proxied path. Where previously our HTML was returning our image tag with <code>&lt;img src=\"./logo.png\" /&gt;</code> we now modify it before returning to the client browser to <code>&lt;img src=\"./docs/logo.png\" /&gt;</code>.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/10jKx6qt2YcarpDyEsFNYV/3b0f11f56e3c9b2deef59934cf8efa7f/image2.png\" />\n          </figure><p>Let’s return for a moment to the magic of CSS view transitions and document preloading. We could of course manually place that code into our projects and have it work, but this Router Worker will <i>automatically</i> handle that logic for us by also using <a href=\"https://developers.cloudflare.com/workers/runtime-apis/html-rewriter/\"><u>HTMLRewriter</u></a>. </p><p>In your Router Worker <code>ROUTES</code> variable, if you set <code>smoothTransitions</code> to <code>true</code> at the root level, then the CSS transition views code will be added automatically. Additionally, if you set the <code>preload</code> key within a route to <code>true</code>, then the script code speculation rules for that route will automatically be added as well. </p><p>Below is an example of both in action:</p>\n            <pre><code>{\n  \"smoothTransitions\":true, \n  \"routes\":[\n    {\"binding\": \"APP1\", \"path\": \"/app1\", \"preload\": true},\n    {\"binding\": \"APP2\", \"path\": \"/app2\", \"preload\": true}\n  ]\n}</code></pre>\n            \n    <div>\n      <h2>Get started</h2>\n      <a href=\"#get-started\">\n        \n      </a>\n    </div>\n    <p>You can start building with the Vertical Microfrontend template today.</p><p>Visit the Cloudflare Dashboard <a href=\"https://dash.cloudflare.com/?to=/:account/workers-and-pages/create?type=vmfe\"><u>deeplink here</u></a> or go to “Workers &amp; Pages” and click the “Create application” button to get started. From there, click “Select a template” and then “Create microfrontend” and you can begin configuring your setup.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1teTcTNHzQH3yCvbTz3xyU/8f9a4b2ef3ec1c6ed13cbdc51d6b13c5/image5.png\" />\n          </figure><p>\nCheck out the <a href=\"https://developers.cloudflare.com/workers/framework-guides/web-apps/microfrontends\"><u>documentation</u></a> to see how to map your existing Workers and enable View Transitions. We can't wait to see what complex, multi-team applications you build on the edge!</p> ","content:encodedSnippet":"Updated at 6:55 a.m. PT\nToday, we’re introducing a new Worker template for Vertical Microfrontends (VMFE). This template allows you to map multiple independent Cloudflare Workers to a single domain, enabling teams to work in complete silos — shipping marketing, docs, and dashboards independently — while presenting a single, seamless application to the user.\n\n\nMost microfrontend architectures are \"horizontal\", meaning different parts of a single page are fetched from different services. Vertical microfrontends take a different approach by splitting the application by URL path. In this model, a team owning the `/blog` path doesn't just own a component; they own the entire vertical stack for that route – framework, library choice, CI/CD and more. Owning the entire stack of a path, or set of paths, allows teams to have true ownership of their work and ship with confidence.\nTeams face problems as they grow, where different frameworks serve varying use cases. A marketing website could be better utilized with Astro, for example, while a dashboard might be better with React. Or say you have a monolithic code base where many teams ship as a collective. An update to add new features from several teams can get frustratingly rolled back because a single team introduced a regression. How do we solve the problem of obscuring the technical implementation details away from the user and letting teams ship a cohesive user experience with full autonomy and control of their domains?\nVertical microfrontends can be the answer. Let’s dive in and explore how they solve developer pain points together.\nWhat are vertical microfrontends?\nA vertical microfrontend is an architectural pattern where a single independent team owns an entire slice of the application’s functionality, from the user interface all the way down to the CI/CD pipeline. These slices are defined by paths on a domain where you can associate individual Workers with specific paths:\n/      = Marketing\n/docs  = Documentation\n/blog  = Blog\n/dash  = Dashboard\nWe could take it a step further and focus on more granular sub-path Worker associations, too, such as a dashboard. Within a dashboard, you likely segment out various features or products by adding depth to your URL path (e.g. /dash/product-a) and navigating between two products could mean two entirely different code bases. \nNow with vertical microfrontends, we could also have the following:\n/dash/product-a  = WorkerA\n/dash/product-b  = WorkerB\nEach of the above paths are their own frontend project with zero shared code between them. The product-a and product-b routes map to separately deployed frontend applications that have their own frameworks, libraries, CI/CD pipelines defined and owned by their own teams. FINALLY.\nYou can now own your own code from end to end. But now we need to find a way to stitch these separate projects together, and even more so, make them feel as if they are a unified experience.\nWe experience this pain point ourselves here at Cloudflare, as the dashboard has many individual teams owning their own products. Teams must contend with the fact that changes made outside their control impact how users experience their product. \nInternally, we are now using a similar strategy for our own dashboard. When users navigate from the core dashboard into our ZeroTrust product, in reality they are two entirely separate projects and the user is simply being routed to that project by its path /:accountId/one.\nVisually unified experiences\nStitching these individual projects together to make them feel like a unified experience isn’t as difficult as you might think: It only takes a few lines of CSS magic. What we absolutely do not want to happen is to leak our implementation details and internal decisions to our users. If we fail to make this user experience feel like one cohesive frontend, then we’ve done a grave injustice to our users. \nTo accomplish this sleight of hand, let us take a little trip in understanding how view transitions and document preloading come into play.\nView transitions\nWhen we want to seamlessly navigate between two distinct pages while making it feel smooth to the end user, view transitions are quite useful. Defining specific DOM elements on our page to stick around until the next page is visible, and defining how any changes are handled, make for quite the powerful quilt-stitching tool for multi-page applications.\nThere may be, however, instances where making the various vertical microfrontends feel different is more than acceptable. Perhaps our marketing website, documentation, and dashboard are each uniquely defined, for instance. A user would not expect all three of those to feel cohesive as you navigate between the three parts. But… if you decide to introduce vertical slices to an individual experience such as the dashboard (e.g. /dash/product-a & /dash/product-b), then users should never know they are two different repositories/workers/projects underneath.\nOkay, enough talk — let’s get to work. I mentioned it was low-effort to make two separate projects feel as if they were one to a user, and if you have yet to hear about CSS View Transitions then I’m about to blow your mind.\nWhat if I told you that you could make animated transitions between different views  — single-page app (SPA) or multi-page app (MPA) — feel as if they were one? Before any view transitions are added, if we navigate between pages owned by two different Workers, the interstitial loading state would be the white blank screen in our browser for some few hundred milliseconds until the full next page began rendering. Pages would not feel cohesive, and it certainly would not feel like a single-page application.\nAppears as multiple navigation elements between each site.\nIf we want elements to stick around, rather than seeing a white blank page, we can achieve that by defining CSS View Transitions. With the code below, we’re telling our current document page that when a view transition event is about to happen, keep the nav DOM element on the screen, and if any delta in appearance exists between our existing page and our destination page, then we’ll animate that with an ease-in-out transition.\nAll of a sudden, two different Workers feel like one.\n@supports (view-transition-name: none) {\n  ::view-transition-old(root),\n  ::view-transition-new(root) {\n    animation-duration: 0.3s;\n    animation-timing-function: ease-in-out;\n  }\n  nav { view-transition-name: navigation; }\n}\nAppears as a single navigation element between three distinct sites.\nPreloading\nTransitioning between two pages makes it look seamless — and we also want it to feel as instant as a client-side SPA. While currently Firefox and Safari do not support Speculation Rules, Chrome/Edge/Opera do support the more recent newcomer. The speculation rules API is designed to improve performance for future navigations, particularly for document URLs, making multi-page applications feel more like single-page applications.\nBreaking it down into code, what we need to define is a script rule in a specific format that tells the supporting browsers how to prefetch the other vertical slices that are connected to our web application — likely linked through some shared navigation.\n<script type=\"speculationrules\">\n  {\n    \"prefetch\": [\n      {\n        \"urls\": [\"https://product-a.com\", \"https://product-b.com\"],\n        \"requires\": [\"anonymous-client-ip-when-cross-origin\"],\n        \"referrer_policy\": \"no-referrer\"\n      }\n    ]\n  }\n</script>\nWith that, our application prefetches our other microfrontends and holds them in our in-memory cache, so if we were to navigate to those pages it would feel nearly instant.\nYou likely won’t require this for clearly discernible vertical slices (marketing, docs, dashboard) because users would expect a slight load between them. However, it is highly encouraged to use when vertical slices are defined within a specific visible experience (e.g. within dashboard pages).\nBetween View Transitions and Speculation Rules, we are able to tie together entirely different code repositories to feel as if they were served from a single-page application. Wild if you ask me.\nZero-config request routing\nNow we need a mechanism to host multiple applications, and a method to stitch them together as requests stream in. Defining a single Cloudflare Worker as the “Router” allows a single logical point (at the edge) to handle network requests and then forward them to whichever vertical microfrontend is responsible for that URL path. Plus it doesn’t hurt that then we can map a single domain to that router Worker and the rest “just works.”\nService bindings\nIf you have yet to explore Cloudflare Worker service bindings, then it is worth taking a moment to do so. \nService bindings allow one Worker to call into another, without going through a publicly-accessible URL. A Service binding allows Worker A to call a method on Worker B, or to forward a request from Worker A to Worker. Breaking it down further, the Router Worker can call into each vertical microfrontend Worker that has been defined (e.g. marketing, docs, dashboard), assuming each of them were Cloudflare Workers.\nWhy is this important? This is precisely the mechanism that “stitches” these vertical slices together. We’ll dig into how the request routing is handling the traffic split in the next section. But to define each of these microfrontends, we’ll need to update our Router Worker’s wrangler definition, so it knows which frontends it’s allowed to call into.\n\n            \n{\n  \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n  \"name\": \"router\",\n  \"main\": \"./src/router.js\",\n  \"services\": [\n    {\n      \"binding\": \"HOME\",\n      \"service\": \"worker_marketing\"\n    },\n    {\n      \"binding\": \"DOCS\",\n      \"service\": \"worker_docs\"\n    },\n    {\n      \"binding\": \"DASH\",\n      \"service\": \"worker_dash\"\n    },\n  ]\n}\nOur above sample definition is defined in our Router Worker, which then tells us that we are permitted to make requests into three separate additional Workers (marketing, docs, and dash). Granting permissions is as simple as that, but let’s tumble into some of the more complex logic with request routing and HTML rewriting network responses.\nRequest routing\nWith knowledge of the various other Workers we are able to call into if needed, now we need some logic in place to know where to direct network requests when. Since the Router Worker is assigned to our custom domain, all incoming requests hit it first at the network edge. It then determines which Worker should handle the request and manages the resulting response. \nThe first step is to map URL paths to associated Workers. When a certain request URL is received, we need to know where it needs to be forwarded. We do this by defining rules. While we support wildcard routes, dynamic paths, and parameter constraints, we are going to stay focused on the basics — literal path prefixes — as it illustrates the point more clearly. \n In this example, we have three microfrontends:\n/      = Marketing\n/docs  = Documentation\n/dash  = Dashboard\nEach of the above paths need to be mapped to an actual Worker (see our wrangler definition for services in the section above). For our Router Worker, we define an additional variable with the following data, so we can know which paths should map to which service bindings. We now know where to route users as requests come in! Define a wrangler variable with the name ROUTES and the following contents:\n{\n  \"routes\":[\n    {\"binding\": \"HOME\", \"path\": \"/\"},\n    {\"binding\": \"DOCS\", \"path\": \"/docs\"},\n    {\"binding\": \"DASH\", \"path\": \"/dash\"}\n  ]\n}\nLet’s envision a user visiting our website path /docs/installation. Under the hood, what happens is the request first reaches our Router Worker which is in charge of understanding what URL paths map to which individual Workers. It understands that the /docs path prefix is mapped to our DOCS service binding which referencing our wrangler file points us at our worker_docs project. Our Router Worker, knowing that /docs is defined as a vertical microfrontend route, removes the /docs prefix from the path and forwards the request to our worker_docs Worker to handle the request and then finally returns whatever response we get.\nWhy does it drop the /docs path, though? This was an implementation detail choice that was made so that when the Worker is accessed via the Router Worker, it can clean up the URL to handle the request as if it were called from outside our Router Worker. Like any Cloudflare Worker, our worker_docs service might have its own individual URL where it can be accessed. We decided we wanted that service URL to continue to work independently. When it’s attached to our new Router Worker, it would automatically handle removing the prefix, so the service could be accessible from its own defined URL or through our Router Worker… either place, doesn’t matter.\nHTMLRewriter\nSplitting our various frontend services with URL paths (e.g. /docs or /dash) makes it easy for us to forward a request, but when our response contains HTML that doesn’t know it’s being reverse proxied through a path component… well, that causes problems. \nSay our documentation website has an image tag in the response <img src=\"./logo.png\" />. If our user was visiting this page at https://website.com/docs/, then loading the logo.png file would likely fail because our /docs path is somewhat artificially defined only by our Router Worker.\nOnly when our services are accessed through our Router Worker do we need to do some HTML rewriting of absolute paths so our returned browser response references valid assets. In practice what happens is that when a request passes through our Router Worker, we pass the request to the correct Service Binding, and we receive the response from that. Before we pass that back to the client, we have an opportunity to rewrite the DOM — so where we see absolute paths, we go ahead and prepend that with the proxied path. Where previously our HTML was returning our image tag with <img src=\"./logo.png\" /> we now modify it before returning to the client browser to <img src=\"./docs/logo.png\" />.\nLet’s return for a moment to the magic of CSS view transitions and document preloading. We could of course manually place that code into our projects and have it work, but this Router Worker will automatically handle that logic for us by also using HTMLRewriter. \nIn your Router Worker ROUTES variable, if you set smoothTransitions to true at the root level, then the CSS transition views code will be added automatically. Additionally, if you set the preload key within a route to true, then the script code speculation rules for that route will automatically be added as well. \nBelow is an example of both in action:\n{\n  \"smoothTransitions\":true, \n  \"routes\":[\n    {\"binding\": \"APP1\", \"path\": \"/app1\", \"preload\": true},\n    {\"binding\": \"APP2\", \"path\": \"/app2\", \"preload\": true}\n  ]\n}\nGet started\nYou can start building with the Vertical Microfrontend template today.\nVisit the Cloudflare Dashboard deeplink here or go to “Workers & Pages” and click the “Create application” button to get started. From there, click “Select a template” and then “Create microfrontend” and you can begin configuring your setup.\n\nCheck out the documentation to see how to map your existing Workers and enable View Transitions. We can't wait to see what complex, multi-team applications you build on the edge!","dc:creator":"Brayden Wilmoth","content":" Deploy multiple Workers under a single domain with the ability to make them feel like single-page applications. We take a look at how service bindings enable URL path routing to multiple projects. ","contentSnippet":"Deploy multiple Workers under a single domain with the ability to make them feel like single-page applications. We take a look at how service bindings enable URL path routing to multiple projects.","guid":"2u7SNZ4BZcQYHZYKqmdEaM","categories":["Cloudflare Workers","Developer Platform","Developers","Dashboard","Front End","Micro-frontends"],"isoDate":"2026-01-30T14:00:00.000Z"},{"creator":"Celso Martinho","title":"Introducing Moltworker: a self-hosted personal AI agent, minus the minis","link":"https://blog.cloudflare.com/moltworker-self-hosted-ai-agent/","pubDate":"Thu, 29 Jan 2026 14:00:00 GMT","content:encoded":" <p><i></i></p><p><i>Editorial note: As of January 30, 2026, Moltbot has been </i><a href=\"https://openclaw.ai/blog/introducing-openclaw\"><i><u>renamed</u></i></a><i> to OpenClaw.</i></p><p>The Internet woke up this week to a flood of people <a href=\"https://x.com/AlexFinn/status/2015133627043270750\"><u>buying Mac minis</u></a> to run <a href=\"https://github.com/moltbot/moltbot\"><u>Moltbot</u></a> (formerly Clawdbot), an open-source, self-hosted AI agent designed to act as a personal assistant. Moltbot runs in the background on a user's own hardware, has a sizable and growing list of integrations for chat applications, AI models, and other popular tools, and can be controlled remotely. Moltbot can help you with your finances, social media, organize your day — all through your favorite messaging app.</p><p>But what if you don’t want to buy new dedicated hardware? And what if you could still run your Moltbot efficiently and securely online? Meet <a href=\"https://github.com/cloudflare/moltworker\"><u>Moltworker</u></a>, a middleware Worker and adapted scripts that allows running Moltbot on Cloudflare's Sandbox SDK and our Developer Platform APIs.</p>\n    <div>\n      <h2>A personal assistant on Cloudflare — how does that work? </h2>\n      <a href=\"#a-personal-assistant-on-cloudflare-how-does-that-work\">\n        \n      </a>\n    </div>\n    <p>Cloudflare Workers has never been <a href=\"https://developers.cloudflare.com/workers/runtime-apis/nodejs/\"><u>as compatible</u></a> with Node.js as it is now. Where in the past we had to mock APIs to get some packages running, now those APIs are supported natively by the Workers Runtime.</p><p>This has changed how we can build tools on Cloudflare Workers. When we first implemented <a href=\"https://developers.cloudflare.com/browser-rendering/playwright/\"><u>Playwright</u></a>, a popular framework for web testing and automation that runs on <a href=\"https://developers.cloudflare.com/browser-rendering/\"><u>Browser Rendering</u></a>, we had to rely on <a href=\"https://www.npmjs.com/package/memfs\"><u>memfs</u></a>. This was bad because not only is memfs a hack and an external dependency, but it also forced us to drift away from the official Playwright codebase. Thankfully, with more Node.js compatibility, we were able to start using <a href=\"https://github.com/cloudflare/playwright/pull/62/changes\"><u>node:fs natively</u></a>, reducing complexity and maintainability, which makes upgrades to the latest versions of Playwright easy to do.</p><p>The list of Node.js APIs we support natively keeps growing. The blog post “<a href=\"https://blog.cloudflare.com/nodejs-workers-2025/\"><u>A year of improving Node.js compatibility in Cloudflare Workers</u></a>” provides an overview of where we are and what we’re doing.</p><p>We measure this progress, too. We recently ran an experiment where we took the 1,000 most popular NPM packages, installed and let AI loose, to try to run them in Cloudflare Workers, <a href=\"https://ghuntley.com/ralph/\"><u>Ralph Wiggum as a \"software engineer\"</u></a> style, and the results were surprisingly good. Excluding the packages that are build tools, CLI tools or browser-only and don’t apply, only 15 packages genuinely didn’t work. <b>That's 1.5%</b>.</p><p>Here’s a graphic of our Node.js API support over time:</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5GhwKJq2A2wG79I3NdhhDl/e462c30daf46b1b36d3f06bff479596b/image9.png\" />\n          </figure><p>We put together a page with the results of our internal experiment on npm packages support <a href=\"https://worksonworkers.southpolesteve.workers.dev/\"><u>here</u></a>, so you can check for yourself.</p><p>Moltbot doesn’t necessarily require a lot of Workers Node.js compatibility because most of the code runs in a container anyway, but we thought it would be important to highlight how far we got supporting so many packages using native APIs. This is because when starting a new AI agent application from scratch, we can actually run a lot of the logic in Workers, closer to the user.</p><p>The other important part of the story is that the list of <a href=\"https://developers.cloudflare.com/directory/?product-group=Developer+platform\"><u>products and APIs</u></a> on our Developer Platform has grown to the point where anyone can build and run any kind of application — even the most complex and demanding ones — on Cloudflare. And once launched, every application running on our Developer Platform immediately benefits from our secure and scalable global network.</p><p>Those products and services gave us the ingredients we needed to get started. First, we now have <a href=\"https://sandbox.cloudflare.com/\"><u>Sandboxes</u></a>, where you can run untrusted code securely in isolated environments, providing a place to run the service. Next, we now have <a href=\"https://developers.cloudflare.com/browser-rendering/\"><u>Browser Rendering</u></a>, where you can programmatically control and interact with headless browser instances. And finally, <a href=\"https://developers.cloudflare.com/r2/\"><u>R2</u></a>, where you can store objects persistently. With those building blocks available, we could begin work on adapting Moltbot.</p>\n    <div>\n      <h2>How we adapted Moltbot to run on us</h2>\n      <a href=\"#how-we-adapted-moltbot-to-run-on-us\">\n        \n      </a>\n    </div>\n    <p>Moltbot on Workers, or Moltworker, is a combination of an entrypoint Worker that acts as an API router and a proxy between our APIs and the isolated environment, both protected by Cloudflare Access. It also provides an administration UI and connects to the Sandbox container where the standard Moltbot Gateway runtime and its integrations are running, using R2 for persistent storage.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3OD2oHgy5ilHpQO2GJvcLU/836a55b67a626d2cd378a654ad47901d/newdiagram.png\" />\n          </figure><p><sup>High-level architecture diagram of Moltworker.</sup></p><p>Let's dive in more.</p>\n    <div>\n      <h3>AI Gateway</h3>\n      <a href=\"#ai-gateway\">\n        \n      </a>\n    </div>\n    <p>Cloudflare AI Gateway acts as a proxy between your AI applications and any popular <a href=\"https://developers.cloudflare.com/ai-gateway/usage/providers/\"><u>AI provider</u></a>, and gives our customers centralized visibility and control over the requests going through.</p><p>Recently we announced support for <a href=\"https://developers.cloudflare.com/changelog/2025-08-25-secrets-store-ai-gateway/\"><u>Bring Your Own Key (BYOK)</u></a>, where instead of passing your provider secrets in plain text with every request, we centrally manage the secrets for you and can use them with your gateway configuration.</p><p>An even better option where you don’t have to manage AI providers' secrets at all end-to-end is to use <a href=\"https://developers.cloudflare.com/ai-gateway/features/unified-billing/\"><u>Unified Billing</u></a>. In this case you top up your account with credits and use AI Gateway with any of the supported providers directly, Cloudflare gets charged, and we will deduct credits from your account.</p><p>To make Moltbot use AI Gateway, first we create a new gateway instance, then we enable the Anthropic provider for it, then we either add our Claude key or purchase credits to use Unified Billing, and then all we need to do is set the ANTHROPIC_BASE_URL environment variable so Moltbot uses the AI Gateway endpoint. That’s it, no code changes necessary.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/cMWRXgHR0mFLc5kp74nYk/a47fa09bdbb6acb3deb60fb16537945d/image11.png\" />\n          </figure><p>Once Moltbot starts using AI Gateway, you’ll have full visibility on costs and have access to logs and analytics that will help you understand how your AI agent is using the AI providers.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5GOrNdgtdwMcU4bE8oLE19/6bc29bcac643125f5332a8ffba9d1322/image1.png\" />\n          </figure><p>Note that Anthropic is one option; Moltbot supports <a href=\"https://www.molt.bot/integrations\"><u>other</u></a> AI providers and so does <a href=\"https://developers.cloudflare.com/ai-gateway/usage/providers/\"><u>AI Gateway</u></a>. The advantage of using AI Gateway is that if a better model comes along from any provider, you don’t have to swap keys in your AI Agent configuration and redeploy — you can simply switch the model in your gateway configuration. And more, you specify model or provider <a href=\"https://developers.cloudflare.com/ai-gateway/configuration/fallbacks/\"><u>fallbacks</u></a> to handle request failures and ensure reliability.</p>\n    <div>\n      <h3>Sandboxes</h3>\n      <a href=\"#sandboxes\">\n        \n      </a>\n    </div>\n    <p>Last year we anticipated the growing need for AI agents to run untrusted code securely in isolated environments, and we <a href=\"https://developers.cloudflare.com/changelog/2025-06-24-announcing-sandboxes/\"><u>announced</u></a> the <a href=\"https://developers.cloudflare.com/sandbox/\"><u>Sandbox SDK</u></a>. This SDK is built on top of <a href=\"https://developers.cloudflare.com/containers/\"><u>Cloudflare Containers</u></a>, but it provides a simple API for executing commands, managing files, running background processes, and exposing services — all from your Workers applications.</p><p>In short, instead of having to deal with the lower-level Container APIs, the Sandbox SDK gives you developer-friendly APIs for secure code execution and handles the complexity of container lifecycle, networking, file systems, and process management — letting you focus on building your application logic with just a few lines of TypeScript. Here’s an example:</p>\n            <pre><code>import { getSandbox } from '@cloudflare/sandbox';\nexport { Sandbox } from '@cloudflare/sandbox';\n\nexport default {\n  async fetch(request: Request, env: Env): Promise&lt;Response&gt; {\n    const sandbox = getSandbox(env.Sandbox, 'user-123');\n\n    // Create a project structure\n    await sandbox.mkdir('/workspace/project/src', { recursive: true });\n\n    // Check node version\n    const version = await sandbox.exec('node -v');\n\n    // Run some python code\n    const ctx = await sandbox.createCodeContext({ language: 'python' });\n    await sandbox.runCode('import math; radius = 5', { context: ctx });\n    const result = await sandbox.runCode('math.pi * radius ** 2', { context: ctx });\n\n    return Response.json({ version, result });\n  }\n};</code></pre>\n            <p>This fits like a glove for Moltbot. Instead of running Docker in your local Mac mini, we run Docker on Containers, use the Sandbox SDK to issue commands into the isolated environment and use callbacks to our entrypoint Worker, effectively establishing a two-way communication channel between the two systems.</p>\n    <div>\n      <h3>R2 for persistent storage</h3>\n      <a href=\"#r2-for-persistent-storage\">\n        \n      </a>\n    </div>\n    <p>The good thing about running things in your local computer or VPS is you get persistent storage for free. Containers, however, are inherently <a href=\"https://developers.cloudflare.com/containers/platform-details/architecture/\"><u>ephemeral</u></a>, meaning data generated within them is lost upon deletion. Fear not, though — the Sandbox SDK provides the sandbox.mountBucket() that you can use to automatically, well, mount your R2 bucket as a filesystem partition when the container starts.</p><p>Once we have a local directory that is guaranteed to survive the container lifecycle, we can use that for Moltbot to store session memory files, conversations and other assets that are required to persist.</p>\n    <div>\n      <h3>Browser Rendering for browser automation</h3>\n      <a href=\"#browser-rendering-for-browser-automation\">\n        \n      </a>\n    </div>\n    <p>AI agents rely heavily on browsing the sometimes not-so-structured web. Moltbot utilizes dedicated Chromium instances to perform actions, navigate the web, fill out forms, take snapshots, and handle tasks that require a web browser. Sure, we can run Chromium on Sandboxes too, but what if we could simplify and use an API instead?</p><p>With Cloudflare’s <a href=\"https://developers.cloudflare.com/browser-rendering/\"><u>Browser Rendering</u></a>, you can programmatically control and interact with headless browser instances running at scale in our edge network. We support <a href=\"https://developers.cloudflare.com/browser-rendering/puppeteer/\"><u>Puppeteer</u></a>, <a href=\"https://developers.cloudflare.com/browser-rendering/stagehand/\"><u>Stagehand</u></a>, <a href=\"https://developers.cloudflare.com/browser-rendering/playwright/\"><u>Playwright</u></a> and other popular packages so that developers can onboard with minimal code changes. We even support <a href=\"https://developers.cloudflare.com/browser-rendering/playwright/playwright-mcp/\"><u>MCP</u></a> for AI.</p><p>In order to get Browser Rendering to work with Moltbot we do two things:</p><ul><li><p>First we create a <a href=\"https://github.com/cloudflare/moltworker/blob/main/src/routes/cdp.ts\"><u>thin CDP proxy</u></a> (<a href=\"https://chromedevtools.github.io/devtools-protocol/\"><u>CDP</u></a> is the protocol that allows instrumenting Chromium-based browsers) from the Sandbox container to the Moltbot Worker, back to Browser Rendering using the Puppeteer APIs.</p></li><li><p>Then we inject a <a href=\"https://github.com/cloudflare/moltworker/pull/20\"><u>Browser Rendering skill</u></a> into the runtime when the Sandbox starts.</p></li></ul>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1ZvQa7vS1T9Mm3nywqarQZ/9dec3d8d06870ee575a519440d34c499/image12.png\" />\n          </figure><p>From the Moltbot runtime perspective, it has a local CDP port it can connect to and perform browser tasks.</p>\n    <div>\n      <h3>Zero Trust Access for authentication policies</h3>\n      <a href=\"#zero-trust-access-for-authentication-policies\">\n        \n      </a>\n    </div>\n    <p>Next up we want to protect our APIs and Admin UI from unauthorized access. Doing authentication from scratch is hard, and is typically the kind of wheel you don’t want to reinvent or have to deal with. Zero Trust Access makes it incredibly easy to protect your application by defining specific policies and login methods for the endpoints. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1MDXXjbMs4PViN3kp9iFBY/a3095f07c986594d0c07d0276dbf22cc/image3.png\" />\n          </figure><p><sup>Zero Trust Access Login methods configuration for the Moltworker application.</sup></p><p>Once the endpoints are protected, Cloudflare will handle authentication for you and automatically include a <a href=\"https://developers.cloudflare.com/cloudflare-one/access-controls/applications/http-apps/authorization-cookie/application-token/\"><u>JWT token</u></a> with every request to your origin endpoints. You can then <a href=\"https://developers.cloudflare.com/cloudflare-one/access-controls/applications/http-apps/authorization-cookie/validating-json/\"><u>validate</u></a> that JWT for extra protection, to ensure that the request came from Access and not a malicious third party.</p><p>Like with AI Gateway, once all your APIs are behind Access you get great observability on who the users are and what they are doing with your Moltbot instance.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3BV4eqxKPXTiq18vvVpmZh/e034b7e7ea637a00c73c2ebe4d1400aa/image8.png\" />\n          </figure>\n    <div>\n      <h2>Moltworker in action</h2>\n      <a href=\"#moltworker-in-action\">\n        \n      </a>\n    </div>\n    <p>Demo time. We’ve put up a Slack instance where we could play with our own instance of Moltbot on Workers. Here are some of the fun things we’ve done with it.</p><p>We hate bad news.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4FxN935AgINZ8953WSswKB/e52d3eb268aa0732c5e6aa64a8e2adba/image6.png\" />\n          </figure><p>Here’s a chat session where we ask Moltbot to find the shortest route between Cloudflare in London and Cloudflare in Lisbon using Google Maps and take a screenshot in a Slack channel. It goes through a sequence of steps using Browser Rendering to navigate Google Maps and does a pretty good job at it. Also look at Moltbot’s memory in action when we ask him the second time.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1phWt3cVUwxe9tvCYpuAW3/97f456094ede6ca8fb55bf0dddf65d5b/image10.png\" />\n          </figure><p>We’re in the mood for some Asian food today, let’s get Moltbot to work for help.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6nJY7GOCopGnMy4IY7KMcf/0d57794df524780c3f4b27e65c968e19/image5.png\" />\n          </figure><p>We eat with our eyes too.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5BzB9pqJhuevRbOSJloeG0/23c2905f0c12c1e7f104aa28fcc1f595/image7.png\" />\n          </figure><p>Let’s get more creative and ask Moltbot to create a video where it browses our developer documentation. As you can see, it downloads and runs ffmpeg to generate the video out of the frames it captured in the browser.</p><div>\n  \n</div>\n    <div>\n      <h2>Run your own Moltworker</h2>\n      <a href=\"#run-your-own-moltworker\">\n        \n      </a>\n    </div>\n    <p>We open-sourced our implementation and made it available at<a href=\"https://github.com/cloudflare/moltworker\"> <u>https://github.com/cloudflare/moltworker</u></a>, so you can deploy and run your own Moltbot on top of Workers today.</p><p>The <a href=\"https://github.com/cloudflare/moltworker/blob/main/README.md\">README</a> guides you through the necessary steps to set up everything. You will need a Cloudflare account and a minimum $5 USD <a href=\"https://developers.cloudflare.com/workers/platform/pricing/\">Workers paid plan</a> subscription to use Sandbox Containers, but all the other products are either free to use, like <a href=\"https://developers.cloudflare.com/ai-gateway/reference/pricing/\">AI Gateway</a>, or have generous <a href=\"https://developers.cloudflare.com/r2/pricing/#free-tier\">free tiers</a> you can use to get you started and run for as long as you want under reasonable limits.</p><p><b>Note that Moltworker is a proof of concept, not a Cloudflare product</b>. Our goal is to showcase some of the most exciting features of our <a href=\"https://developers.cloudflare.com/learning-paths/workers/devplat/intro-to-devplat/\">Developer Platform</a> that can be used to run AI agents and unsupervised code efficiently and securely, and get great observability while taking advantage of our global network.</p><p>Feel free to contribute to or fork our <a href=\"https://github.com/cloudflare/moltworker\"><u>GitHub</u></a> repository; we will keep an eye on it for a while for support. We are also considering contributing upstream to the official project with Cloudflare skills in parallel.</p>\n    <div>\n      <h2>Conclusion</h2>\n      <a href=\"#conclusion\">\n        \n      </a>\n    </div>\n    <p>We hope you enjoyed this experiment, and we were able to convince you that Cloudflare is the perfect place to run your AI applications and agents. We’ve been working relentlessly trying to anticipate the future and release features like the <a href=\"https://developers.cloudflare.com/agents/\"><u>Agents SDK</u></a> that you can use to build your first agent <a href=\"https://developers.cloudflare.com/agents/guides/slack-agent/\"><u>in minutes</u></a>, <a href=\"https://developers.cloudflare.com/sandbox/\"><u>Sandboxes</u></a> where you can run arbitrary code in an isolated environment without the complications of the lifecycle of a container, and <a href=\"https://developers.cloudflare.com/ai-search/\"><u>AI Search</u></a>, Cloudflare’s managed vector-based search service, to name a few.</p><p>Cloudflare now offers a complete toolkit for AI development: inference, storage APIs, databases, durable execution for stateful workflows, and built-in AI capabilities. Together, these building blocks make it possible to build and run even the most demanding AI applications on our global edge network.</p><p>If you're excited about AI and want to help us build the next generation of products and APIs, we're <a href=\"https://www.cloudflare.com/en-gb/careers/jobs/?department=Engineering\"><u>hiring</u></a>.</p> ","content:encodedSnippet":"Editorial note: As of January 30, 2026, Moltbot has been renamed to OpenClaw.\nThe Internet woke up this week to a flood of people buying Mac minis to run Moltbot (formerly Clawdbot), an open-source, self-hosted AI agent designed to act as a personal assistant. Moltbot runs in the background on a user's own hardware, has a sizable and growing list of integrations for chat applications, AI models, and other popular tools, and can be controlled remotely. Moltbot can help you with your finances, social media, organize your day — all through your favorite messaging app.\nBut what if you don’t want to buy new dedicated hardware? And what if you could still run your Moltbot efficiently and securely online? Meet Moltworker, a middleware Worker and adapted scripts that allows running Moltbot on Cloudflare's Sandbox SDK and our Developer Platform APIs.\nA personal assistant on Cloudflare — how does that work? \nCloudflare Workers has never been as compatible with Node.js as it is now. Where in the past we had to mock APIs to get some packages running, now those APIs are supported natively by the Workers Runtime.\nThis has changed how we can build tools on Cloudflare Workers. When we first implemented Playwright, a popular framework for web testing and automation that runs on Browser Rendering, we had to rely on memfs. This was bad because not only is memfs a hack and an external dependency, but it also forced us to drift away from the official Playwright codebase. Thankfully, with more Node.js compatibility, we were able to start using node:fs natively, reducing complexity and maintainability, which makes upgrades to the latest versions of Playwright easy to do.\nThe list of Node.js APIs we support natively keeps growing. The blog post “A year of improving Node.js compatibility in Cloudflare Workers” provides an overview of where we are and what we’re doing.\nWe measure this progress, too. We recently ran an experiment where we took the 1,000 most popular NPM packages, installed and let AI loose, to try to run them in Cloudflare Workers, Ralph Wiggum as a \"software engineer\" style, and the results were surprisingly good. Excluding the packages that are build tools, CLI tools or browser-only and don’t apply, only 15 packages genuinely didn’t work. That's 1.5%.\nHere’s a graphic of our Node.js API support over time:\nWe put together a page with the results of our internal experiment on npm packages support here, so you can check for yourself.\nMoltbot doesn’t necessarily require a lot of Workers Node.js compatibility because most of the code runs in a container anyway, but we thought it would be important to highlight how far we got supporting so many packages using native APIs. This is because when starting a new AI agent application from scratch, we can actually run a lot of the logic in Workers, closer to the user.\nThe other important part of the story is that the list of products and APIs on our Developer Platform has grown to the point where anyone can build and run any kind of application — even the most complex and demanding ones — on Cloudflare. And once launched, every application running on our Developer Platform immediately benefits from our secure and scalable global network.\nThose products and services gave us the ingredients we needed to get started. First, we now have Sandboxes, where you can run untrusted code securely in isolated environments, providing a place to run the service. Next, we now have Browser Rendering, where you can programmatically control and interact with headless browser instances. And finally, R2, where you can store objects persistently. With those building blocks available, we could begin work on adapting Moltbot.\nHow we adapted Moltbot to run on us\nMoltbot on Workers, or Moltworker, is a combination of an entrypoint Worker that acts as an API router and a proxy between our APIs and the isolated environment, both protected by Cloudflare Access. It also provides an administration UI and connects to the Sandbox container where the standard Moltbot Gateway runtime and its integrations are running, using R2 for persistent storage.\nHigh-level architecture diagram of Moltworker.\nLet's dive in more.\nAI Gateway\nCloudflare AI Gateway acts as a proxy between your AI applications and any popular AI provider, and gives our customers centralized visibility and control over the requests going through.\nRecently we announced support for Bring Your Own Key (BYOK), where instead of passing your provider secrets in plain text with every request, we centrally manage the secrets for you and can use them with your gateway configuration.\nAn even better option where you don’t have to manage AI providers' secrets at all end-to-end is to use Unified Billing. In this case you top up your account with credits and use AI Gateway with any of the supported providers directly, Cloudflare gets charged, and we will deduct credits from your account.\nTo make Moltbot use AI Gateway, first we create a new gateway instance, then we enable the Anthropic provider for it, then we either add our Claude key or purchase credits to use Unified Billing, and then all we need to do is set the ANTHROPIC_BASE_URL environment variable so Moltbot uses the AI Gateway endpoint. That’s it, no code changes necessary.\nOnce Moltbot starts using AI Gateway, you’ll have full visibility on costs and have access to logs and analytics that will help you understand how your AI agent is using the AI providers.\nNote that Anthropic is one option; Moltbot supports other AI providers and so does AI Gateway. The advantage of using AI Gateway is that if a better model comes along from any provider, you don’t have to swap keys in your AI Agent configuration and redeploy — you can simply switch the model in your gateway configuration. And more, you specify model or provider fallbacks to handle request failures and ensure reliability.\nSandboxes\nLast year we anticipated the growing need for AI agents to run untrusted code securely in isolated environments, and we announced the Sandbox SDK. This SDK is built on top of Cloudflare Containers, but it provides a simple API for executing commands, managing files, running background processes, and exposing services — all from your Workers applications.\nIn short, instead of having to deal with the lower-level Container APIs, the Sandbox SDK gives you developer-friendly APIs for secure code execution and handles the complexity of container lifecycle, networking, file systems, and process management — letting you focus on building your application logic with just a few lines of TypeScript. Here’s an example:\nimport { getSandbox } from '@cloudflare/sandbox';\nexport { Sandbox } from '@cloudflare/sandbox';\n\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const sandbox = getSandbox(env.Sandbox, 'user-123');\n\n    // Create a project structure\n    await sandbox.mkdir('/workspace/project/src', { recursive: true });\n\n    // Check node version\n    const version = await sandbox.exec('node -v');\n\n    // Run some python code\n    const ctx = await sandbox.createCodeContext({ language: 'python' });\n    await sandbox.runCode('import math; radius = 5', { context: ctx });\n    const result = await sandbox.runCode('math.pi * radius ** 2', { context: ctx });\n\n    return Response.json({ version, result });\n  }\n};\nThis fits like a glove for Moltbot. Instead of running Docker in your local Mac mini, we run Docker on Containers, use the Sandbox SDK to issue commands into the isolated environment and use callbacks to our entrypoint Worker, effectively establishing a two-way communication channel between the two systems.\nR2 for persistent storage\nThe good thing about running things in your local computer or VPS is you get persistent storage for free. Containers, however, are inherently ephemeral, meaning data generated within them is lost upon deletion. Fear not, though — the Sandbox SDK provides the sandbox.mountBucket() that you can use to automatically, well, mount your R2 bucket as a filesystem partition when the container starts.\nOnce we have a local directory that is guaranteed to survive the container lifecycle, we can use that for Moltbot to store session memory files, conversations and other assets that are required to persist.\nBrowser Rendering for browser automation\nAI agents rely heavily on browsing the sometimes not-so-structured web. Moltbot utilizes dedicated Chromium instances to perform actions, navigate the web, fill out forms, take snapshots, and handle tasks that require a web browser. Sure, we can run Chromium on Sandboxes too, but what if we could simplify and use an API instead?\nWith Cloudflare’s Browser Rendering, you can programmatically control and interact with headless browser instances running at scale in our edge network. We support Puppeteer, Stagehand, Playwright and other popular packages so that developers can onboard with minimal code changes. We even support MCP for AI.\nIn order to get Browser Rendering to work with Moltbot we do two things:\n\nFirst we create a thin CDP proxy (CDP is the protocol that allows instrumenting Chromium-based browsers) from the Sandbox container to the Moltbot Worker, back to Browser Rendering using the Puppeteer APIs.\n\nThen we inject a Browser Rendering skill into the runtime when the Sandbox starts.\n\nFrom the Moltbot runtime perspective, it has a local CDP port it can connect to and perform browser tasks.\nZero Trust Access for authentication policies\nNext up we want to protect our APIs and Admin UI from unauthorized access. Doing authentication from scratch is hard, and is typically the kind of wheel you don’t want to reinvent or have to deal with. Zero Trust Access makes it incredibly easy to protect your application by defining specific policies and login methods for the endpoints. \nZero Trust Access Login methods configuration for the Moltworker application.\nOnce the endpoints are protected, Cloudflare will handle authentication for you and automatically include a JWT token with every request to your origin endpoints. You can then validate that JWT for extra protection, to ensure that the request came from Access and not a malicious third party.\nLike with AI Gateway, once all your APIs are behind Access you get great observability on who the users are and what they are doing with your Moltbot instance.\nMoltworker in action\nDemo time. We’ve put up a Slack instance where we could play with our own instance of Moltbot on Workers. Here are some of the fun things we’ve done with it.\nWe hate bad news.\nHere’s a chat session where we ask Moltbot to find the shortest route between Cloudflare in London and Cloudflare in Lisbon using Google Maps and take a screenshot in a Slack channel. It goes through a sequence of steps using Browser Rendering to navigate Google Maps and does a pretty good job at it. Also look at Moltbot’s memory in action when we ask him the second time.\nWe’re in the mood for some Asian food today, let’s get Moltbot to work for help.\nWe eat with our eyes too.\nLet’s get more creative and ask Moltbot to create a video where it browses our developer documentation. As you can see, it downloads and runs ffmpeg to generate the video out of the frames it captured in the browser.\n\n  \n\n    \nRun your own Moltworker\nWe open-sourced our implementation and made it available at https://github.com/cloudflare/moltworker, so you can deploy and run your own Moltbot on top of Workers today.\nThe README guides you through the necessary steps to set up everything. You will need a Cloudflare account and a minimum $5 USD Workers paid plan subscription to use Sandbox Containers, but all the other products are either free to use, like AI Gateway, or have generous free tiers you can use to get you started and run for as long as you want under reasonable limits.\nNote that Moltworker is a proof of concept, not a Cloudflare product. Our goal is to showcase some of the most exciting features of our Developer Platform that can be used to run AI agents and unsupervised code efficiently and securely, and get great observability while taking advantage of our global network.\nFeel free to contribute to or fork our GitHub repository; we will keep an eye on it for a while for support. We are also considering contributing upstream to the official project with Cloudflare skills in parallel.\nConclusion\nWe hope you enjoyed this experiment, and we were able to convince you that Cloudflare is the perfect place to run your AI applications and agents. We’ve been working relentlessly trying to anticipate the future and release features like the Agents SDK that you can use to build your first agent in minutes, Sandboxes where you can run arbitrary code in an isolated environment without the complications of the lifecycle of a container, and AI Search, Cloudflare’s managed vector-based search service, to name a few.\nCloudflare now offers a complete toolkit for AI development: inference, storage APIs, databases, durable execution for stateful workflows, and built-in AI capabilities. Together, these building blocks make it possible to build and run even the most demanding AI applications on our global edge network.\nIf you're excited about AI and want to help us build the next generation of products and APIs, we're hiring.","dc:creator":"Celso Martinho","content":" Moltworker is a middleware Worker and adapted scripts that allows running OpenClaw (formerly Moltbot, formerly Clawdbot) on Cloudflare's Sandbox SDK and our Developer Platform APIs. So you can self-host an AI personal assistant — without any new hardware. ","contentSnippet":"Moltworker is a middleware Worker and adapted scripts that allows running OpenClaw (formerly Moltbot, formerly Clawdbot) on Cloudflare's Sandbox SDK and our Developer Platform APIs. So you can self-host an AI personal assistant — without any new hardware.","guid":"45LuZGCXAcs7EMnB64zTQm","categories":["AI","Agents","Cloudflare Workers","Containers","Sandbox"],"isoDate":"2026-01-29T14:00:00.000Z"},{"creator":"Nick Kuntz","title":"Building a serverless, post-quantum Matrix homeserver","link":"https://blog.cloudflare.com/serverless-matrix-homeserver-workers/","pubDate":"Tue, 27 Jan 2026 14:00:00 GMT","content:encoded":" <p><sup><i>* </i></sup><sup><i>This post was updated at 11:45 a.m. Pacific time to clarify that the use case described here is a proof of concept and a personal project. Some sections have been updated for clarity.</i></sup></p><p>Matrix is the gold standard for decentralized, end-to-end encrypted communication. It powers government messaging systems, open-source communities, and privacy-focused organizations worldwide. </p><p>For the individual developer, however, the appeal is often closer to home: bridging fragmented chat networks (like Discord and Slack) into a single inbox, or simply ensuring your conversation history lives on infrastructure you control. Functionally, Matrix operates as a decentralized, eventually consistent state machine. Instead of a central server pushing updates, homeservers exchange signed JSON events over HTTP, using a conflict resolution algorithm to merge these streams into a unified view of the room's history.</p><p><b>But there is a \"tax\" to running it. </b>Traditionally, operating a Matrix <a href=\"https://matrix.org/homeserver/about/\"><u>homeserver</u></a> has meant accepting a heavy operational burden. You have to provision virtual private servers (VPS), tune PostgreSQL for heavy write loads, manage Redis for caching, configure <a href=\"https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/\"><u>reverse proxies</u></a>, and handle rotation for TLS certificates. It’s a stateful, heavy beast that demands to be fed time and money, whether you’re using it a lot or a little.</p><p>We wanted to see if we could eliminate that tax entirely.</p><p><b>Spoiler: We could.</b> In this post, we’ll explain how we ported a Matrix homeserver to <a href=\"https://workers.cloudflare.com/\"><u>Cloudflare Workers</u></a>. The resulting proof of concept is a serverless architecture where operations disappear, costs scale to zero when idle, and every connection is protected by <a href=\"https://www.cloudflare.com/learning/ssl/quantum/what-is-post-quantum-cryptography/\"><u>post-quantum cryptography</u></a> by default. You can view the source code and <a href=\"https://github.com/nkuntz1934/matrix-workers\"><u>deploy your own instance directly from Github</u></a>.</p><a href=\"https://deploy.workers.cloudflare.com/?url=https://github.com/nkuntz1934/matrix-workers\"><img src=\"https://deploy.workers.cloudflare.com/button\" /></a>\n<p></p><p></p>\n    <div>\n      <h2>From Synapse to Workers</h2>\n      <a href=\"#from-synapse-to-workers\">\n        \n      </a>\n    </div>\n    <p>Our starting point was <a href=\"https://github.com/matrix-org/synapse\"><u>Synapse</u></a>, the Python-based reference Matrix homeserver designed for traditional deployments. PostgreSQL for persistence, Redis for caching, filesystem for media.</p><p>Porting it to Workers meant questioning every storage assumption we’d taken for granted.</p><p>The challenge was storage. Traditional homeservers assume strong consistency via a central SQL database. Cloudflare <a href=\"https://developers.cloudflare.com/durable-objects/\"><u>Durable Objects</u></a> offers a powerful alternative. This primitive gives us the strong consistency and atomicity required for Matrix state resolution, while still allowing the application to run at the edge.</p><p>We ported the core Matrix protocol logic — event authorization, room state resolution, cryptographic verification — in TypeScript using the Hono framework. D1 replaces PostgreSQL, KV replaces Redis, R2 replaces the filesystem, and Durable Objects handle real-time coordination.</p><p>Here’s how the mapping worked out:</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1JTja38UZRbFygluawrnz1/9bce290e3070155c734e874c17051551/BLOG-3101_2.png\" />\n          </figure>\n    <div>\n      <h2>From monolith to serverless</h2>\n      <a href=\"#from-monolith-to-serverless\">\n        \n      </a>\n    </div>\n    <p>Moving to Cloudflare Workers brings several advantages for a developer: simple deployment, lower costs, low latency, and built-in security.</p><p><b>Easy deployment: </b>A traditional Matrix deployment requires server provisioning, PostgreSQL administration, Redis cluster management, TLS certificate renewal, load balancer configuration, monitoring infrastructure, and on-call rotations.</p><p>With Workers, deployment is simply: wrangler deploy. Workers handles TLS, load balancing, DDoS protection, and global distribution. </p><p><b>Usage-based costs: </b>Traditional homeservers cost money whether anyone is using them or not. Workers pricing is request-based, so you pay when you’re using it, but costs drop to near zero when everyone’s asleep. </p><p><b>Lower latency globally:</b> A traditional Matrix homeserver in us-east-1 adds 200ms+ latency for users in Asia or Europe. Workers, meanwhile, run in 300+ locations worldwide. When a user in Tokyo sends a message, the Worker executes in Tokyo. </p><p><b>Built-in security: </b>Matrix homeservers can be high-value targets: They handle encrypted communications, store message history, and authenticate users. Traditional deployments require careful hardening: firewall configuration, rate limiting, DDoS mitigation, WAF rules, IP reputation filtering.</p><p>Workers provide all of this by default. </p>\n    <div>\n      <h3>Post-quantum protection </h3>\n      <a href=\"#post-quantum-protection\">\n        \n      </a>\n    </div>\n    <p>Cloudflare deployed post-quantum hybrid key agreement across all <a href=\"https://www.cloudflare.com/learning/ssl/why-use-tls-1.3/\"><u>TLS 1.3</u></a> connections in <a href=\"https://blog.cloudflare.com/post-quantum-for-all/\"><u>October 2022</u></a>. Every connection to our Worker automatically negotiates X25519MLKEM768 — a hybrid combining classical X25519 with ML-KEM, the post-quantum algorithm standardized by NIST.</p><p>Classical cryptography relies on mathematical problems that are hard for traditional computers but trivial for quantum computers running Shor’s algorithm. ML-KEM is based on lattice problems that remain hard even for quantum computers. The hybrid approach means both algorithms must fail for the connection to be compromised.</p>\n    <div>\n      <h3>Following a message through the system</h3>\n      <a href=\"#following-a-message-through-the-system\">\n        \n      </a>\n    </div>\n    <p>Understanding where encryption happens matters for security architecture. When someone sends a message through our homeserver, here’s the actual path:</p><p>The sender’s client takes the plaintext message and encrypts it with Megolm — Matrix’s end-to-end encryption. This encrypted payload then gets wrapped in TLS for transport. On Cloudflare, that TLS connection uses X25519MLKEM768, making it quantum-resistant.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/wGGYZ4LYspufH1c4psmL1/28acad8ab8e6535525dda413669c2d74/BLOG-3101_3.png\" />\n          </figure><p>The Worker terminates TLS, but what it receives is still encrypted — the Megolm ciphertext. We store that ciphertext in D1, index it by room and timestamp, and deliver it to recipients. But we never see the plaintext. The message “Hello, world” exists only on the sender’s device and the recipient’s device.</p><p>When the recipient syncs, the process reverses. They receive the encrypted payload over another quantum-resistant TLS connection, then decrypt locally with their Megolm session keys.</p>\n    <div>\n      <h3>Two layers, independent protection</h3>\n      <a href=\"#two-layers-independent-protection\">\n        \n      </a>\n    </div>\n    <p>This protects via two encryption layers that operate independently:</p><p>The <a href=\"https://www.cloudflare.com/learning/ssl/transport-layer-security-tls/\"><u>transport layer (TLS)</u></a> protects data in transit. It’s encrypted at the client and decrypted at the Cloudflare edge. With X25519MLKEM768, this layer is now post-quantum.</p><p>The <a href=\"https://www.cloudflare.com/learning/ddos/what-is-layer-7/\"><u>application layer</u></a> (Megolm E2EE) protects message content. It’s encrypted on the sender’s device and decrypted only on recipient devices. This uses classical Curve25519 cryptography.</p>\n    <div>\n      <h3>Who sees what</h3>\n      <a href=\"#who-sees-what\">\n        \n      </a>\n    </div>\n    <p>Any Matrix homeserver operator — whether running Synapse on a VPS or this implementation on Workers — can see metadata: which rooms exist, who’s in them, when messages were sent. But no one in the infrastructure chain can see the message content, because the E2EE payload is encrypted on sender devices before it ever hits the network. Cloudflare terminates TLS and passes requests to your Worker, but both see only Megolm ciphertext. Media in encrypted rooms is encrypted client-side before upload, and private keys never leave user devices.</p>\n    <div>\n      <h3>What traditional deployments would need</h3>\n      <a href=\"#what-traditional-deployments-would-need\">\n        \n      </a>\n    </div>\n    <p>Achieving post-quantum TLS on a traditional Matrix deployment would require upgrading OpenSSL or BoringSSL to a version supporting ML-KEM, configuring cipher suite preferences correctly, testing client compatibility across all Matrix apps, monitoring for TLS negotiation failures, staying current as PQC standards evolve, and handling clients that don’t support PQC gracefully.</p><p>With Workers, it’s automatic. Chrome, Firefox, and Edge all support X25519MLKEM768. Mobile apps using platform TLS stacks inherit this support. The security posture improves as Cloudflare’s <a href=\"https://developers.cloudflare.com/ssl/post-quantum-cryptography/\"><u>PQC</u></a> deployment expands — no action required on our part.</p>\n    <div>\n      <h2>The storage architecture that made it work</h2>\n      <a href=\"#the-storage-architecture-that-made-it-work\">\n        \n      </a>\n    </div>\n    <p>The key insight from porting Tuwunel was that different data needs different consistency guarantees. We use each Cloudflare primitive for what it does best.</p>\n    <div>\n      <h3>D1 for the data model</h3>\n      <a href=\"#d1-for-the-data-model\">\n        \n      </a>\n    </div>\n    <p>D1 stores everything that needs to survive restarts and support queries: users, rooms, events, device keys. Over 25 tables covering the full Matrix data model. </p>\n            <pre><code>CREATE TABLE events (\n\tevent_id TEXT PRIMARY KEY,\n\troom_id TEXT NOT NULL,\n\tsender TEXT NOT NULL,\n\tevent_type TEXT NOT NULL,\n\tstate_key TEXT,\n\tcontent TEXT NOT NULL,\n\torigin_server_ts INTEGER NOT NULL,\n\tdepth INTEGER NOT NULL\n);\n</code></pre>\n            <p>D1’s SQLite foundation meant we could port Tuwunel’s queries with minimal changes. Joins, indexes, and aggregations work as expected.</p><p>We learned one hard lesson: D1’s eventual consistency breaks foreign key constraints. A write to rooms might not be visible when a subsequent write to events checks the foreign key. We removed all foreign keys and enforce referential integrity in application code.</p>\n    <div>\n      <h3>KV for ephemeral state</h3>\n      <a href=\"#kv-for-ephemeral-state\">\n        \n      </a>\n    </div>\n    <p>OAuth authorization codes live for 10 minutes, while refresh tokens last for a session.</p>\n            <pre><code>// Store OAuth code with 10-minute TTL\nkv.put(&amp;format!(\"oauth_code:{}\", code), &amp;token_data)?\n\t.expiration_ttl(600)\n\t.execute()\n\t.await?;</code></pre>\n            <p>KV’s global distribution means OAuth flows work fast regardless of where users are located.</p>\n    <div>\n      <h3>R2 for media</h3>\n      <a href=\"#r2-for-media\">\n        \n      </a>\n    </div>\n    <p>Matrix media maps directly to R2, so you can upload an image, get back a content-addressed URL – and egress is free.</p>\n    <div>\n      <h3>Durable Objects for atomicity</h3>\n      <a href=\"#durable-objects-for-atomicity\">\n        \n      </a>\n    </div>\n    <p>Some operations can’t tolerate eventual consistency. When a client claims a one-time encryption key, that key must be atomically removed. If two clients claim the same key, encrypted session establishment fails.</p><p>Durable Objects provide single-threaded, strongly consistent storage:</p>\n            <pre><code>#[durable_object]\npub struct UserKeysObject {\n\tstate: State,\n\tenv: Env,\n}\n\nimpl UserKeysObject {\n\tasync fn claim_otk(&amp;self, algorithm: &amp;str) -&gt; Result&lt;Option&lt;Key&gt;&gt; {\n    \t// Atomic within single DO - no race conditions possible\n    \tlet mut keys: Vec&lt;Key&gt; = self.state.storage()\n        \t.get(\"one_time_keys\")\n        \t.await\n        \t.ok()\n        \t.flatten()\n        \t.unwrap_or_default();\n\n    \tif let Some(idx) = keys.iter().position(|k| k.algorithm == algorithm) {\n        \tlet key = keys.remove(idx);\n        \tself.state.storage().put(\"one_time_keys\", &amp;keys).await?;\n        \treturn Ok(Some(key));\n    \t}\n    \tOk(None)\n\t}\n}</code></pre>\n            <p>We use UserKeysObject for E2EE key management, RoomObject for real-time room events like typing indicators and read receipts, and UserSyncObject for to-device message queues. The rest flows through D1.</p>\n    <div>\n      <h3>Complete end-to-end encryption, complete OAuth</h3>\n      <a href=\"#complete-end-to-end-encryption-complete-oauth\">\n        \n      </a>\n    </div>\n    <p>Our implementation supports the full Matrix E2EE stack: device keys, cross-signing keys, one-time keys, fallback keys, key backup, and dehydrated devices.</p><p>Modern Matrix clients use OAuth 2.0/OIDC instead of legacy password flows. We implemented a complete OAuth provider, with dynamic client registration, PKCE authorization, RS256-signed JWT tokens, token refresh with rotation, and standard OIDC discovery endpoints.\n</p>\n            <pre><code>curl https://matrix.example.com/.well-known/openid-configuration\n{\n  \"issuer\": \"https://matrix.example.com\",\n  \"authorization_endpoint\": \"https://matrix.example.com/oauth/authorize\",\n  \"token_endpoint\": \"https://matrix.example.com/oauth/token\",\n  \"jwks_uri\": \"https://matrix.example.com/.well-known/jwks.json\"\n}\n</code></pre>\n            <p>Point Element or any Matrix client at the domain, and it discovers everything automatically.</p>\n    <div>\n      <h2>Sliding Sync for mobile</h2>\n      <a href=\"#sliding-sync-for-mobile\">\n        \n      </a>\n    </div>\n    <p>Traditional Matrix sync transfers megabytes of data on initial connection,  draining mobile battery and data plans.</p><p>Sliding Sync lets clients request exactly what they need. Instead of downloading everything, clients get the 20 most recent rooms with minimal state. As users scroll, they request more ranges. The server tracks position and sends only deltas.</p><p>Combined with edge execution, mobile clients can connect and render their room list in under 500ms, even on slow networks.</p>\n    <div>\n      <h2>The comparison</h2>\n      <a href=\"#the-comparison\">\n        \n      </a>\n    </div>\n    <p>For a homeserver serving a small team:</p><table><tr><th><p> </p></th><th><p><b>Traditional (VPS)</b></p></th><th><p><b>Workers</b></p></th></tr><tr><td><p>Monthly cost (idle)</p></td><td><p>$20-50</p></td><td><p>&lt;$1</p></td></tr><tr><td><p>Monthly cost (active)</p></td><td><p>$20-50</p></td><td><p>$3-10</p></td></tr><tr><td><p>Global latency</p></td><td><p>100-300ms</p></td><td><p>20-50ms</p></td></tr><tr><td><p>Time to deploy</p></td><td><p>Hours</p></td><td><p>Seconds</p></td></tr><tr><td><p>Maintenance</p></td><td><p>Weekly</p></td><td><p>None</p></td></tr><tr><td><p>DDoS protection</p></td><td><p>Additional cost</p></td><td><p>Included</p></td></tr><tr><td><p>Post-quantum TLS</p></td><td><p>Complex setup</p></td><td><p>Automatic</p></td></tr></table><p><sup>*</sup><sup><i>Based on public rates and metrics published by DigitalOcean, AWS Lightsail, and Linode as of January 15, 2026.</i></sup></p><p>The economics improve further at scale. Traditional deployments require capacity planning and over-provisioning. Workers scale automatically.</p>\n    <div>\n      <h2>The future of decentralized protocols</h2>\n      <a href=\"#the-future-of-decentralized-protocols\">\n        \n      </a>\n    </div>\n    <p>We started this as an experiment: could Matrix run on Workers? It can—and the approach can work for other stateful protocols, too.</p><p>By mapping traditional stateful components to Cloudflare’s primitives — Postgres to D1, Redis to KV, mutexes to Durable Objects — we can see  that complex applications don't need complex infrastructure. We stripped away the operating system, the database management, and the network configuration, leaving only the application logic and the data itself.</p><p>Workers offers the sovereignty of owning your data, without the burden of owning the infrastructure.</p><p>I have been experimenting with the implementation and am excited for any contributions from others interested in this kind of service. </p><p>Ready to build powerful, real-time applications on Workers? Get started with<a href=\"https://developers.cloudflare.com/workers/\"> <u>Cloudflare Workers</u></a> and explore<a href=\"https://developers.cloudflare.com/durable-objects/\"> <u>Durable Objects</u></a> for your own stateful edge applications. Join our<a href=\"https://discord.cloudflare.com\"> <u>Discord community</u></a> to connect with other developers building at the edge.</p> ","content:encodedSnippet":"* This post was updated at 11:45 a.m. Pacific time to clarify that the use case described here is a proof of concept and a personal project. Some sections have been updated for clarity.\nMatrix is the gold standard for decentralized, end-to-end encrypted communication. It powers government messaging systems, open-source communities, and privacy-focused organizations worldwide. \nFor the individual developer, however, the appeal is often closer to home: bridging fragmented chat networks (like Discord and Slack) into a single inbox, or simply ensuring your conversation history lives on infrastructure you control. Functionally, Matrix operates as a decentralized, eventually consistent state machine. Instead of a central server pushing updates, homeservers exchange signed JSON events over HTTP, using a conflict resolution algorithm to merge these streams into a unified view of the room's history.\nBut there is a \"tax\" to running it. Traditionally, operating a Matrix homeserver has meant accepting a heavy operational burden. You have to provision virtual private servers (VPS), tune PostgreSQL for heavy write loads, manage Redis for caching, configure reverse proxies, and handle rotation for TLS certificates. It’s a stateful, heavy beast that demands to be fed time and money, whether you’re using it a lot or a little.\nWe wanted to see if we could eliminate that tax entirely.\nSpoiler: We could. In this post, we’ll explain how we ported a Matrix homeserver to Cloudflare Workers. The resulting proof of concept is a serverless architecture where operations disappear, costs scale to zero when idle, and every connection is protected by post-quantum cryptography by default. You can view the source code and deploy your own instance directly from Github.\n\n\n\nFrom Synapse to Workers\nOur starting point was Synapse, the Python-based reference Matrix homeserver designed for traditional deployments. PostgreSQL for persistence, Redis for caching, filesystem for media.\nPorting it to Workers meant questioning every storage assumption we’d taken for granted.\nThe challenge was storage. Traditional homeservers assume strong consistency via a central SQL database. Cloudflare Durable Objects offers a powerful alternative. This primitive gives us the strong consistency and atomicity required for Matrix state resolution, while still allowing the application to run at the edge.\nWe ported the core Matrix protocol logic — event authorization, room state resolution, cryptographic verification — in TypeScript using the Hono framework. D1 replaces PostgreSQL, KV replaces Redis, R2 replaces the filesystem, and Durable Objects handle real-time coordination.\nHere’s how the mapping worked out:\nFrom monolith to serverless\nMoving to Cloudflare Workers brings several advantages for a developer: simple deployment, lower costs, low latency, and built-in security.\nEasy deployment: A traditional Matrix deployment requires server provisioning, PostgreSQL administration, Redis cluster management, TLS certificate renewal, load balancer configuration, monitoring infrastructure, and on-call rotations.\nWith Workers, deployment is simply: wrangler deploy. Workers handles TLS, load balancing, DDoS protection, and global distribution. \nUsage-based costs: Traditional homeservers cost money whether anyone is using them or not. Workers pricing is request-based, so you pay when you’re using it, but costs drop to near zero when everyone’s asleep. \nLower latency globally: A traditional Matrix homeserver in us-east-1 adds 200ms+ latency for users in Asia or Europe. Workers, meanwhile, run in 300+ locations worldwide. When a user in Tokyo sends a message, the Worker executes in Tokyo. \nBuilt-in security: Matrix homeservers can be high-value targets: They handle encrypted communications, store message history, and authenticate users. Traditional deployments require careful hardening: firewall configuration, rate limiting, DDoS mitigation, WAF rules, IP reputation filtering.\nWorkers provide all of this by default. \nPost-quantum protection \nCloudflare deployed post-quantum hybrid key agreement across all TLS 1.3 connections in October 2022. Every connection to our Worker automatically negotiates X25519MLKEM768 — a hybrid combining classical X25519 with ML-KEM, the post-quantum algorithm standardized by NIST.\nClassical cryptography relies on mathematical problems that are hard for traditional computers but trivial for quantum computers running Shor’s algorithm. ML-KEM is based on lattice problems that remain hard even for quantum computers. The hybrid approach means both algorithms must fail for the connection to be compromised.\nFollowing a message through the system\nUnderstanding where encryption happens matters for security architecture. When someone sends a message through our homeserver, here’s the actual path:\nThe sender’s client takes the plaintext message and encrypts it with Megolm — Matrix’s end-to-end encryption. This encrypted payload then gets wrapped in TLS for transport. On Cloudflare, that TLS connection uses X25519MLKEM768, making it quantum-resistant.\nThe Worker terminates TLS, but what it receives is still encrypted — the Megolm ciphertext. We store that ciphertext in D1, index it by room and timestamp, and deliver it to recipients. But we never see the plaintext. The message “Hello, world” exists only on the sender’s device and the recipient’s device.\nWhen the recipient syncs, the process reverses. They receive the encrypted payload over another quantum-resistant TLS connection, then decrypt locally with their Megolm session keys.\nTwo layers, independent protection\nThis protects via two encryption layers that operate independently:\nThe transport layer (TLS) protects data in transit. It’s encrypted at the client and decrypted at the Cloudflare edge. With X25519MLKEM768, this layer is now post-quantum.\nThe application layer (Megolm E2EE) protects message content. It’s encrypted on the sender’s device and decrypted only on recipient devices. This uses classical Curve25519 cryptography.\nWho sees what\nAny Matrix homeserver operator — whether running Synapse on a VPS or this implementation on Workers — can see metadata: which rooms exist, who’s in them, when messages were sent. But no one in the infrastructure chain can see the message content, because the E2EE payload is encrypted on sender devices before it ever hits the network. Cloudflare terminates TLS and passes requests to your Worker, but both see only Megolm ciphertext. Media in encrypted rooms is encrypted client-side before upload, and private keys never leave user devices.\nWhat traditional deployments would need\nAchieving post-quantum TLS on a traditional Matrix deployment would require upgrading OpenSSL or BoringSSL to a version supporting ML-KEM, configuring cipher suite preferences correctly, testing client compatibility across all Matrix apps, monitoring for TLS negotiation failures, staying current as PQC standards evolve, and handling clients that don’t support PQC gracefully.\nWith Workers, it’s automatic. Chrome, Firefox, and Edge all support X25519MLKEM768. Mobile apps using platform TLS stacks inherit this support. The security posture improves as Cloudflare’s PQC deployment expands — no action required on our part.\nThe storage architecture that made it work\nThe key insight from porting Tuwunel was that different data needs different consistency guarantees. We use each Cloudflare primitive for what it does best.\nD1 for the data model\nD1 stores everything that needs to survive restarts and support queries: users, rooms, events, device keys. Over 25 tables covering the full Matrix data model. \nCREATE TABLE events (\n\tevent_id TEXT PRIMARY KEY,\n\troom_id TEXT NOT NULL,\n\tsender TEXT NOT NULL,\n\tevent_type TEXT NOT NULL,\n\tstate_key TEXT,\n\tcontent TEXT NOT NULL,\n\torigin_server_ts INTEGER NOT NULL,\n\tdepth INTEGER NOT NULL\n);\n\nD1’s SQLite foundation meant we could port Tuwunel’s queries with minimal changes. Joins, indexes, and aggregations work as expected.\nWe learned one hard lesson: D1’s eventual consistency breaks foreign key constraints. A write to rooms might not be visible when a subsequent write to events checks the foreign key. We removed all foreign keys and enforce referential integrity in application code.\nKV for ephemeral state\nOAuth authorization codes live for 10 minutes, while refresh tokens last for a session.\n// Store OAuth code with 10-minute TTL\nkv.put(&format!(\"oauth_code:{}\", code), &token_data)?\n\t.expiration_ttl(600)\n\t.execute()\n\t.await?;\nKV’s global distribution means OAuth flows work fast regardless of where users are located.\nR2 for media\nMatrix media maps directly to R2, so you can upload an image, get back a content-addressed URL – and egress is free.\nDurable Objects for atomicity\nSome operations can’t tolerate eventual consistency. When a client claims a one-time encryption key, that key must be atomically removed. If two clients claim the same key, encrypted session establishment fails.\nDurable Objects provide single-threaded, strongly consistent storage:\n#[durable_object]\npub struct UserKeysObject {\n\tstate: State,\n\tenv: Env,\n}\n\nimpl UserKeysObject {\n\tasync fn claim_otk(&self, algorithm: &str) -> Result<Option<Key>> {\n    \t// Atomic within single DO - no race conditions possible\n    \tlet mut keys: Vec<Key> = self.state.storage()\n        \t.get(\"one_time_keys\")\n        \t.await\n        \t.ok()\n        \t.flatten()\n        \t.unwrap_or_default();\n\n    \tif let Some(idx) = keys.iter().position(|k| k.algorithm == algorithm) {\n        \tlet key = keys.remove(idx);\n        \tself.state.storage().put(\"one_time_keys\", &keys).await?;\n        \treturn Ok(Some(key));\n    \t}\n    \tOk(None)\n\t}\n}\nWe use UserKeysObject for E2EE key management, RoomObject for real-time room events like typing indicators and read receipts, and UserSyncObject for to-device message queues. The rest flows through D1.\nComplete end-to-end encryption, complete OAuth\nOur implementation supports the full Matrix E2EE stack: device keys, cross-signing keys, one-time keys, fallback keys, key backup, and dehydrated devices.\nModern Matrix clients use OAuth 2.0/OIDC instead of legacy password flows. We implemented a complete OAuth provider, with dynamic client registration, PKCE authorization, RS256-signed JWT tokens, token refresh with rotation, and standard OIDC discovery endpoints.\n\n            \ncurl https://matrix.example.com/.well-known/openid-configuration\n{\n  \"issuer\": \"https://matrix.example.com\",\n  \"authorization_endpoint\": \"https://matrix.example.com/oauth/authorize\",\n  \"token_endpoint\": \"https://matrix.example.com/oauth/token\",\n  \"jwks_uri\": \"https://matrix.example.com/.well-known/jwks.json\"\n}\n\nPoint Element or any Matrix client at the domain, and it discovers everything automatically.\nSliding Sync for mobile\nTraditional Matrix sync transfers megabytes of data on initial connection,  draining mobile battery and data plans.\nSliding Sync lets clients request exactly what they need. Instead of downloading everything, clients get the 20 most recent rooms with minimal state. As users scroll, they request more ranges. The server tracks position and sends only deltas.\nCombined with edge execution, mobile clients can connect and render their room list in under 500ms, even on slow networks.\nThe comparison\nFor a homeserver serving a small team:\n\n\n \nTraditional (VPS)\n\nWorkers\n\n\nMonthly cost (idle)\n\n$20-50\n\n<$1\n\n\nMonthly cost (active)\n\n$20-50\n\n$3-10\n\n\nGlobal latency\n\n100-300ms\n\n20-50ms\n\n\nTime to deploy\n\nHours\n\nSeconds\n\n\nMaintenance\n\nWeekly\n\nNone\n\n\nDDoS protection\n\nAdditional cost\n\nIncluded\n\n\nPost-quantum TLS\n\nComplex setup\n\nAutomatic\n\n\n*Based on public rates and metrics published by DigitalOcean, AWS Lightsail, and Linode as of January 15, 2026.\nThe economics improve further at scale. Traditional deployments require capacity planning and over-provisioning. Workers scale automatically.\nThe future of decentralized protocols\nWe started this as an experiment: could Matrix run on Workers? It can—and the approach can work for other stateful protocols, too.\nBy mapping traditional stateful components to Cloudflare’s primitives — Postgres to D1, Redis to KV, mutexes to Durable Objects — we can see  that complex applications don't need complex infrastructure. We stripped away the operating system, the database management, and the network configuration, leaving only the application logic and the data itself.\nWorkers offers the sovereignty of owning your data, without the burden of owning the infrastructure.\nI have been experimenting with the implementation and am excited for any contributions from others interested in this kind of service. \nReady to build powerful, real-time applications on Workers? Get started with Cloudflare Workers and explore Durable Objects for your own stateful edge applications. Join our Discord community to connect with other developers building at the edge.","dc:creator":"Nick Kuntz","content":" As a proof of concept, we built a Matrix homeserver to Cloudflare Workers — delivering encrypted messaging at the edge with automatic post-quantum cryptography. ","contentSnippet":"As a proof of concept, we built a Matrix homeserver to Cloudflare Workers — delivering encrypted messaging at the edge with automatic post-quantum cryptography.","guid":"6VOVAMNwIZ18hMaUlC6aqp","categories":["Cloudflare Workers","Durable Objects","D1","Cloudflare Workers KV","R2","Security","Developer Platform","Developers","Rust","WebAssembly","Post-Quantum","Encryption"],"isoDate":"2026-01-27T14:00:00.000Z"},{"creator":"David Belson","title":"Cable cuts, storms, and DNS: a look at Internet disruptions in Q4 2025","link":"https://blog.cloudflare.com/q4-2025-internet-disruption-summary/","pubDate":"Mon, 26 Jan 2026 14:00:00 GMT","content:encoded":" <p>In 2025, we <a href=\"https://radar.cloudflare.com/outage-center?dateStart=2025-01-01&amp;dateEnd=2025-12-31\"><u>observed over 180 Internet disruptions</u></a> spurred by a variety of causes – some were brief and partial, while others were complete outages lasting for days. In the fourth quarter, we tracked only a single <a href=\"#government-directed\"><u>government-directed</u></a> Internet shutdown, but multiple <a href=\"#cable-cuts\"><u>cable cuts</u></a> wreaked havoc on connectivity in several countries. <a href=\"#power-outages\"><u>Power outages</u></a> and <a href=\"#weather\"><u>extreme weather</u></a> disrupted Internet services in multiple places, and the ongoing <a href=\"#military-action\"><u>conflict</u></a> in Ukraine impacted connectivity there as well. As always, a number of the disruptions we observed were due to <a href=\"#known-or-unspecified-technical-problems\"><u>technical problems</u></a> – with some acknowledged by the relevant providers, while others had unknown causes. In addition, incidents at several hyperscaler <a href=\"#cloud-platforms\"><u>cloud platforms</u></a> and <a href=\"#cloudflare\"><u>Cloudflare</u></a> impacted the availability of websites and applications.  </p><p>This post is intended as a summary overview of observed and confirmed disruptions and is not an exhaustive or complete list of issues that have occurred during the quarter. These anomalies are detected through significant deviations from expected traffic patterns observed across our network. Check out the <a href=\"https://radar.cloudflare.com/outage-center\"><u>Cloudflare Radar Outage Center</u></a> for a full list of verified anomalies and confirmed outages. </p>\n    <div>\n      <h2>Government-directed</h2>\n      <a href=\"#government-directed\">\n        \n      </a>\n    </div>\n    \n    <div>\n      <h3>Tanzania</h3>\n      <a href=\"#tanzania\">\n        \n      </a>\n    </div>\n    <p><a href=\"https://bsky.app/profile/radar.cloudflare.com/post/3m4df6i7hjk25\"><u>The Internet was shut down in Tanzania</u></a> on October 29 as <a href=\"https://www.theguardian.com/world/2025/oct/29/tanzania-election-president-samia-suluhu-hassan-poised-to-retain-power\"><u>violent protests</u></a> took place during the country’s presidential election. Traffic initially fell around 12:30 local time (09:30 UTC), dropping more than 90% lower than the previous week. The disruption lasted approximately 26 hours, with <a href=\"https://bsky.app/profile/radar.cloudflare.com/post/3m4qec7zdnt2u\"><u>traffic beginning to return</u></a> around 14:30 local time (11:30 UTC) on October 30. However, that restoration <a href=\"https://bsky.app/profile/radar.cloudflare.com/post/3m4gjngzck72u\"><u>proved to be quite brief</u></a>, with a significant decrease in traffic occurring around 16:15 local time (13:15 UTC), approximately two hours after it returned. This second near-complete outage lasted until November 3, <a href=\"https://bsky.app/profile/radar.cloudflare.com/post/3m4g47vasfm2u\"><u>when traffic aggressively returned</u></a> after 17:00 local time (14:00 UTC). Nominal drops in <a href=\"https://radar.cloudflare.com/routing/tz?dateStart=2025-10-29&amp;dateEnd=2025-11-04#announced-ip-address-space\"><u>announced IPv4 and IPv6 address space</u></a> were also observed during the shutdown, but there was never a complete loss of announcements, which would have signified a total disconnection of the country from the Internet. (<a href=\"https://www.cloudflare.com/learning/network-layer/what-is-an-autonomous-system/\"><u>Autonomous systems</u></a> announce IP address space to other Internet providers, letting them know what blocks of IP addresses they are responsible for.)</p><p>Tanzania’s president later <a href=\"https://apnews.com/article/tanzania-samia-suluhu-hassan-internet-shutdown-october-election-1ec66b897e7809865d8971699a7284e0\"><u>expressed sympathy</u></a> for the members of the diplomatic community and foreigners residing in the country regarding the impact of the Internet shutdown. Internet and social media services were also <a href=\"https://www.dw.com/en/tanzania-internet-slowdown-comes-at-a-high-cost/a-55512732\"><u>restricted in 2020</u></a> ahead of the country’s general elections.</p>\n    <div>\n      <h2>Cable cuts</h2>\n      <a href=\"#cable-cuts\">\n        \n      </a>\n    </div>\n    \n    <div>\n      <h3>Digicel Haiti</h3>\n      <a href=\"#digicel-haiti\">\n        \n      </a>\n    </div>\n    <p>Digicel Haiti is unfortunately no stranger to Internet disruptions caused by cable cuts, and the network experienced two more such incidents during the fourth quarter. On October 16, traffic from <a href=\"https://radar.cloudflare.com/as27653\"><u>Digicel Haiti (AS27653)</u></a> began to fall at 14:30 local time (18:30 UTC), reaching near zero at 16:00 local time (20:00 UTC). A translated <a href=\"https://x.com/jpbrun30/status/1978920959089230003\"><u>X post from the company’s Director General</u></a> noted: “<i>We advise our clientele that @DigicelHT is experiencing 2 cuts on its international fiber optic infrastructure.</i>” Traffic began to recover after 17:00 local time (21:00 UTC), and reached expected levels within the following hour. At 17:33 local time (21:34 UTC), the Director General <a href=\"https://x.com/jpbrun30/status/1978937426841063504\"><u>posted</u></a> that “<i>the first fiber on the international infrastructure has been repaired” </i>and service had been restored. </p><p>On November 25, another translated <a href=\"https://x.com/jpbrun30/status/1993283730467963345\"><u>X post from the provider’s Director General</u></a> stated that its “<i>international optical fiber infrastructure on National Road 1</i>” had been cut. We observed traffic dropping on Digicel’s network approximately an hour earlier, with a complete outage observed between 02:00 - 08:00 local time (07:00 - 13:00 UTC). A <a href=\"https://x.com/jpbrun30/status/1993309357438910484\"><u>follow-on X post</u></a> at 08:22 local time (13:22 UTC) stated that all services had been restored.</p>\n    <div>\n      <h3>Cybernet/StormFiber (Pakistan)</h3>\n      <a href=\"#cybernet-stormfiber-pakistan\">\n        \n      </a>\n    </div>\n    <p>At 17:30 local time (12:30 UTC) on October 20, Internet traffic for <a href=\"https://radar.cloudflare.com/as9541\"><u>Cybernet/StormFiber (AS9541)</u></a> dropped sharply, falling to a level approximately 50% the same time a week prior. At the same time, the network’s announced IPv4 address space dropped by over a third. The cause of these shifts was damage to the <a href=\"https://www.submarinecablemap.com/submarine-cable/peace-cable\"><u>PEACE</u></a> submarine cable, which suffered a cut in the Red Sea near Sudan. </p><p>PEACE is one of several submarine cable systems (including <a href=\"https://www.submarinecablemap.com/submarine-cable/imewe\"><u>IMEWE</u></a> and <a href=\"https://www.submarinecablemap.com/submarine-cable/seamewe-4\"><u>SEA-ME-WE-4</u></a>) that carry international Internet traffic for Pakistani providers. The provider <a href=\"https://profit.pakistantoday.com.pk/2025/10/24/stormfiber-pledges-full-restoration-by-monday-after-weeklong-internet-disruptions/\"><u>pledged to fully restore service</u></a> by October 27, but traffic and announced IPv4 address space had recovered to near expected levels by around 02:00 local time on October 21 (21:00 UTC on October 20).</p>\n<p>\n\n\n    </p><div>\n      <h3>Camtel, MTN Cameroon, Orange Cameroun</h3>\n      <a href=\"#camtel-mtn-cameroon-orange-cameroun\">\n        \n      </a>\n    </div>\n    <p>Unusual traffic patterns observed across multiple Internet providers in Cameroon on October 23 were reportedly caused by problems on the <a href=\"https://www.submarinecablemap.com/submarine-cable/west-africa-cable-system-wacs\"><u>WACS (West Africa Cable System)</u></a> submarine cable, which connects countries along the west coast of Africa to Portugal. </p><p>A (translated) <a href=\"https://teleasu.tv/internet-graves-perturbations-observees-ce-jeudi-23-octobre-2025/\"><u>published report</u></a> stated that MTN informed subscribers that “<i>following an incident on the WACS fiber optic cable, Internet service is temporarily disrupted</i>” and Orange Cameroun informed subscribers that “<i>due to an incident on the international access fiber, Internet service is disrupted.</i>” An <a href=\"https://x.com/Camtelonline/status/1981424170316464390\"><u>X post from Camtel</u></a> stated “<i>Cameroon Telecommunications (CAMTEL) wishes to inform the public that a technical incident involving WACS cable equipment in Batoke (LIMBE) occurred in the early hours of 23 October 2025, causing Internet connectivity disruptions throughout the country.</i>” </p><p>Traffic across the impacted providers originally fell just at around  05:00 local time (04:00 UTC) before recovering to expected levels around 22:00 local time (21:00 UTC). Traffic across these networks was quite volatile during the day, dropping 90-99% at times. It isn’t clear what caused the visible spikiness in the traffic pattern—possibly attempts to shift Internet traffic to <a href=\"https://www.submarinecablemap.com/country/cameroon\"><u>other submarine cable systems that connect to Cameroon</u></a>. Announced IP address space from <a href=\"https://radar.cloudflare.com/routing/as30992?dateStart=2025-10-23&amp;dateEnd=2025-10-23#announced-ip-address-space\"><u>MTN Cameroon</u></a> and <a href=\"https://radar.cloudflare.com/routing/as36912?dateStart=2025-10-23&amp;dateEnd=2025-10-23#announced-ip-address-space\"><u>Orange Cameroon</u></a> dropped during this period as well, although <a href=\"https://radar.cloudflare.com/routing/as15964?dateStart=2025-10-23&amp;dateEnd=2025-10-23#announced-ip-address-space\"><u>Camtel’s</u></a> announced IP address space did not change.</p><p>Connectivity in the <a href=\"https://radar.cloudflare.com/cf\"><u>Central African Republic</u></a> and <a href=\"https://radar.cloudflare.com/cg\"><u>Republic of Congo</u></a> was also reportedly impacted by the WACS issues.</p>\n\n\n\n    <div>\n      <h3>Claro Dominicana</h3>\n      <a href=\"#claro-dominicana\">\n        \n      </a>\n    </div>\n    <p>On December 9, we saw traffic from <a href=\"https://radar.cloudflare.com/as6400\"><u>Claro Dominicana (AS6400)</u></a>, an Internet provider in the Dominican Republic, drop sharply around 12:15 local time (16:15 UTC). Traffic levels fell again around 14:15 local time (18:15 UTC), bottoming out 77% lower than the previous week before quickly returning to expected levels. The connectivity disruption was likely caused by two fiber optic outages, as an <a href=\"https://x.com/ClaroRD/status/1998468046311002183\"><u>X post from the provider</u></a> during the outage noted that they were “causing intermittency and slowness in some services.” A <a href=\"https://x.com/ClaroRD/status/1998496113838764343\"><u>subsequent post on X</u></a> from Claro stated that technicians had restored Internet services nationwide by repairing the severed fiber optic cables.</p>\n    <div>\n      <h2>Power outages</h2>\n      <a href=\"#power-outages\">\n        \n      </a>\n    </div>\n    \n    <div>\n      <h3>Dominican Republic</h3>\n      <a href=\"#dominican-republic\">\n        \n      </a>\n    </div>\n    <p>According to a (translated) <a href=\"https://x.com/ETED_RD/status/1988326178219061450\"><u>X post from the Empresa de Transmisión Eléctrica Dominicana</u></a> (ETED), a transmission line outage caused an interruption in electrical service in the <a href=\"https://radar.cloudflare.com/do\"><u>Dominican Republic</u></a> on November 11. This power outage impacted Internet traffic from the country, resulting in a <a href=\"https://noc.social/@cloudflareradar/115533081511310085\"><u>nearly 50% drop in traffic</u></a> compared to the prior week, starting at 13:15 local time (17:15 UTC). Traffic levels remained lower until approximately 02:00 local time (06:00 UTC) on December 12, with a later <a href=\"https://x.com/ETED_RD/status/1988575130990330153\"><u>(translated) X post from ETED</u></a> noting “<i>At 2:20 a.m. we have completed the recovery of the national electrical system, supplying 96% of the demand…</i>”</p><p>A subsequent <a href=\"https://dominicantoday.com/dr/local/2025/11/27/manual-line-disconnection-triggered-nationwide-blackout-report-says/\"><u>technical report found</u></a> that “<i>the blackout began at the 138 kV San Pedro de Macorís I substation, where a live line was manually disconnected, triggering a high-intensity short circuit. Protection systems responded immediately, but the fault caused several nearby lines to disconnect, separating 575 MW of generation in the eastern region from the rest of the grid. The imbalance caused major power plants to trip automatically as part of their built-in safety mechanisms.</i>”</p>\n    <div>\n      <h3>Kenya</h3>\n      <a href=\"#kenya\">\n        \n      </a>\n    </div>\n    <p>On December 9, a <a href=\"https://www.tuko.co.ke/kenya/612181-kenya-power-reveals-7-pm-nationwide-blackout-multiple-regions/\"><u>major power outage</u></a> impacted multiple regions across <a href=\"https://radar.cloudflare.com/ke\"><u>Kenya</u></a>. Kenya Power explained that the outage “<i>was triggered by an incident on the regional Kenya-Uganda interconnected power network, which caused a disturbance on the Kenyan side of the system</i>” and claimed that “<i>[p]ower was restored to most of the affected areas within approximately 30 minutes.</i>” However, impacts to Internet connectivity lasted for nearly four hours, between 19:15 - 23:00 local time (16:15 - 20:00 UTC). The power outage caused traffic to drop as much as 18% at a national level, with the traffic shifts most visible in <a href=\"https://radar.cloudflare.com/traffic/7668902\"><u>Nakuru County</u></a> and <a href=\"https://radar.cloudflare.com/traffic/192709\"><u>Kaimbu County</u></a>.</p>\n\n\n    <div>\n      <h2>Military action</h2>\n      <a href=\"#military-action\">\n        \n      </a>\n    </div>\n    \n    <div>\n      <h3>Odesa, Ukraine</h3>\n      <a href=\"#odesa-ukraine\">\n        \n      </a>\n    </div>\n    <p><a href=\"https://odessa-journal.com/russia-carried-out-a-massive-drone-attack-on-the-odessa-region\"><u>Russian drone strikes</u></a> on the <a href=\"https://radar.cloudflare.com/traffic/698738\"><u>Odesa region</u></a> in <a href=\"https://radar.cloudflare.com/ua\"><u>Ukraine</u></a> on December 12 damaged warehouses and energy infrastructure, with the latter causing power outages in parts of the region. Those outages disrupted Internet connectivity, resulting in <a href=\"https://x.com/CloudflareRadar/status/2000993223406211327?s=20\"><u>traffic dropping by as much as 57%</u></a> as compared to the prior week. After the initial drop at midnight on December 13 (22:00 UTC on December 12), traffic gradually recovered over the following several days, returning to expected levels around 14:30 local time (12:30 UTC) on December 16.</p>\n    <div>\n      <h2>Weather</h2>\n      <a href=\"#weather\">\n        \n      </a>\n    </div>\n    \n    <div>\n      <h3>Jamaica</h3>\n      <a href=\"#jamaica\">\n        \n      </a>\n    </div>\n    <p><a href=\"https://www.nytimes.com/live/2025/10/28/weather/hurricane-melissa-jamaica-landfall?smid=url-share#df989e67-a90e-50fb-92d0-8d5d52f76e84\"><u>Hurricane Melissa</u></a> made landfall on <a href=\"https://radar.cloudflare.com/jm\"><u>Jamaica</u></a> on October 28 and left a trail of damage and destruction in its path. Associated <a href=\"https://www.jamaicaobserver.com/2025/10/28/eyeonmelissa-35-jps-customers-without-power/\"><u>power outages</u></a> and infrastructure damage impacted Internet connectivity, causing traffic to initially <a href=\"https://x.com/CloudflareRadar/status/1983266694715084866\"><u>drop by approximately half</u></a>, <a href=\"https://x.com/CloudflareRadar/status/1983217966347866383\"><u>starting</u></a> around 06:15 local time (11:15 UTC), ultimately reaching as much as <a href=\"https://x.com/CloudflareRadar/status/1983357587707048103\"><u>70% lower</u></a> than the previous week. Internet traffic from Jamaica remained well below pre-hurricane levels for several days, and ultimately started to make greater progress towards expected levels <a href=\"https://x.com/CloudflareRadar/status/1985708253872107713?s=20\"><u>during the morning of November 4</u></a>. It can often take weeks or months for Internet traffic from a country to return to “normal” levels following storms that cause massive and widespread damage – while power may be largely restored within several days, damage to physical infrastructure takes significantly longer to address.</p>\n    <div>\n      <h3>Sri Lanka &amp; Indonesia</h3>\n      <a href=\"#sri-lanka-indonesia\">\n        \n      </a>\n    </div>\n    <p>On November 26, <a href=\"https://apnews.com/article/indonesia-sri-lanka-thailand-malaysia-floods-landsides-aa9947df1f6192a3c6c72ef58659d4d2\"><u>Cyclone Senyar</u></a> caused catastrophic floods and landslides in <a href=\"https://radar.cloudflare.com/lk\"><u>Sri Lanka</u></a> and <a href=\"https://radar.cloudflare.com/id\"><u>Indonesia</u></a>, killing over 1,000 people and damaging telecommunications and power infrastructure across these countries. The infrastructure damage resulted in <a href=\"https://x.com/CloudflareRadar/status/1996233525989720083\"><u>disruptions to Internet connectivity</u></a>, and resultant lower traffic levels, across multiple regions.</p><p>In Sri Lanka, regions outside the main Western Province were the most affected, and several provinces saw traffic drop <a href=\"https://x.com/CloudflareRadar/status/1996233528032301513\"><u>between 80% and 95%</u></a> as compared to the prior week, including <a href=\"https://radar.cloudflare.com/traffic/1232860?dateStart=2025-11-24&amp;dateEnd=2025-12-14\"><u>North Western</u></a>, <a href=\"https://radar.cloudflare.com/traffic/1227618?dateStart=2025-11-24&amp;dateEnd=2025-12-14\"><u>Southern</u></a>, <a href=\"https://radar.cloudflare.com/traffic/1225265?dateStart=2025-11-24&amp;dateEnd=2025-12-14\"><u>Uva</u></a>, <a href=\"https://radar.cloudflare.com/traffic/8133521?dateStart=2025-11-24&amp;dateEnd=2025-12-14\"><u>Eastern</u></a>, <a href=\"https://radar.cloudflare.com/traffic/7671049?dateStart=2025-11-24&amp;dateEnd=2025-12-14\"><u>Northern</u></a>, <a href=\"https://radar.cloudflare.com/traffic/1232870?dateStart=2025-11-24&amp;dateEnd=2025-12-14\"><u>North Central</u></a>, and <a href=\"https://radar.cloudflare.com/traffic/1228435?dateStart=2025-11-24&amp;dateEnd=2025-12-14\"><u>Sabaragamuwa</u></a>.</p>\n\n<p>In <a href=\"https://x.com/CloudflareRadar/status/1996233530267885938\"><u>Indonesia</u></a>, <a href=\"https://radar.cloudflare.com/traffic/1215638?dateStart=2025-11-24&amp;dateEnd=2025-12-14\"><u>Aceh</u></a> and the Sumatra regions saw the biggest Internet disruptions. In Aceh, traffic initially dropped over 75% as compared to the previous week. In Sumatra, <a href=\"https://radar.cloudflare.com/traffic/1213642?dateStart=2025-11-24&amp;dateEnd=2025-12-14\"><u>North Sumatra</u></a> was the most affected, with an early 30% drop as compared to the previous week, before starting to recover more actively the following week.</p>\n\n\n    <div>\n      <h2>Known or unspecified technical problems</h2>\n      <a href=\"#known-or-unspecified-technical-problems\">\n        \n      </a>\n    </div>\n    \n    <div>\n      <h3>Smartfren (Indonesia)</h3>\n      <a href=\"#smartfren-indonesia\">\n        \n      </a>\n    </div>\n    <p>On October 3, subscribers to Indonesian Internet provider <a href=\"https://radar.cloudflare.com/as18004\"><u>Smartfren (AS18004</u></a>) experienced a service disruption. The issues were <a href=\"https://x.com/smartfrenworld/status/1973957300466643203\"><u>acknowledged by the provider in an X post</u></a>, which stated (in translation), “<i>Currently, telephone, SMS and data services are experiencing problems in several areas.</i>” Traffic from the provider fell as much as 84%, starting around 09:00 local time (02:00 UTC). The disruption lasted for approximately eight hours, as traffic returned to expected levels around 17:00 local time (10:00 UTC). Smartfren did not provide any additional information on what caused the service problems.</p>\n    <div>\n      <h3>Vodafone UK</h3>\n      <a href=\"#vodafone-uk\">\n        \n      </a>\n    </div>\n    <p>Major British Internet provider Vodafone UK (<a href=\"https://radar.cloudflare.com/as5378\"><u>AS5378</u></a> &amp; <a href=\"https://radar.cloudflare.com/as25135\"><u>AS25135</u></a>) experienced a brief service outage on October 23. At 15:00 local time (14:00 UTC), traffic on both Vodafone <a href=\"https://www.cloudflare.com/learning/network-layer/what-is-an-autonomous-system/\"><u>ASNs</u></a> dropped to zero. Announced IPv4 address space from <a href=\"https://radar.cloudflare.com/routing/as5378?dateStart=2025-10-13&amp;dateEnd=2025-10-13#announced-ip-address-space\"><u>AS5378</u></a> fell by 75%, while announced IPv4 address space from <a href=\"https://radar.cloudflare.com/routing/as25135?dateStart=2025-10-13&amp;dateEnd=2025-10-13#announced-ip-address-space\"><u>AS25135</u></a> disappeared entirely. Both Internet traffic and address space recovered two hours later, returning to expected levels around 17:00 local time (16:00 UTC). Vodafone did not provide any information on their social media channels about the cause of the outage, and their <a href=\"https://www.vodafone.co.uk/network/status-checker\"><u>network status checker page</u></a> was also unavailable during the outage.</p>\n\n\n\n\n\n\n    <div>\n      <h3>Fastweb (Italy)</h3>\n      <a href=\"#fastweb-italy\">\n        \n      </a>\n    </div>\n    <p>According to a <a href=\"https://tg24.sky.it/tecnologia/2025/10/22/fastweb-down-problemi-internet-oggi\"><u>published report</u></a>, a DNS resolution issue disrupted Internet services for customers of Italian provider <a href=\"https://radar.cloudflare.com/as12874\"><u>Fastweb (AS12874)</u></a> on October 22, causing observed traffic volumes to drop by over 75%. Fastweb <a href=\"https://www.firstonline.info/en/fastweb-down-oggi-internet-bloccato-in-tutta-italia-migliaia-di-segnalazioni/\"><u>acknowledged the issue</u></a>, which impacted wired Internet customers between 09:30 - 13:00 local time (08:30 - 12:00 UTC).</p><p>Although not an Internet outage caused by connectivity failure, the impact of DNS resolution issues on Internet traffic is very similar. When a provider’s <a href=\"https://www.cloudflare.com/learning/dns/dns-server-types/\"><u>DNS resolver</u></a> is experiencing problems, switching to a service like Cloudflare’s <a href=\"https://1.1.1.1/dns\"><u>1.1.1.1 public DNS resolver</u></a> will often restore connectivity.</p>\n    <div>\n      <h3>SBIN, MTN Benin, Etisalat Benin</h3>\n      <a href=\"#sbin-mtn-benin-etisalat-benin\">\n        \n      </a>\n    </div>\n    <p>On December 7, a concurrent drop in traffic was observed across <a href=\"https://radar.cloudflare.com/as28683\"><u>SBIN (AS28683)</u></a>, <a href=\"https://radar.cloudflare.com/as37424\"><u>MTN Benin (AS37424)</u></a>, and <a href=\"https://radar.cloudflare.com/as37136\"><u>Etisalat Benin (AS37136)</u></a>. Between 18:30 - 19:30 local time (17:30 - 18:30 UTC), traffic dropped as much as 80% as compared to the prior week at a country level, nearly 100% at Etisalat and MTN, and over 80% at SBIN.</p><p>While an <a href=\"https://www.reuters.com/world/africa/soldiers-benins-national-television-claim-have-seized-power-2025-12-07/\"><u>attempted coup</u></a> had taken place earlier in the day, it is unclear whether the observed Internet disruption was related in any way. From a routing perspective, all three impacted networks share <a href=\"https://radar.cloudflare.com/as174\"><u>Cogent (AS174)</u></a> as an upstream provider, so a localized issue at Cogent may have contributed to the brief outage.  </p>\n\n\n\n    <div>\n      <h3>Cellcom Israel</h3>\n      <a href=\"#cellcom-israel\">\n        \n      </a>\n    </div>\n    <p>According to a <a href=\"https://www.ynetnews.com/article/2gpt1kt35\"><u>reported announcement</u></a> from Israeli provider <a href=\"https://radar.cloudflare.com/as1680\"><u>Cellcom (AS1680)</u></a>, on December 18, there was “<i>a malfunction affecting Internet connectivity that is impacting some of our customers.</i>” This malfunction dropped traffic nearly 70% as compared to the prior week, and occurred between 09:30 - 11:00 local time (07:30 - 09:00 UTC). The “malfunction” may have been a DNS failure, according to a <a href=\"https://www.israelnationalnews.com/news/419552\"><u>published report</u></a>.</p>\n    <div>\n      <h3>Partner Communications (Israel)</h3>\n      <a href=\"#partner-communications-israel\">\n        \n      </a>\n    </div>\n    <p>Closing out 2025, on December 30, a major technical failure at Israeli provider <a href=\"https://radar.cloudflare.com/as12400\"><u>Partner Communications (AS12400)</u></a> <a href=\"https://www.ynetnews.com/tech-and-digital/article/hjewkibnwe\"><u>disrupted</u></a> mobile, TV, and Internet services across the country. Internet traffic from Partner fell by two-thirds as compared to the previous week between 14:00 - 15:00 local time (12:00 - 13:00 UTC). During the outage, queries to Cloudflare’s 1.1.1.1 public DNS resolver spiked, suggesting that the problem may have been related to Partner’s DNS infrastructure. However, the provider did not publicly confirm what caused the outage.</p>\n\n\n\n\n    <div>\n      <h2>Cloud Platforms</h2>\n      <a href=\"#cloud-platforms\">\n        \n      </a>\n    </div>\n    <p>During the fourth quarter, we launched a new <a href=\"https://radar.cloudflare.com/cloud-observatory\"><u>Cloud Observatory</u></a> page on Radar that tracks availability and performance issues at a region level across hyperscaler cloud platforms, including <a href=\"https://radar.cloudflare.com/cloud-observatory/amazon\"><u>Amazon Web Services</u></a>, <a href=\"https://radar.cloudflare.com/cloud-observatory/microsoft\"><u>Microsoft Azure</u></a>, <a href=\"https://radar.cloudflare.com/cloud-observatory/google\"><u>Google Cloud Platform</u></a>, and <a href=\"https://radar.cloudflare.com/cloud-observatory/oracle\"><u>Oracle Cloud Infrastructure</u></a>.</p>\n    <div>\n      <h3>Amazon Web Services</h3>\n      <a href=\"#amazon-web-services\">\n        \n      </a>\n    </div>\n    <p>On October 20, the Amazon Web Services us-east-1 region in Northern Virginia experienced “<a href=\"https://health.aws.amazon.com/health/status?eventID=arn:aws:health:us-east-1::event/MULTIPLE_SERVICES/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE_BA540_514A652BE1A\"><u>increased error rates and latencies</u></a>” that affected multiple services within the region. The issues impacted not only customers with public-facing Web sites and applications that rely on infrastructure within the region, but also Cloudflare customers that have origin resources hosted in us-east-1.</p><p>We began to see the impact of the problems around 06:30 UTC, as the share of <a href=\"https://radar.cloudflare.com/cloud-observatory/amazon/us-east-1?dateStart=2025-10-20&amp;dateEnd=2025-10-21#success-rate\"><u>error</u></a> (<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status#server_error_responses\"><u>5xx-class</u></a>) responses began to climb, reaching as high as 17% around 08:00 UTC. The number of <a href=\"https://radar.cloudflare.com/cloud-observatory/amazon/us-east-1?dateStart=2025-10-20&amp;dateEnd=2025-10-21#connection-failures\"><u>failures encountered when attempting to connect to origins</u></a> in us-east-1 climbed as well, peaking around 12:00 UTC.</p>\n\n<p>The impact could also be clearly seen in key network performance metrics, which remained elevated throughout the incident, returning to normal levels just before the end of the incident, around 23:00 UTC. Both <a href=\"https://radar.cloudflare.com/cloud-observatory/amazon/us-east-1?dateStart=2025-10-20&amp;dateEnd=2025-10-21#tcp-handshake-duration\"><u>TCP</u></a> and <a href=\"https://radar.cloudflare.com/cloud-observatory/amazon/us-east-1?dateStart=2025-10-20&amp;dateEnd=2025-10-21#tls-handshake-duration\"><u>TLS</u></a> handshake durations got progressively worse throughout the incident—these metrics measure the amount of time needed for Cloudflare to establish TCP and TLS connections respectively with customer origin servers in us-east-1. In addition, the amount of time elapsed before Cloudflare <a href=\"https://radar.cloudflare.com/cloud-observatory/amazon/us-east-1/#response-header-receive-duration\"><u>received response headers</u></a> from the origin increased significantly during the first several hours of the incident, before gradually returning to expected levels.  </p>\n\n\n\n\n\n    <div>\n      <h3>Microsoft Azure</h3>\n      <a href=\"#microsoft-azure\">\n        \n      </a>\n    </div>\n    <p>On October 29, Microsoft Azure experienced an <a href=\"https://azure.status.microsoft/en-us/status/history/?trackingId=YKYN-BWZ\"><u>incident</u></a> impacting <a href=\"https://azure.microsoft.com/en-us/products/frontdoor\"><u>Azure Front Door</u></a>, its content delivery network service. According to <a href=\"https://azure.status.microsoft/en-us/status/history/?trackingId=YKYN-BWZ\"><u>Azure's report on the incident</u></a>, “<i>A specific sequence of customer configuration changes, performed across two different control plane build versions, resulted in incompatible customer configuration metadata being generated. These customer configuration changes themselves were valid and non-malicious – however they produced metadata that, when deployed to edge site servers, exposed a latent bug in the data plane. This incompatibility triggered a crash during asynchronous processing within the data plane service.</i>”</p><p>The incident report marked the start time at 15:41 UTC, although we observed the volume of <a href=\"https://radar.cloudflare.com/cloud-observatory/microsoft/global?dateStart=2025-10-29&amp;dateEnd=2025-10-30#connection-failures\"><u>failed connection attempts</u></a> to Azure-hosted origins begin to climb about 45 minutes prior. The TCP and TLS handshake metrics also became more volatile during the incident period, with <a href=\"https://radar.cloudflare.com/cloud-observatory/microsoft/global?dateStart=2025-10-29&amp;dateEnd=2025-10-30#tcp-handshake-duration\"><u>TCP handshakes</u></a> taking over 50% longer at times, and <a href=\"https://radar.cloudflare.com/cloud-observatory/microsoft/global?dateStart=2025-10-29&amp;dateEnd=2025-10-30#tls-handshake-duration\"><u>TLS handshakes</u></a> taking nearly 200% longer at peak. The impacted metrics began to improve after 20:00 UTC, and according to Microsoft, the incident ended at 00:05 UTC on October 30.</p>\n\n\n\n    <div>\n      <h2>Cloudflare</h2>\n      <a href=\"#cloudflare\">\n        \n      </a>\n    </div>\n    <p>In addition to the outages discussed above, Cloudflare also experienced two disruptions during the fourth quarter. While these were not Internet outages in the classic sense, they did prevent users from accessing Web sites and applications delivered and protected by Cloudflare when they occurred.</p><p>The first incident took place on November 18, and was caused by a software failure triggered by a change to one of our database systems' permissions, which caused the database to output multiple entries into a “feature file” used by our Bot Management system. Additional details, including a root cause analysis and timeline, can be found in the associated <a href=\"https://blog.cloudflare.com/18-november-2025-outage/\"><u>blog post</u></a>.</p><p>The second incident occurred on December 5, and impacted a subset of customers, accounting for approximately 28% of all HTTP traffic served by Cloudflare. It was triggered by changes being made to our request body parsing logic while attempting to detect and mitigate a newly disclosed industry-wide React Server Components vulnerability. A post-mortem <a href=\"https://blog.cloudflare.com/5-december-2025-outage/\"><u>blog post</u></a> contains additional details, including a root cause analysis and timeline.</p><p>For more information about the work underway at Cloudflare to prevent outages like these from happening again, check out our <a href=\"https://blog.cloudflare.com/fail-small-resilience-plan/\"><u>blog post</u></a> detailing “Code Orange: Fail Small.”</p>\n    <div>\n      <h2>Conclusion</h2>\n      <a href=\"#conclusion\">\n        \n      </a>\n    </div>\n    <p>The disruptions observed in the fourth quarter underscore the importance of real-time data in maintaining global connectivity. Whether it’s a government-ordered shutdown or a minor technical issue, transparency allows the technical community to respond faster and more effectively. We will continue to track these shifts on Cloudflare Radar, providing the insights needed to navigate the complexities of modern networking. We share our observations on the <a href=\"https://radar.cloudflare.com/outage-center\"><u>Cloudflare Radar Outage Center</u></a>, via social media, and in posts on <a href=\"https://blog.cloudflare.com/tag/cloudflare-radar/\"><u>blog.cloudflare.com</u></a>. Follow us on social media at <a href=\"https://twitter.com/CloudflareRadar\"><u>@CloudflareRadar</u></a> (X), <a href=\"https://noc.social/@cloudflareradar\"><u>noc.social/@cloudflareradar</u></a> (Mastodon), and <a href=\"https://bsky.app/profile/radar.cloudflare.com\"><u>radar.cloudflare.com</u></a> (Bluesky), or contact us via <a><u>email</u></a>.</p><p>As a reminder, while these blog posts feature graphs from <a href=\"https://radar.cloudflare.com/\"><u>Radar</u></a> and the <a href=\"https://radar.cloudflare.com/explorer\"><u>Radar Data Explorer</u></a>, the underlying data is available from our <a href=\"https://developers.cloudflare.com/api/resources/radar/\"><u>API</u></a>. You can use the API to retrieve data to do your own local monitoring or analysis, or you can use the <a href=\"https://github.com/cloudflare/mcp-server-cloudflare/tree/main/apps/radar#cloudflare-radar-mcp-server-\"><u>Radar MCP server</u></a> to incorporate Radar data into your AI tools.</p> ","content:encodedSnippet":"In 2025, we observed over 180 Internet disruptions spurred by a variety of causes – some were brief and partial, while others were complete outages lasting for days. In the fourth quarter, we tracked only a single government-directed Internet shutdown, but multiple cable cuts wreaked havoc on connectivity in several countries. Power outages and extreme weather disrupted Internet services in multiple places, and the ongoing conflict in Ukraine impacted connectivity there as well. As always, a number of the disruptions we observed were due to technical problems – with some acknowledged by the relevant providers, while others had unknown causes. In addition, incidents at several hyperscaler cloud platforms and Cloudflare impacted the availability of websites and applications.  \nThis post is intended as a summary overview of observed and confirmed disruptions and is not an exhaustive or complete list of issues that have occurred during the quarter. These anomalies are detected through significant deviations from expected traffic patterns observed across our network. Check out the Cloudflare Radar Outage Center for a full list of verified anomalies and confirmed outages. \nGovernment-directed\nTanzania\nThe Internet was shut down in Tanzania on October 29 as violent protests took place during the country’s presidential election. Traffic initially fell around 12:30 local time (09:30 UTC), dropping more than 90% lower than the previous week. The disruption lasted approximately 26 hours, with traffic beginning to return around 14:30 local time (11:30 UTC) on October 30. However, that restoration proved to be quite brief, with a significant decrease in traffic occurring around 16:15 local time (13:15 UTC), approximately two hours after it returned. This second near-complete outage lasted until November 3, when traffic aggressively returned after 17:00 local time (14:00 UTC). Nominal drops in announced IPv4 and IPv6 address space were also observed during the shutdown, but there was never a complete loss of announcements, which would have signified a total disconnection of the country from the Internet. (Autonomous systems announce IP address space to other Internet providers, letting them know what blocks of IP addresses they are responsible for.)\nTanzania’s president later expressed sympathy for the members of the diplomatic community and foreigners residing in the country regarding the impact of the Internet shutdown. Internet and social media services were also restricted in 2020 ahead of the country’s general elections.\nCable cuts\nDigicel Haiti\nDigicel Haiti is unfortunately no stranger to Internet disruptions caused by cable cuts, and the network experienced two more such incidents during the fourth quarter. On October 16, traffic from Digicel Haiti (AS27653) began to fall at 14:30 local time (18:30 UTC), reaching near zero at 16:00 local time (20:00 UTC). A translated X post from the company’s Director General noted: “We advise our clientele that @DigicelHT is experiencing 2 cuts on its international fiber optic infrastructure.” Traffic began to recover after 17:00 local time (21:00 UTC), and reached expected levels within the following hour. At 17:33 local time (21:34 UTC), the Director General posted that “the first fiber on the international infrastructure has been repaired” and service had been restored. \nOn November 25, another translated X post from the provider’s Director General stated that its “international optical fiber infrastructure on National Road 1” had been cut. We observed traffic dropping on Digicel’s network approximately an hour earlier, with a complete outage observed between 02:00 - 08:00 local time (07:00 - 13:00 UTC). A follow-on X post at 08:22 local time (13:22 UTC) stated that all services had been restored.\nCybernet/StormFiber (Pakistan)\nAt 17:30 local time (12:30 UTC) on October 20, Internet traffic for Cybernet/StormFiber (AS9541) dropped sharply, falling to a level approximately 50% the same time a week prior. At the same time, the network’s announced IPv4 address space dropped by over a third. The cause of these shifts was damage to the PEACE submarine cable, which suffered a cut in the Red Sea near Sudan. \nPEACE is one of several submarine cable systems (including IMEWE and SEA-ME-WE-4) that carry international Internet traffic for Pakistani providers. The provider pledged to fully restore service by October 27, but traffic and announced IPv4 address space had recovered to near expected levels by around 02:00 local time on October 21 (21:00 UTC on October 20).\n\n      \nCamtel, MTN Cameroon, Orange Cameroun\nUnusual traffic patterns observed across multiple Internet providers in Cameroon on October 23 were reportedly caused by problems on the WACS (West Africa Cable System) submarine cable, which connects countries along the west coast of Africa to Portugal. \nA (translated) published report stated that MTN informed subscribers that “following an incident on the WACS fiber optic cable, Internet service is temporarily disrupted” and Orange Cameroun informed subscribers that “due to an incident on the international access fiber, Internet service is disrupted.” An X post from Camtel stated “Cameroon Telecommunications (CAMTEL) wishes to inform the public that a technical incident involving WACS cable equipment in Batoke (LIMBE) occurred in the early hours of 23 October 2025, causing Internet connectivity disruptions throughout the country.” \nTraffic across the impacted providers originally fell just at around  05:00 local time (04:00 UTC) before recovering to expected levels around 22:00 local time (21:00 UTC). Traffic across these networks was quite volatile during the day, dropping 90-99% at times. It isn’t clear what caused the visible spikiness in the traffic pattern—possibly attempts to shift Internet traffic to other submarine cable systems that connect to Cameroon. Announced IP address space from MTN Cameroon and Orange Cameroon dropped during this period as well, although Camtel’s announced IP address space did not change.\nConnectivity in the Central African Republic and Republic of Congo was also reportedly impacted by the WACS issues.\nClaro Dominicana\nOn December 9, we saw traffic from Claro Dominicana (AS6400), an Internet provider in the Dominican Republic, drop sharply around 12:15 local time (16:15 UTC). Traffic levels fell again around 14:15 local time (18:15 UTC), bottoming out 77% lower than the previous week before quickly returning to expected levels. The connectivity disruption was likely caused by two fiber optic outages, as an X post from the provider during the outage noted that they were “causing intermittency and slowness in some services.” A subsequent post on X from Claro stated that technicians had restored Internet services nationwide by repairing the severed fiber optic cables.\nPower outages\nDominican Republic\nAccording to a (translated) X post from the Empresa de Transmisión Eléctrica Dominicana (ETED), a transmission line outage caused an interruption in electrical service in the Dominican Republic on November 11. This power outage impacted Internet traffic from the country, resulting in a nearly 50% drop in traffic compared to the prior week, starting at 13:15 local time (17:15 UTC). Traffic levels remained lower until approximately 02:00 local time (06:00 UTC) on December 12, with a later (translated) X post from ETED noting “At 2:20 a.m. we have completed the recovery of the national electrical system, supplying 96% of the demand…”\nA subsequent technical report found that “the blackout began at the 138 kV San Pedro de Macorís I substation, where a live line was manually disconnected, triggering a high-intensity short circuit. Protection systems responded immediately, but the fault caused several nearby lines to disconnect, separating 575 MW of generation in the eastern region from the rest of the grid. The imbalance caused major power plants to trip automatically as part of their built-in safety mechanisms.”\nKenya\nOn December 9, a major power outage impacted multiple regions across Kenya. Kenya Power explained that the outage “was triggered by an incident on the regional Kenya-Uganda interconnected power network, which caused a disturbance on the Kenyan side of the system” and claimed that “[p]ower was restored to most of the affected areas within approximately 30 minutes.” However, impacts to Internet connectivity lasted for nearly four hours, between 19:15 - 23:00 local time (16:15 - 20:00 UTC). The power outage caused traffic to drop as much as 18% at a national level, with the traffic shifts most visible in Nakuru County and Kaimbu County.\nMilitary action\nOdesa, Ukraine\nRussian drone strikes on the Odesa region in Ukraine on December 12 damaged warehouses and energy infrastructure, with the latter causing power outages in parts of the region. Those outages disrupted Internet connectivity, resulting in traffic dropping by as much as 57% as compared to the prior week. After the initial drop at midnight on December 13 (22:00 UTC on December 12), traffic gradually recovered over the following several days, returning to expected levels around 14:30 local time (12:30 UTC) on December 16.\nWeather\nJamaica\nHurricane Melissa made landfall on Jamaica on October 28 and left a trail of damage and destruction in its path. Associated power outages and infrastructure damage impacted Internet connectivity, causing traffic to initially drop by approximately half, starting around 06:15 local time (11:15 UTC), ultimately reaching as much as 70% lower than the previous week. Internet traffic from Jamaica remained well below pre-hurricane levels for several days, and ultimately started to make greater progress towards expected levels during the morning of November 4. It can often take weeks or months for Internet traffic from a country to return to “normal” levels following storms that cause massive and widespread damage – while power may be largely restored within several days, damage to physical infrastructure takes significantly longer to address.\nSri Lanka & Indonesia\nOn November 26, Cyclone Senyar caused catastrophic floods and landslides in Sri Lanka and Indonesia, killing over 1,000 people and damaging telecommunications and power infrastructure across these countries. The infrastructure damage resulted in disruptions to Internet connectivity, and resultant lower traffic levels, across multiple regions.\nIn Sri Lanka, regions outside the main Western Province were the most affected, and several provinces saw traffic drop between 80% and 95% as compared to the prior week, including North Western, Southern, Uva, Eastern, Northern, North Central, and Sabaragamuwa.\nIn Indonesia, Aceh and the Sumatra regions saw the biggest Internet disruptions. In Aceh, traffic initially dropped over 75% as compared to the previous week. In Sumatra, North Sumatra was the most affected, with an early 30% drop as compared to the previous week, before starting to recover more actively the following week.\nKnown or unspecified technical problems\nSmartfren (Indonesia)\nOn October 3, subscribers to Indonesian Internet provider Smartfren (AS18004) experienced a service disruption. The issues were acknowledged by the provider in an X post, which stated (in translation), “Currently, telephone, SMS and data services are experiencing problems in several areas.” Traffic from the provider fell as much as 84%, starting around 09:00 local time (02:00 UTC). The disruption lasted for approximately eight hours, as traffic returned to expected levels around 17:00 local time (10:00 UTC). Smartfren did not provide any additional information on what caused the service problems.\nVodafone UK\nMajor British Internet provider Vodafone UK (AS5378 & AS25135) experienced a brief service outage on October 23. At 15:00 local time (14:00 UTC), traffic on both Vodafone ASNs dropped to zero. Announced IPv4 address space from AS5378 fell by 75%, while announced IPv4 address space from AS25135 disappeared entirely. Both Internet traffic and address space recovered two hours later, returning to expected levels around 17:00 local time (16:00 UTC). Vodafone did not provide any information on their social media channels about the cause of the outage, and their network status checker page was also unavailable during the outage.\nFastweb (Italy)\nAccording to a published report, a DNS resolution issue disrupted Internet services for customers of Italian provider Fastweb (AS12874) on October 22, causing observed traffic volumes to drop by over 75%. Fastweb acknowledged the issue, which impacted wired Internet customers between 09:30 - 13:00 local time (08:30 - 12:00 UTC).\nAlthough not an Internet outage caused by connectivity failure, the impact of DNS resolution issues on Internet traffic is very similar. When a provider’s DNS resolver is experiencing problems, switching to a service like Cloudflare’s 1.1.1.1 public DNS resolver will often restore connectivity.\nSBIN, MTN Benin, Etisalat Benin\nOn December 7, a concurrent drop in traffic was observed across SBIN (AS28683), MTN Benin (AS37424), and Etisalat Benin (AS37136). Between 18:30 - 19:30 local time (17:30 - 18:30 UTC), traffic dropped as much as 80% as compared to the prior week at a country level, nearly 100% at Etisalat and MTN, and over 80% at SBIN.\nWhile an attempted coup had taken place earlier in the day, it is unclear whether the observed Internet disruption was related in any way. From a routing perspective, all three impacted networks share Cogent (AS174) as an upstream provider, so a localized issue at Cogent may have contributed to the brief outage.  \nCellcom Israel\nAccording to a reported announcement from Israeli provider Cellcom (AS1680), on December 18, there was “a malfunction affecting Internet connectivity that is impacting some of our customers.” This malfunction dropped traffic nearly 70% as compared to the prior week, and occurred between 09:30 - 11:00 local time (07:30 - 09:00 UTC). The “malfunction” may have been a DNS failure, according to a published report.\nPartner Communications (Israel)\nClosing out 2025, on December 30, a major technical failure at Israeli provider Partner Communications (AS12400) disrupted mobile, TV, and Internet services across the country. Internet traffic from Partner fell by two-thirds as compared to the previous week between 14:00 - 15:00 local time (12:00 - 13:00 UTC). During the outage, queries to Cloudflare’s 1.1.1.1 public DNS resolver spiked, suggesting that the problem may have been related to Partner’s DNS infrastructure. However, the provider did not publicly confirm what caused the outage.\nCloud Platforms\nDuring the fourth quarter, we launched a new Cloud Observatory page on Radar that tracks availability and performance issues at a region level across hyperscaler cloud platforms, including Amazon Web Services, Microsoft Azure, Google Cloud Platform, and Oracle Cloud Infrastructure.\nAmazon Web Services\nOn October 20, the Amazon Web Services us-east-1 region in Northern Virginia experienced “increased error rates and latencies” that affected multiple services within the region. The issues impacted not only customers with public-facing Web sites and applications that rely on infrastructure within the region, but also Cloudflare customers that have origin resources hosted in us-east-1.\nWe began to see the impact of the problems around 06:30 UTC, as the share of error (5xx-class) responses began to climb, reaching as high as 17% around 08:00 UTC. The number of failures encountered when attempting to connect to origins in us-east-1 climbed as well, peaking around 12:00 UTC.\nThe impact could also be clearly seen in key network performance metrics, which remained elevated throughout the incident, returning to normal levels just before the end of the incident, around 23:00 UTC. Both TCP and TLS handshake durations got progressively worse throughout the incident—these metrics measure the amount of time needed for Cloudflare to establish TCP and TLS connections respectively with customer origin servers in us-east-1. In addition, the amount of time elapsed before Cloudflare received response headers from the origin increased significantly during the first several hours of the incident, before gradually returning to expected levels.  \nMicrosoft Azure\nOn October 29, Microsoft Azure experienced an incident impacting Azure Front Door, its content delivery network service. According to Azure's report on the incident, “A specific sequence of customer configuration changes, performed across two different control plane build versions, resulted in incompatible customer configuration metadata being generated. These customer configuration changes themselves were valid and non-malicious – however they produced metadata that, when deployed to edge site servers, exposed a latent bug in the data plane. This incompatibility triggered a crash during asynchronous processing within the data plane service.”\nThe incident report marked the start time at 15:41 UTC, although we observed the volume of failed connection attempts to Azure-hosted origins begin to climb about 45 minutes prior. The TCP and TLS handshake metrics also became more volatile during the incident period, with TCP handshakes taking over 50% longer at times, and TLS handshakes taking nearly 200% longer at peak. The impacted metrics began to improve after 20:00 UTC, and according to Microsoft, the incident ended at 00:05 UTC on October 30.\nCloudflare\nIn addition to the outages discussed above, Cloudflare also experienced two disruptions during the fourth quarter. While these were not Internet outages in the classic sense, they did prevent users from accessing Web sites and applications delivered and protected by Cloudflare when they occurred.\nThe first incident took place on November 18, and was caused by a software failure triggered by a change to one of our database systems' permissions, which caused the database to output multiple entries into a “feature file” used by our Bot Management system. Additional details, including a root cause analysis and timeline, can be found in the associated blog post.\nThe second incident occurred on December 5, and impacted a subset of customers, accounting for approximately 28% of all HTTP traffic served by Cloudflare. It was triggered by changes being made to our request body parsing logic while attempting to detect and mitigate a newly disclosed industry-wide React Server Components vulnerability. A post-mortem blog post contains additional details, including a root cause analysis and timeline.\nFor more information about the work underway at Cloudflare to prevent outages like these from happening again, check out our blog post detailing “Code Orange: Fail Small.”\nConclusion\nThe disruptions observed in the fourth quarter underscore the importance of real-time data in maintaining global connectivity. Whether it’s a government-ordered shutdown or a minor technical issue, transparency allows the technical community to respond faster and more effectively. We will continue to track these shifts on Cloudflare Radar, providing the insights needed to navigate the complexities of modern networking. We share our observations on the Cloudflare Radar Outage Center, via social media, and in posts on blog.cloudflare.com. Follow us on social media at @CloudflareRadar (X), noc.social/@cloudflareradar (Mastodon), and radar.cloudflare.com (Bluesky), or contact us via email.\nAs a reminder, while these blog posts feature graphs from Radar and the Radar Data Explorer, the underlying data is available from our API. You can use the API to retrieve data to do your own local monitoring or analysis, or you can use the Radar MCP server to incorporate Radar data into your AI tools.","dc:creator":"David Belson","content":" The last quarter of 2025 brought several notable disruptions to Internet connectivity. Cloudflare Radar data reveals the impact of cable cuts, power outages, extreme weather, technical problems, and more. ","contentSnippet":"The last quarter of 2025 brought several notable disruptions to Internet connectivity. Cloudflare Radar data reveals the impact of cable cuts, power outages, extreme weather, technical problems, and more.","guid":"6dRT0oOSVcyQzjnZCkzH7S","categories":["Radar","Internet Shutdown","Internet Traffic","Outage","Internet Trends","AWS","Microsoft Azure","Consumer Services"],"isoDate":"2026-01-26T14:00:00.000Z"},{"creator":"Bryton Herdes","title":"Route leak incident on January 22, 2026","link":"https://blog.cloudflare.com/route-leak-incident-january-22-2026/","pubDate":"Fri, 23 Jan 2026 14:00:00 GMT","content:encoded":" <p>On January 22, 2026, an automated routing policy configuration error caused us to leak some <a href=\"http://cloudflare.com/learning/security/glossary/what-is-bgp/\"><u>Border Gateway Protocol (BGP)</u></a> prefixes unintentionally from a router at our data center in Miami, Florida. While the route leak caused some impact to Cloudflare customers, multiple external parties were also affected because their traffic was accidentally funnelled through our Miami data center location.</p><p>The route leak lasted 25 minutes, causing congestion on some of our backbone infrastructure in Miami, elevated loss for some Cloudflare customer traffic, and higher latency for traffic across these links. Additionally, some traffic was discarded by firewall filters on our routers that are designed to only accept traffic for Cloudflare services and our customers.</p><p>While we’ve written about route leaks before, we rarely find ourselves causing them. This route leak was the result of an accidental misconfiguration on a router in Cloudflare’s network, and only affected IPv6 traffic. We sincerely apologize to the users, customers, and networks we impacted yesterday as a result of this BGP route leak.</p>\n    <div>\n      <h3>BGP route leaks </h3>\n      <a href=\"#bgp-route-leaks\">\n        \n      </a>\n    </div>\n    <p>We have <a href=\"https://blog.cloudflare.com/how-verizon-and-a-bgp-optimizer-knocked-large-parts-of-the-internet-offline-today/\"><u>written multiple times</u></a> about <a href=\"https://blog.cloudflare.com/cloudflare-1111-incident-on-june-27-2024/\"><u>BGP route leaks</u></a>, and we even record <a href=\"https://blog.cloudflare.com/route-leak-detection-with-cloudflare-radar/\"><u>route leak events</u></a> on Cloudflare Radar for anyone to view and learn from. To get a fuller understanding of what route leaks are, you can refer to this <a href=\"https://blog.cloudflare.com/bgp-route-leak-venezuela/#background-bgp-route-leaks\"><u>detailed background section</u></a>, or refer to the formal definition within <a href=\"https://datatracker.ietf.org/doc/html/rfc7908\"><u>RFC7908</u></a>. </p><p>Essentially, a route leak occurs when a network tells the broader Internet to send it traffic that it's not supposed to forward. Technically, a route leak occurs when a network, or Autonomous System (AS), appears unexpectedly in an AS path. An AS path is what BGP uses to determine the path across the Internet to a final destination. An example of an anomalous AS path indicative of a route leak would be finding a network sending routes received from a peer to a provider.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/30z4NDtf6DjVQZOfZatGUX/6ff06eb02c61d8e818d9da8ecd87c1c8/BLOG-3135_2.png\" />\n          </figure><p>During this type of route leak, the rules of <a href=\"https://people.eecs.berkeley.edu/~sylvia/cs268-2019/papers/gao-rexford.pdf\"><u>valley-free routing</u></a> are violated, as BGP updates are sent from AS64501 to their peer (AS64502), and then unexpectedly up to a provider (AS64503). Oftentimes the leaker, in this case AS64502, is not prepared to handle the amount of traffic they are going to receive and may not even have firewall filters configured to accept all of the traffic coming in their direction. In simple terms, once a route update is sent to a peer or provider, it should only be sent further to customers and not to another peer or provider AS.</p><p>During the incident on January 22, we caused a similar kind of route leak, in which we took routes from some of our peers and redistributed them in Miami to some of our peers and providers. According to the route leak definitions in RFC7908, we caused a mixture of Type 3 and Type 4 route leaks on the Internet. </p>\n    <div>\n      <h3>Timeline</h3>\n      <a href=\"#timeline\">\n        \n      </a>\n    </div>\n    <table><tr><th><p><b>Time (UTC)</b></p></th><th><p><b>Event</b></p></th></tr><tr><td><p>2026-01-22 19:52 UTC</p></td><td><p>A change that ultimately triggers the routing policy bug is merged in our network automation code repository</p></td></tr><tr><td><p>2026-01-22 20:25 UTC</p></td><td><p>Automation is run on single Miami edge-router resulting in unexpected advertisements to BGP transit providers and peers</p><p><b>IMPACT START</b></p></td></tr><tr><td><p>2026-01-22 20:40 UTC</p></td><td><p>Network team begins investigating unintended route advertisements from Miami</p></td></tr><tr><td><p>2026-01-22 20:44 UTC</p></td><td><p>Incident is raised to coordinate response</p></td></tr><tr><td><p>2026-01-22 20:50 UTC</p></td><td><p>The bad configuration change is manually reverted by a network operator, and automation is paused for the router, so it cannot run again</p><p><b>IMPACT STOP</b></p></td></tr><tr><td><p>2026-01-22 21:47 UTC</p></td><td><p>The change that triggered the leak is reverted from our code repository</p></td></tr><tr><td><p>2026-01-22 22:07 UTC</p></td><td><p>Automation is confirmed by operators to be healthy to run again on the Miami router, without the routing policy bug</p></td></tr><tr><td><p>2026-01-22 22:40 UTC</p></td><td><p>Automation is unpaused on the single router in Miami</p></td></tr></table>\n    <div>\n      <h3>What happened: the configuration error</h3>\n      <a href=\"#what-happened-the-configuration-error\">\n        \n      </a>\n    </div>\n    <p>On January 22, 2026, at 20:25 UTC, we pushed a change via our policy automation platform to remove the BGP announcements from Miami for one of our data centers in Bogotá, Colombia. This was purposeful, as we previously forwarded some IPv6 traffic through Miami toward the Bogotá data center, but recent infrastructure upgrades removed the need for us to do so.</p><p>This change generated the following diff (a program that <a href=\"https://www.google.com/search?sca_esv=3236b0192813a1a3&amp;rlz=1C5GCEM_enUS1183US1183&amp;sxsrf=ANbL-n4_5E8v7Ar8tpKKnczz7xfci6HL8w:1769145786825&amp;q=compares&amp;si=AL3DRZGCrnAF0R35UNPJcgaBbCFaNwxQEU_o22EUn2GaHpSR2UyenaROnahi_5cmhKmzHjtezT-J9hw3KLJeTkLeyo7_nJgoJebLkbDRWvoJl5t5oX8bAPI%3D&amp;expnd=1&amp;sa=X&amp;ved=2ahUKEwiW1bfR9aCSAxXaOjQIHSIBDMoQyecJegQIKhAR\">compares</a> configuration files in order to determine how or whether they differ):</p>\n            <pre><code>[edit policy-options policy-statement 6-COGENT-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-COMCAST-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-GTT-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-LEVEL3-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-PRIVATE-PEER-ANYCAST-OUT term ADV-SITELOCAL from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-PUBLIC-PEER-ANYCAST-OUT term ADV-SITELOCAL from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-PUBLIC-PEER-OUT term ADV-SITELOCAL from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-TELEFONICA-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-TELIA-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;</code></pre>\n            <p>While this policy change looks innocent at a glance, only removing the prefix lists containing BOG04 unicast prefixes resulted in a policy that was too permissive:</p>\n            <pre><code>policy-options policy-statement 6-TELIA-ACCEPT-EXPORT {\n    term ADV-SITELOCAL-GRE-RECEIVER {\n        from route-type internal;\n        then {\n            community add STATIC-ROUTE;\n            community add SITE-LOCAL-ROUTE;\n            community add MIA01;\n            community add NORTH-AMERICA;\n            accept;\n        }\n    }\n}\n</code></pre>\n            <p>The policy would now mark every prefix of type “internal” as acceptable, and proceed to add some informative communities to all matching prefixes. But more importantly, the policy also accepted the route through the policy filter, which resulted in the prefix — which was intended to be “internal” —  being advertised externally. This is an issue because the “route-type internal” match in JunOS or JunOS EVO (the operating systems used by <a href=\"https://www.hpe.com/us/en/home.html\"><u>HPE Juniper Networks</u></a> devices) will match any non-external route type, such as Internal BGP (IBGP) routes, which is what happened here.</p><p>As a result, all IPv6 prefixes that Cloudflare redistributes internally across the backbone were accepted by this policy, and advertised to all our BGP neighbors in Miami. This is unfortunately very similar to the outage we experienced in 2020, on which you can read more <a href=\"https://blog.cloudflare.com/cloudflare-outage-on-july-17-2020/\"><u>on our blog</u></a>.</p><p>When the policy misconfiguration was applied at 20:25 UTC, a series of unintended BGP updates were sent from AS13335 to peers and providers in Miami. These BGP updates are viewable historically by looking at MRT files with the <a href=\"https://github.com/bgpkit/monocle\"><u>monocle</u></a> tool or using <a href=\"https://stat.ripe.net/bgplay/2a03%3A2880%3Af312%3A%3A%2F48#starttime=1769112000&amp;endtime=1769115659&amp;instant=56,1769113845\"><u>RIPE BGPlay</u></a>. </p>\n            <pre><code>➜  ~ monocle search --start-ts 2026-01-22T20:24:00Z --end-ts 2026-01-22T20:30:00Z --as-path \".*13335[ \\d$]32934$*\"\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f077::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f091::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f16f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f17c::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f26f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f27c::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f33f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f17c::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc\nA|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f27c::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc\nA|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f091::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc\nA|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f091::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc\nA|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f17c::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc\nA|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f27c::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc\n{trimmed}\n</code></pre>\n            <p><sup><i>In the monocle output seen above, we have the timestamp of our BGP update, followed by the next-hop in the announcement, the ASN of the network feeding a given route-collector, the prefix involved, and the AS path and BGP communities if any are found. At the end of the output per-line, we also find the route-collector instance.</i></sup></p><p>Looking at the first update for prefix 2a03:2880:f077::/48, the AS path is <i>64112 22850 174 3356 13335 32934</i>. This means we (AS13335) took the prefix received from Meta (AS32934), our peer, and then advertised it toward Lumen (AS3356), one of our upstream transit providers. We know this is a route leak as routes received from peers are only meant to be readvertised to downstream (customer) networks, not laterally to other peers or up to providers.</p><p>As a result of the leak and the forwarding of unintended traffic into our Miami router from providers and peers, we experienced congestion on our backbone between Miami and Atlanta, as you can see in the graph below. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/SIiBSb7qnfStZ0jQAZ8ne/14009779b7551e4f26c4cc3ae2c1141b/BLOG-3135_3.png\" />\n          </figure><p>This would have resulted in elevated loss for some Cloudflare customer traffic, and higher latency than usual for traffic traversing these links. In addition to this congestion, the networks whose prefixes we leaked would have had their traffic discarded by firewall filters on our routers that are designed to only accept traffic for Cloudflare services and our customers. At peak, we discarded around 12Gbps of traffic ingressing our router in Miami for these non-downstream prefixes. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/jMaMLbijdtS8GYZOVzwoX/40325907032cbb8d27f00bc561191d23/BLOG-3135_4.png\" />\n          </figure>\n    <div>\n      <h3>Follow-ups and preventing route leaks </h3>\n      <a href=\"#follow-ups-and-preventing-route-leaks\">\n        \n      </a>\n    </div>\n    <p>We are big supporters and active contributors to efforts within the <a href=\"https://www.ietf.org/\"><u>IETF</u></a> and <a href=\"https://manrs.org/\"><u>infrastructure community</u></a> that strengthen routing security. We know firsthand how easy it is to cause a route leak accidentally, as evidenced by this incident. </p><p>Preventing route leaks will require a multi-faceted approach, but we have identified multiple areas in which we can improve, both short- and long-term.</p><p>In terms of our routing policy configurations and automation, we are:</p><ul><li><p>Patching the failure in our routing policy automation that caused the route leak, and will mitigate this potential failure and others like it immediately </p></li><li><p>Implementing additional BGP community-based safeguards in our routing policies that explicitly reject routes that were received from providers and peers on external export policies </p></li><li><p>Adding automatic routing policy evaluation into our <a href=\"https://www.cloudflare.com/learning/serverless/glossary/what-is-ci-cd/\">CI/CD pipelines</a> that looks specifically for empty or erroneous policy terms </p></li><li><p>Improve early detection of issues with network configurations and the negative effects of an automated change</p></li></ul><p>To help prevent route leaks in general, we are: </p><ul><li><p>Validating routing equipment vendors' implementation of <a href=\"https://datatracker.ietf.org/doc/rfc9234/\"><u>RFC9234</u></a> (BGP roles and the Only-to-Customer Attribute) in preparation for our rollout of the feature, which is the only way <i>independent of routing policy</i> to prevent route leaks caused at the <i>local</i> Autonomous System (AS)</p></li><li><p>Encouraging the long term adoption of RPKI <a href=\"https://datatracker.ietf.org/doc/draft-ietf-sidrops-aspa-verification/\"><u>Autonomous System Provider Authorization (ASPA)</u></a>, where networks could automatically reject routes that contain anomalous AS paths</p></li></ul><p>Most importantly, we would again like to apologize for the impact we caused users and customers of Cloudflare, as well as any impact felt by external networks.</p><p></p> ","content:encodedSnippet":"On January 22, 2026, an automated routing policy configuration error caused us to leak some Border Gateway Protocol (BGP) prefixes unintentionally from a router at our data center in Miami, Florida. While the route leak caused some impact to Cloudflare customers, multiple external parties were also affected because their traffic was accidentally funnelled through our Miami data center location.\nThe route leak lasted 25 minutes, causing congestion on some of our backbone infrastructure in Miami, elevated loss for some Cloudflare customer traffic, and higher latency for traffic across these links. Additionally, some traffic was discarded by firewall filters on our routers that are designed to only accept traffic for Cloudflare services and our customers.\nWhile we’ve written about route leaks before, we rarely find ourselves causing them. This route leak was the result of an accidental misconfiguration on a router in Cloudflare’s network, and only affected IPv6 traffic. We sincerely apologize to the users, customers, and networks we impacted yesterday as a result of this BGP route leak.\nBGP route leaks \nWe have written multiple times about BGP route leaks, and we even record route leak events on Cloudflare Radar for anyone to view and learn from. To get a fuller understanding of what route leaks are, you can refer to this detailed background section, or refer to the formal definition within RFC7908. \nEssentially, a route leak occurs when a network tells the broader Internet to send it traffic that it's not supposed to forward. Technically, a route leak occurs when a network, or Autonomous System (AS), appears unexpectedly in an AS path. An AS path is what BGP uses to determine the path across the Internet to a final destination. An example of an anomalous AS path indicative of a route leak would be finding a network sending routes received from a peer to a provider.\nDuring this type of route leak, the rules of valley-free routing are violated, as BGP updates are sent from AS64501 to their peer (AS64502), and then unexpectedly up to a provider (AS64503). Oftentimes the leaker, in this case AS64502, is not prepared to handle the amount of traffic they are going to receive and may not even have firewall filters configured to accept all of the traffic coming in their direction. In simple terms, once a route update is sent to a peer or provider, it should only be sent further to customers and not to another peer or provider AS.\nDuring the incident on January 22, we caused a similar kind of route leak, in which we took routes from some of our peers and redistributed them in Miami to some of our peers and providers. According to the route leak definitions in RFC7908, we caused a mixture of Type 3 and Type 4 route leaks on the Internet. \nTimeline\n\nTime (UTC)\n\nEvent\n\n\n2026-01-22 19:52 UTC\n\nA change that ultimately triggers the routing policy bug is merged in our network automation code repository\n\n\n2026-01-22 20:25 UTC\n\nAutomation is run on single Miami edge-router resulting in unexpected advertisements to BGP transit providers and peers\nIMPACT START\n\n\n2026-01-22 20:40 UTC\n\nNetwork team begins investigating unintended route advertisements from Miami\n\n\n2026-01-22 20:44 UTC\n\nIncident is raised to coordinate response\n\n\n2026-01-22 20:50 UTC\n\nThe bad configuration change is manually reverted by a network operator, and automation is paused for the router, so it cannot run again\nIMPACT STOP\n\n\n2026-01-22 21:47 UTC\n\nThe change that triggered the leak is reverted from our code repository\n\n\n2026-01-22 22:07 UTC\n\nAutomation is confirmed by operators to be healthy to run again on the Miami router, without the routing policy bug\n\n\n2026-01-22 22:40 UTC\n\nAutomation is unpaused on the single router in Miami\n\n\n    \nWhat happened: the configuration error\nOn January 22, 2026, at 20:25 UTC, we pushed a change via our policy automation platform to remove the BGP announcements from Miami for one of our data centers in Bogotá, Colombia. This was purposeful, as we previously forwarded some IPv6 traffic through Miami toward the Bogotá data center, but recent infrastructure upgrades removed the need for us to do so.\nThis change generated the following diff (a program that compares configuration files in order to determine how or whether they differ):\n[edit policy-options policy-statement 6-COGENT-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-COMCAST-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-GTT-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-LEVEL3-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-PRIVATE-PEER-ANYCAST-OUT term ADV-SITELOCAL from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-PUBLIC-PEER-ANYCAST-OUT term ADV-SITELOCAL from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-PUBLIC-PEER-OUT term ADV-SITELOCAL from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-TELEFONICA-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\n[edit policy-options policy-statement 6-TELIA-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]\n-      prefix-list 6-BOG04-SITE-LOCAL;\nWhile this policy change looks innocent at a glance, only removing the prefix lists containing BOG04 unicast prefixes resulted in a policy that was too permissive:\npolicy-options policy-statement 6-TELIA-ACCEPT-EXPORT {\n    term ADV-SITELOCAL-GRE-RECEIVER {\n        from route-type internal;\n        then {\n            community add STATIC-ROUTE;\n            community add SITE-LOCAL-ROUTE;\n            community add MIA01;\n            community add NORTH-AMERICA;\n            accept;\n        }\n    }\n}\n\nThe policy would now mark every prefix of type “internal” as acceptable, and proceed to add some informative communities to all matching prefixes. But more importantly, the policy also accepted the route through the policy filter, which resulted in the prefix — which was intended to be “internal” —  being advertised externally. This is an issue because the “route-type internal” match in JunOS or JunOS EVO (the operating systems used by HPE Juniper Networks devices) will match any non-external route type, such as Internal BGP (IBGP) routes, which is what happened here.\nAs a result, all IPv6 prefixes that Cloudflare redistributes internally across the backbone were accepted by this policy, and advertised to all our BGP neighbors in Miami. This is unfortunately very similar to the outage we experienced in 2020, on which you can read more on our blog.\nWhen the policy misconfiguration was applied at 20:25 UTC, a series of unintended BGP updates were sent from AS13335 to peers and providers in Miami. These BGP updates are viewable historically by looking at MRT files with the monocle tool or using RIPE BGPlay. \n➜  ~ monocle search --start-ts 2026-01-22T20:24:00Z --end-ts 2026-01-22T20:30:00Z --as-path \".*13335[ \\d$]32934$*\"\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f077::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f091::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f16f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f17c::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f26f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f27c::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f33f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl\nA|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f17c::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc\nA|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f27c::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc\nA|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f091::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc\nA|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f091::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc\nA|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f17c::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc\nA|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f27c::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc\n{trimmed}\n\nIn the monocle output seen above, we have the timestamp of our BGP update, followed by the next-hop in the announcement, the ASN of the network feeding a given route-collector, the prefix involved, and the AS path and BGP communities if any are found. At the end of the output per-line, we also find the route-collector instance.\nLooking at the first update for prefix 2a03:2880:f077::/48, the AS path is 64112 22850 174 3356 13335 32934. This means we (AS13335) took the prefix received from Meta (AS32934), our peer, and then advertised it toward Lumen (AS3356), one of our upstream transit providers. We know this is a route leak as routes received from peers are only meant to be readvertised to downstream (customer) networks, not laterally to other peers or up to providers.\nAs a result of the leak and the forwarding of unintended traffic into our Miami router from providers and peers, we experienced congestion on our backbone between Miami and Atlanta, as you can see in the graph below. \nThis would have resulted in elevated loss for some Cloudflare customer traffic, and higher latency than usual for traffic traversing these links. In addition to this congestion, the networks whose prefixes we leaked would have had their traffic discarded by firewall filters on our routers that are designed to only accept traffic for Cloudflare services and our customers. At peak, we discarded around 12Gbps of traffic ingressing our router in Miami for these non-downstream prefixes. \nFollow-ups and preventing route leaks \nWe are big supporters and active contributors to efforts within the IETF and infrastructure community that strengthen routing security. We know firsthand how easy it is to cause a route leak accidentally, as evidenced by this incident. \nPreventing route leaks will require a multi-faceted approach, but we have identified multiple areas in which we can improve, both short- and long-term.\nIn terms of our routing policy configurations and automation, we are:\n\nPatching the failure in our routing policy automation that caused the route leak, and will mitigate this potential failure and others like it immediately \n\nImplementing additional BGP community-based safeguards in our routing policies that explicitly reject routes that were received from providers and peers on external export policies \n\nAdding automatic routing policy evaluation into our CI/CD pipelines that looks specifically for empty or erroneous policy terms \n\nImprove early detection of issues with network configurations and the negative effects of an automated change\n\nTo help prevent route leaks in general, we are: \n\nValidating routing equipment vendors' implementation of RFC9234 (BGP roles and the Only-to-Customer Attribute) in preparation for our rollout of the feature, which is the only way independent of routing policy to prevent route leaks caused at the local Autonomous System (AS)\n\nEncouraging the long term adoption of RPKI Autonomous System Provider Authorization (ASPA), where networks could automatically reject routes that contain anomalous AS paths\n\nMost importantly, we would again like to apologize for the impact we caused users and customers of Cloudflare, as well as any impact felt by external networks.","dc:creator":"Bryton Herdes","content":" An automated routing policy configuration error caused us to leak some Border Gateway Protocol prefixes unintentionally from a router at our Miami data center. We discuss the impact and the changes we are implementing as a result. ","contentSnippet":"An automated routing policy configuration error caused us to leak some Border Gateway Protocol prefixes unintentionally from a router at our Miami data center. We discuss the impact and the changes we are implementing as a result.","guid":"1lDFdmcpnlPwczsEbswsTs","categories":["BGP","Post Mortem"],"isoDate":"2026-01-23T14:00:00.000Z"},{"creator":"Hrushikesh Deshpande","title":"How we mitigated a vulnerability in Cloudflare’s ACME validation logic","link":"https://blog.cloudflare.com/acme-path-vulnerability/","pubDate":"Mon, 19 Jan 2026 14:00:00 GMT","content:encoded":" <p><i>This post was updated on January 20, 2026.</i></p><p>On October 13, 2025, security researchers from <a href=\"https://fearsoff.org/\"><u>FearsOff</u></a> identified and reported a vulnerability in Cloudflare's ACME (Automatic Certificate Management Environment) validation logic that disabled some of the <a href=\"https://developers.cloudflare.com/waf/\"><u>WAF</u></a> features on specific ACME-related paths. The vulnerability was reported and validated through Cloudflare’s <a href=\"https://hackerone.com/cloudflare?type=team\"><u>bug bounty</u></a> program.</p><p>The vulnerability was rooted in how our edge network processed requests destined for the ACME HTTP-01 challenge path (<code><i>/.well-known/acme-challenge/*</i></code>). </p><p>Here, we’ll briefly explain how this protocol works and the action we took to address the vulnerability. </p><p><b>Cloudflare has patched this vulnerability and there is no action necessary for Cloudflare customers.</b> There is no evidence of any malicious actor abusing this vulnerability.</p>\n    <div>\n      <h3>How ACME works to validate certificates</h3>\n      <a href=\"#how-acme-works-to-validate-certificates\">\n        \n      </a>\n    </div>\n    <p>ACME is a protocol used to automate the issuance, renewal, and revocation of <a href=\"https://www.cloudflare.com/learning/ssl/what-is-an-ssl-certificate/\"><u>SSL/TLS certificates</u></a>. When an HTTP-01 challenge is used to validate domain ownership, a Certificate Authority (CA) will expect to find a validation token at the HTTP path following the format of <i>http://{customer domain}/.well-known/acme-challenge/{token value}</i>. </p><p>If this challenge is used by a certificate order managed by Cloudflare, then Cloudflare will respond on this path and provide the token provided by the CA to the caller. If the token provided does not correlate to a Cloudflare managed order, then this request would be passed on to the customer origin, since they may be attempting to complete domain validation as a part of some other system. Check out the flow below for more details — other use cases are discussed later in the blog post.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6myH4sEuRj4hhBPYiITWsB/9be3e62bdd7001ab1ef9b01db094de5b/BLOG-3067_2.png\" />\n          </figure>\n    <div>\n      <h3>The underlying logic flaw </h3>\n      <a href=\"#the-underlying-logic-flaw\">\n        \n      </a>\n    </div>\n    <p>Certain requests to<i> /.well-known/acme-challenge/*</i> would cause the logic serving ACME challenge tokens to disable WAF features on a challenge request, and allow the challenge request to continue to the origin when it should have been blocked.</p><p>Previously, when Cloudflare was serving a HTTP-01 challenge token, if the path requested by the caller matched a token for an active challenge in our system, the logic serving an ACME challenge token would disable WAF features, since Cloudflare would be directly serving the response. This is done because those features can interfere with the CA’s ability to validate the token values and would cause failures with automated certificate orders and renewals.</p><p>However, in the scenario that the token used was associated with a different zone and not directly managed by Cloudflare, the request would be allowed to proceed onto the customer origin without further processing by WAF rulesets.</p>\n    <div>\n      <h3>How we mitigated this vulnerability</h3>\n      <a href=\"#how-we-mitigated-this-vulnerability\">\n        \n      </a>\n    </div>\n    <p>To mitigate this issue, a code change was released. This code change only allows the set of security features to be disabled in the event that the request matches a valid ACME HTTP-01 challenge token for the hostname. In that case, Cloudflare has a challenge response to serve back.</p>\n    <div>\n      <h3>Cloudflare customers are protected</h3>\n      <a href=\"#cloudflare-customers-are-protected\">\n        \n      </a>\n    </div>\n    <p>As we noted above,<b> Cloudflare has patched this vulnerability and Cloudflare customers do not need to take any action.</b> In addition, there is no evidence  of any malicious actor abusing this vulnerability.</p>\n    <div>\n      <h3>Moving quickly with vulnerability transparency</h3>\n      <a href=\"#moving-quickly-with-vulnerability-transparency\">\n        \n      </a>\n    </div>\n    <p>As always, we thank the external researchers for responsibly disclosing this vulnerability. We encourage the Cloudflare community to submit any identified vulnerabilities to help us continually improve the security posture of our products and platform. </p><p>We also recognize that the trust you place in us is paramount to the success of your infrastructure on Cloudflare. We consider these vulnerabilities with the utmost concern and will continue to do everything in our power to mitigate impact. We deeply appreciate your continued trust in our platform and remain committed not only to prioritizing security in all we do, but also acting swiftly and transparently whenever an issue does arise. </p> ","content:encodedSnippet":"This post was updated on January 20, 2026.\nOn October 13, 2025, security researchers from FearsOff identified and reported a vulnerability in Cloudflare's ACME (Automatic Certificate Management Environment) validation logic that disabled some of the WAF features on specific ACME-related paths. The vulnerability was reported and validated through Cloudflare’s bug bounty program.\nThe vulnerability was rooted in how our edge network processed requests destined for the ACME HTTP-01 challenge path (/.well-known/acme-challenge/*). \nHere, we’ll briefly explain how this protocol works and the action we took to address the vulnerability. \nCloudflare has patched this vulnerability and there is no action necessary for Cloudflare customers. There is no evidence of any malicious actor abusing this vulnerability.\nHow ACME works to validate certificates\nACME is a protocol used to automate the issuance, renewal, and revocation of SSL/TLS certificates. When an HTTP-01 challenge is used to validate domain ownership, a Certificate Authority (CA) will expect to find a validation token at the HTTP path following the format of http://{customer domain}/.well-known/acme-challenge/{token value}. \nIf this challenge is used by a certificate order managed by Cloudflare, then Cloudflare will respond on this path and provide the token provided by the CA to the caller. If the token provided does not correlate to a Cloudflare managed order, then this request would be passed on to the customer origin, since they may be attempting to complete domain validation as a part of some other system. Check out the flow below for more details — other use cases are discussed later in the blog post.\nThe underlying logic flaw \nCertain requests to /.well-known/acme-challenge/* would cause the logic serving ACME challenge tokens to disable WAF features on a challenge request, and allow the challenge request to continue to the origin when it should have been blocked.\nPreviously, when Cloudflare was serving a HTTP-01 challenge token, if the path requested by the caller matched a token for an active challenge in our system, the logic serving an ACME challenge token would disable WAF features, since Cloudflare would be directly serving the response. This is done because those features can interfere with the CA’s ability to validate the token values and would cause failures with automated certificate orders and renewals.\nHowever, in the scenario that the token used was associated with a different zone and not directly managed by Cloudflare, the request would be allowed to proceed onto the customer origin without further processing by WAF rulesets.\nHow we mitigated this vulnerability\nTo mitigate this issue, a code change was released. This code change only allows the set of security features to be disabled in the event that the request matches a valid ACME HTTP-01 challenge token for the hostname. In that case, Cloudflare has a challenge response to serve back.\nCloudflare customers are protected\nAs we noted above, Cloudflare has patched this vulnerability and Cloudflare customers do not need to take any action. In addition, there is no evidence  of any malicious actor abusing this vulnerability.\nMoving quickly with vulnerability transparency\nAs always, we thank the external researchers for responsibly disclosing this vulnerability. We encourage the Cloudflare community to submit any identified vulnerabilities to help us continually improve the security posture of our products and platform. \nWe also recognize that the trust you place in us is paramount to the success of your infrastructure on Cloudflare. We consider these vulnerabilities with the utmost concern and will continue to do everything in our power to mitigate impact. We deeply appreciate your continued trust in our platform and remain committed not only to prioritizing security in all we do, but also acting swiftly and transparently whenever an issue does arise.","dc:creator":"Hrushikesh Deshpande","content":" A vulnerability was recently identified in Cloudflare’s automation of certificate validation. Here we explain the vulnerability and outline the steps we’ve taken to mitigate it.  ","contentSnippet":"A vulnerability was recently identified in Cloudflare’s automation of certificate validation. Here we explain the vulnerability and outline the steps we’ve taken to mitigate it.","guid":"lHLq8aIK0VMgRiInLXnrw","categories":["Vulnerabilities","WAF","Security","Network Services"],"isoDate":"2026-01-19T14:00:00.000Z"},{"creator":"Fred Schott","title":"Astro is joining Cloudflare","link":"https://blog.cloudflare.com/astro-joins-cloudflare/","pubDate":"Fri, 16 Jan 2026 14:00:00 GMT","content:encoded":" <p>The Astro Technology Company, creators of the Astro web framework, is joining Cloudflare.</p><p><a href=\"https://astro.build/\"><u>Astro</u></a> is the web framework for building fast, content-driven websites. Over the past few years, we’ve seen an incredibly diverse range of developers and companies use Astro to build for the web. This ranges from established brands like Porsche and IKEA, to fast-growing AI companies like Opencode and OpenAI. Platforms that are built on Cloudflare, like <a href=\"https://webflow.com/feature/cloud\"><u>Webflow Cloud</u></a> and <a href=\"https://vibe.wix.com/\"><u>Wix Vibe</u></a>, have chosen Astro to power the websites their customers build and deploy to their own platforms. At Cloudflare, we use Astro, too — for our <a href=\"https://developers.cloudflare.com/\"><u>developer docs</u></a>, <a href=\"https://workers.cloudflare.com/\"><u>website</u></a>, <a href=\"https://sandbox.cloudflare.com/\"><u>landing pages</u></a>, <a href=\"https://blog.cloudflare.com/\"><u>blogg</u></a>, and more. Astro is used almost everywhere there is content on the Internet. </p><p>By joining forces with the Astro team, we are doubling down on making Astro the best framework for content-driven websites for many years to come. The best version of Astro — <a href=\"https://github.com/withastro/astro/milestone/37\"><u>Astro 6</u></a> —  is just around the corner, bringing a redesigned development server powered by Vite. The first public beta release of Astro 6 is <a href=\"https://github.com/withastro/astro/releases/tag/astro%406.0.0-beta.0\"><u>now available</u></a>, with GA coming in the weeks ahead.</p><p>We are excited to share this news and even more thrilled for what it means for developers building with Astro. If you haven’t yet tried Astro — give it a spin and run <a href=\"https://docs.astro.build/en/getting-started/\"><u>npm create astro@latest</u></a>.</p>\n    <div>\n      <h3>What this means for Astro</h3>\n      <a href=\"#what-this-means-for-astro\">\n        \n      </a>\n    </div>\n    <p>Astro will remain open source, MIT-licensed, and open to contributions, with a public roadmap and open governance. All full-time employees of The Astro Technology Company are now employees of Cloudflare, and will continue to work on Astro. We’re committed to Astro’s long-term success and eager to keep building.</p><p>Astro wouldn’t be what it is today without an incredibly strong community of open-source contributors. Cloudflare is also committed to continuing to support open-source contributions, via the <a href=\"https://astro.build/blog/astro-ecosystem-fund-update/\"><u>Astro Ecosystem Fund</u></a>, alongside industry partners including Webflow, Netlify, Wix, Sentry, Stainless and many more.</p><p>From day one, Astro has been a bet on the web and portability: Astro is built to run anywhere, across clouds and platforms. Nothing changes about that. You can deploy Astro to any platform or cloud, and we’re committed to supporting Astro developers everywhere.</p>\n    <div>\n      <h3>There are many web frameworks out there — so why are developers choosing Astro?</h3>\n      <a href=\"#there-are-many-web-frameworks-out-there-so-why-are-developers-choosing-astro\">\n        \n      </a>\n    </div>\n    <p>Astro has been growing rapidly:</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6SiPDolNqvmfQmHftQAr2W/b0b0b0c6725203b945d83da9b190c443/BLOG-3112_2.png\" />\n          </figure><p>Why? Many web frameworks have come and gone trying to be everything to everyone, aiming to serve the needs of both content-driven websites and web applications.</p><p>The key to Astro’s success: Instead of trying to serve every use case, Astro has stayed focused on <a href=\"https://docs.astro.build/en/concepts/why-astro/#design-principles\"><u>five design principles</u></a>. Astro is…</p><ul><li><p><b>Content-driven:</b> Astro was designed to showcase your content.</p></li><li><p><b>Server-first:</b> Websites run faster when they render HTML on the server.</p></li><li><p><b>Fast by default:</b> It should be impossible to build a slow website in Astro.</p></li><li><p><b>Easy to use:</b> You don’t need to be an expert to build something with Astro.</p></li><li><p><b>Developer-focused:</b> You should have the resources you need to be successful.</p></li></ul><p>Astro’s <a href=\"https://docs.astro.build/en/concepts/islands/\"><u>Islands Architecture</u></a> is a core part of what makes all of this possible. The majority of each page can be fast, static HTML — fast and simple to build by default, oriented around rendering content. And when you need it, you can render a specific part of a page as a client island, using any client UI framework. You can even mix and match multiple frameworks on the same page, whether that’s React.js, Vue, Svelte, Solid, or anything else:</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1SjrMUpO9xZb0wxlATkrQo/16afe1efdb57da6b8b17cd804d94cfb2/BLOG-3112_3.png\" />\n          </figure>\n    <div>\n      <h3>Bringing back the joy in building websites</h3>\n      <a href=\"#bringing-back-the-joy-in-building-websites\">\n        \n      </a>\n    </div>\n    <p>The more Astro and Cloudflare started talking, the clearer it became how much we have in common. Cloudflare’s mission is to help build a better Internet — and part of that is to help build a <i>faster</i> Internet. Almost all of us grew up building websites, and we want a world where people have fun building things on the Internet, where anyone can publish to a site that is truly their own.</p><p>When Astro first <a href=\"https://astro.build/blog/introducing-astro/\"><u>launched</u></a> in 2021, it had become painful to build great websites — it felt like a fight with build tools and frameworks. It sounds strange to say it, with the coding agents and powerful LLMs of 2026, but in 2021 it was very hard to build an excellent and fast website without being a domain expert in JavaScript build tooling. So much has gotten better, both because of Astro and in the broader frontend ecosystem, that we take this almost for granted today.</p><p>The Astro project has spent the past five years working to simplify web development. So as LLMs, then vibe coding, and now true coding agents have come along and made it possible for truly anyone to build — Astro provided a foundation that was simple and fast by default. We’ve all seen how much better and faster agents get when building off the right foundation, in a well-structured codebase. More and more, we’ve seen both builders and platforms choose Astro as that foundation.</p><p>We’ve seen this most clearly through the platforms that both Cloudflare and Astro serve, that extend Cloudflare to their own customers in creative ways using <a href=\"https://developers.cloudflare.com/cloudflare-for-platforms/\"><u>Cloudflare for Platforms</u></a>, and have chosen Astro as the framework that their customers build on. </p><p>When you deploy to <a href=\"https://webflow.com/feature/cloud\"><u>Webflow Cloud</u></a>, your Astro site just works and is deployed across Cloudflare’s network. When you start a new project with <a href=\"https://vibe.wix.com/\"><u>Wix Vibe</u></a>, behind the scenes you’re creating an Astro site, running on Cloudflare. And when you generate a developer docs site using <a href=\"https://www.stainless.com/\"><u>Stainless</u></a>, that generates an Astro project, running on Cloudflare, powered by <a href=\"https://astro.build/blog/stainless-astro-launch/\"><u>Starlight</u></a> — a framework built on Astro.</p><p>Each of these platforms is built for a different audience. But what they have in common — beyond their use of Cloudflare and Astro — is they make it <i>fun</i> to create and publish content to the Internet. In a world where everyone can be both a builder and content creator, we think there are still so many more platforms to build and people to reach.</p>\n    <div>\n      <h3><b>Astro 6 — new local dev server, powered by Vite</b></h3>\n      <a href=\"#astro-6-new-local-dev-server-powered-by-vite\">\n        \n      </a>\n    </div>\n    <p>Astro 6 is coming, and the first open beta release is <a href=\"https://astro.build/blog/astro-6-beta/\"><u>now available</u></a>. To be one of the first to try it out, run:</p><p><code>npm create astro@latest -- --ref next</code></p><p>Or to upgrade your existing Astro app, run:</p><p><code>npx @astrojs/upgrade beta</code></p><p>Astro 6 brings a brand new development server, built on the <a href=\"https://vite.dev/guide/api-environment\"><u>Vite Environments API</u></a>, that runs your code locally using the same runtime that you deploy to. This means that when you run <code>astro dev</code> with the <a href=\"https://developers.cloudflare.com/workers/vite-plugin/\"><u>Cloudflare Vite plugin</u></a>, your code runs in <a href=\"https://github.com/cloudflare/workerd\"><u>workerd</u></a>, the open-source Cloudflare Workers runtime, and can use <a href=\"https://developers.cloudflare.com/durable-objects/\"><u>Durable Objects</u></a>, <a href=\"https://developers.cloudflare.com/d1/\"><u>D1</u></a>, <a href=\"https://developers.cloudflare.com/kv/\"><u>KV</u></a>, <a href=\"https://developers.cloudflare.com/agents/\"><u>Agents</u></a> and <a href=\"https://developers.cloudflare.com/workers/runtime-apis/bindings/\"><u>more</u></a>. This isn’t just a Cloudflare feature: Any JavaScript runtime with a plugin that uses the Vite Environments API can benefit from this new support, and ensure local dev runs in the same environment, with the same runtime APIs as production.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4YAgzaSkgUr3gxK5Mkh62V/09847d3f15744b6f049864a6e898a343/BLOG-3112_4.png\" />\n          </figure><p><a href=\"https://docs.astro.build/en/reference/experimental-flags/live-content-collections/\"><u>Live Content Collections</u></a> in Astro are also stable in Astro 6 and out of beta. These content collections let you update data in real time, without requiring a rebuild of your site. This makes it easy to bring in content that changes often, such as the current inventory in a storefront, while still benefitting from the built-in validation and caching that come with Astro’s existing support for <a href=\"https://v6.docs.astro.build/en/guides/content-collections\"><u>content collections</u></a>.</p><p>There’s more to Astro 6, including Astro’s most upvoted feature request — first-class support for Content Security Policy (CSP) — as well as simpler APIs, an upgrade to <a href=\"https://zod.dev/?id=introduction\"><u>Zod</u></a> 4, and more.</p>\n    <div>\n      <h3>Doubling down on Astro</h3>\n      <a href=\"#doubling-down-on-astro\">\n        \n      </a>\n    </div>\n    <p>We're thrilled to welcome the Astro team to Cloudflare. We’re excited to keep building, keep shipping, and keep making Astro the best way to build content-driven sites. We’re already thinking about what comes next beyond V6, and we’d love to hear from you.</p><p>To keep up with the latest, follow the <a href=\"https://astro.build/blog/\"><u>Astro blog</u></a> and join the <a href=\"https://astro.build/chat\"><u>Astro Discord</u></a>. Tell us what you’re building!</p><p></p> ","content:encodedSnippet":"The Astro Technology Company, creators of the Astro web framework, is joining Cloudflare.\nAstro is the web framework for building fast, content-driven websites. Over the past few years, we’ve seen an incredibly diverse range of developers and companies use Astro to build for the web. This ranges from established brands like Porsche and IKEA, to fast-growing AI companies like Opencode and OpenAI. Platforms that are built on Cloudflare, like Webflow Cloud and Wix Vibe, have chosen Astro to power the websites their customers build and deploy to their own platforms. At Cloudflare, we use Astro, too — for our developer docs, website, landing pages, blogg, and more. Astro is used almost everywhere there is content on the Internet. \nBy joining forces with the Astro team, we are doubling down on making Astro the best framework for content-driven websites for many years to come. The best version of Astro — Astro 6 —  is just around the corner, bringing a redesigned development server powered by Vite. The first public beta release of Astro 6 is now available, with GA coming in the weeks ahead.\nWe are excited to share this news and even more thrilled for what it means for developers building with Astro. If you haven’t yet tried Astro — give it a spin and run npm create astro@latest.\nWhat this means for Astro\nAstro will remain open source, MIT-licensed, and open to contributions, with a public roadmap and open governance. All full-time employees of The Astro Technology Company are now employees of Cloudflare, and will continue to work on Astro. We’re committed to Astro’s long-term success and eager to keep building.\nAstro wouldn’t be what it is today without an incredibly strong community of open-source contributors. Cloudflare is also committed to continuing to support open-source contributions, via the Astro Ecosystem Fund, alongside industry partners including Webflow, Netlify, Wix, Sentry, Stainless and many more.\nFrom day one, Astro has been a bet on the web and portability: Astro is built to run anywhere, across clouds and platforms. Nothing changes about that. You can deploy Astro to any platform or cloud, and we’re committed to supporting Astro developers everywhere.\nThere are many web frameworks out there — so why are developers choosing Astro?\nAstro has been growing rapidly:\nWhy? Many web frameworks have come and gone trying to be everything to everyone, aiming to serve the needs of both content-driven websites and web applications.\nThe key to Astro’s success: Instead of trying to serve every use case, Astro has stayed focused on five design principles. Astro is…\n\nContent-driven: Astro was designed to showcase your content.\n\nServer-first: Websites run faster when they render HTML on the server.\n\nFast by default: It should be impossible to build a slow website in Astro.\n\nEasy to use: You don’t need to be an expert to build something with Astro.\n\nDeveloper-focused: You should have the resources you need to be successful.\n\nAstro’s Islands Architecture is a core part of what makes all of this possible. The majority of each page can be fast, static HTML — fast and simple to build by default, oriented around rendering content. And when you need it, you can render a specific part of a page as a client island, using any client UI framework. You can even mix and match multiple frameworks on the same page, whether that’s React.js, Vue, Svelte, Solid, or anything else:\nBringing back the joy in building websites\nThe more Astro and Cloudflare started talking, the clearer it became how much we have in common. Cloudflare’s mission is to help build a better Internet — and part of that is to help build a faster Internet. Almost all of us grew up building websites, and we want a world where people have fun building things on the Internet, where anyone can publish to a site that is truly their own.\nWhen Astro first launched in 2021, it had become painful to build great websites — it felt like a fight with build tools and frameworks. It sounds strange to say it, with the coding agents and powerful LLMs of 2026, but in 2021 it was very hard to build an excellent and fast website without being a domain expert in JavaScript build tooling. So much has gotten better, both because of Astro and in the broader frontend ecosystem, that we take this almost for granted today.\nThe Astro project has spent the past five years working to simplify web development. So as LLMs, then vibe coding, and now true coding agents have come along and made it possible for truly anyone to build — Astro provided a foundation that was simple and fast by default. We’ve all seen how much better and faster agents get when building off the right foundation, in a well-structured codebase. More and more, we’ve seen both builders and platforms choose Astro as that foundation.\nWe’ve seen this most clearly through the platforms that both Cloudflare and Astro serve, that extend Cloudflare to their own customers in creative ways using Cloudflare for Platforms, and have chosen Astro as the framework that their customers build on. \nWhen you deploy to Webflow Cloud, your Astro site just works and is deployed across Cloudflare’s network. When you start a new project with Wix Vibe, behind the scenes you’re creating an Astro site, running on Cloudflare. And when you generate a developer docs site using Stainless, that generates an Astro project, running on Cloudflare, powered by Starlight — a framework built on Astro.\nEach of these platforms is built for a different audience. But what they have in common — beyond their use of Cloudflare and Astro — is they make it fun to create and publish content to the Internet. In a world where everyone can be both a builder and content creator, we think there are still so many more platforms to build and people to reach.\nAstro 6 — new local dev server, powered by Vite\nAstro 6 is coming, and the first open beta release is now available. To be one of the first to try it out, run:\nnpm create astro@latest -- --ref next\nOr to upgrade your existing Astro app, run:\nnpx @astrojs/upgrade beta\nAstro 6 brings a brand new development server, built on the Vite Environments API, that runs your code locally using the same runtime that you deploy to. This means that when you run astro dev with the Cloudflare Vite plugin, your code runs in workerd, the open-source Cloudflare Workers runtime, and can use Durable Objects, D1, KV, Agents and more. This isn’t just a Cloudflare feature: Any JavaScript runtime with a plugin that uses the Vite Environments API can benefit from this new support, and ensure local dev runs in the same environment, with the same runtime APIs as production.\nLive Content Collections in Astro are also stable in Astro 6 and out of beta. These content collections let you update data in real time, without requiring a rebuild of your site. This makes it easy to bring in content that changes often, such as the current inventory in a storefront, while still benefitting from the built-in validation and caching that come with Astro’s existing support for content collections.\nThere’s more to Astro 6, including Astro’s most upvoted feature request — first-class support for Content Security Policy (CSP) — as well as simpler APIs, an upgrade to Zod 4, and more.\nDoubling down on Astro\nWe're thrilled to welcome the Astro team to Cloudflare. We’re excited to keep building, keep shipping, and keep making Astro the best way to build content-driven sites. We’re already thinking about what comes next beyond V6, and we’d love to hear from you.\nTo keep up with the latest, follow the Astro blog and join the Astro Discord. Tell us what you’re building!","dc:creator":"Fred Schott","content":" The Astro Technology Company team — the creators of the Astro web framework — is joining Cloudflare. We’re doubling down on making Astro the best framework for content-driven websites, today and in the years to come. ","contentSnippet":"The Astro Technology Company team — the creators of the Astro web framework — is joining Cloudflare. We’re doubling down on making Astro the best framework for content-driven websites, today and in the years to come.","guid":"6snDEFT5jgryV5wPhY4HEj","categories":["Acquisitions","Application Services","Developer Platform","Cloudflare Workers","Workers AI","Security","AI"],"isoDate":"2026-01-16T14:00:00.000Z"},{"creator":"Will Allen","title":"Human Native is joining Cloudflare","link":"https://blog.cloudflare.com/human-native-joins-cloudflare/","pubDate":"Thu, 15 Jan 2026 14:00:00 GMT","content:encoded":" <p></p><p>Today, we’re excited to share that Cloudflare has acquired <a href=\"https://www.humannative.ai/\"><u>Human Native</u></a>, a UK-based AI data marketplace specializing in transforming multimedia content into searchable and useful data.</p>\n    <div>\n      <h3>Human Native x Cloudflare</h3>\n      <a href=\"#human-native-x-cloudflare\">\n        \n      </a>\n    </div>\n    <p>The Human Native team has spent the past few years focused on helping AI developers create better AI through licensed data. Their technology helps publishers and developers turn messy, unstructured content into something that can be understood, licensed and ultimately valued. They have approached data not as something to be scraped, but as an asset class that deserves structure, transparency and respect.</p><p>Access to high-quality data can lead to better technical performance. One of Human Native’s customers, a prominent UK video AI company, threw away their existing training data after achieving superior results with data sourced through Human Native. Going forward they are only training on fully licensed, reputably sourced, high-quality content.</p><p>This gives a preview of what the economic model of the Internet can be in the age of generative AI: better AI built on better data, with fair control, compensation and credit for creators.</p>\n    <div>\n      <h3>The Internet needs new economic models</h3>\n      <a href=\"#the-internet-needs-new-economic-models\">\n        \n      </a>\n    </div>\n    <p>For the last 30 years, the open Internet has been based on a fundamental value exchange: creators create content, aggregators (such as search engines or social media) send traffic. Creators can monetize that traffic through advertisements, subscriptions or direct support. This is the economic loop that has powered the explosive growth of the Internet.</p><p>But it’s under real strain.</p><p><a href=\"https://blog.cloudflare.com/crawlers-click-ai-bots-training/\"><u>Crawl-to-referral</u></a> ratios are skyrocketing, with 10s of thousands of AI and bot crawls per real human visitor, and<b> </b>it’s unclear how multipurpose crawlers are using the content they access.</p><p>The community of creators who publish on the Internet is a diverse group: news publishers, content creators, financial professionals, technology companies, aggregators and more. But they have one thing in common: They want to decide how their content is used by AI systems.</p><p>Cloudflare’s work in building <a href=\"https://www.cloudflare.com/en-gb/ai-crawl-control/\"><u>AI Crawl Control</u></a> and <a href=\"https://developers.cloudflare.com/ai-crawl-control/features/pay-per-crawl/what-is-pay-per-crawl/\"><u>Pay Per Crawl</u></a> is predicated on a simple philosophy: Content owners should get to decide how and when their content is accessed by others. Many of our customers want to optimize their brand and content to make sure it is in every training data set and shows up in every new search; others want to have more control and only allow access if there is direct compensation.</p><p>Our tools like <a href=\"https://developers.cloudflare.com/ai-search/\"><u>AI Search</u></a>, AI Crawl Control and Pay Per Crawl can help, wherever you land in that equation. The important thing is that the content owner gets to decide.</p>\n    <div>\n      <h3>New tools for AI developers</h3>\n      <a href=\"#new-tools-for-ai-developers\">\n        \n      </a>\n    </div>\n    <p>With the Human Native team joining Cloudflare, we are accelerating our work in helping customers transform their content to be easily accessed and understood by AI bots and agents in addition to their traditional human audiences.</p><p>Crawling is complex, expensive in terms of engineering and compute to process the content, and has no guarantees of quality control. A crawled index can contain duplicates, spam, illegal material and many more headaches. Developers are left with messy, unstructured data.</p><p>We recently announced our work in building the <a href=\"https://blog.cloudflare.com/an-ai-index-for-all-our-customers/\"><u>AI Index</u></a>, a powerful new way for both foundation model companies and agents to access content at scale.</p><p>Instead of sending crawlers blindly and repeatedly across the open Internet, AI developers will be able to connect via a pub/sub model: participating websites will expose structured updates whenever their content changes, and developers will be able to subscribe to receive those updates in real time. </p><p>This opens up new avenues for content creators to experiment with new business models. </p>\n    <div>\n      <h3>Building the foundation for these new business models</h3>\n      <a href=\"#building-the-foundation-for-these-new-business-models\">\n        \n      </a>\n    </div>\n    <p>Cloudflare is investing heavily in creating the foundations for these new business models, starting with x402.</p><p>We recently announced that we are creating the <a href=\"https://blog.cloudflare.com/x402/\"><u>x402 Foundation</u></a>, in partnership with Coinbase, to enable machine-to-machine transactions for digital resources.</p><p>Payments on the web have historically been designed for humans. We browse a merchant’s website, show intent by adding items to a cart, and confirm our intent to purchase by putting in our credit card information and clicking “Pay.” But what if you want to enable direct transactions between automated systems? We need protocols to allow machine-to-machine transactions. </p><p>Together, Human Native and Cloudflare will accelerate our work in building the basis of these new economic models for the Internet. </p>\n    <div>\n      <h3>What’s next</h3>\n      <a href=\"#whats-next\">\n        \n      </a>\n    </div>\n    <p>The Internet works best when it is open, fair, and independently sustainable. We’re excited to welcome the Human Native team to Cloudflare, and even more excited about what we will build together to improve the foundations of the Internet in the age of AI.</p><p>Onwards.</p> ","content:encodedSnippet":"Today, we’re excited to share that Cloudflare has acquired Human Native, a UK-based AI data marketplace specializing in transforming multimedia content into searchable and useful data.\nHuman Native x Cloudflare\nThe Human Native team has spent the past few years focused on helping AI developers create better AI through licensed data. Their technology helps publishers and developers turn messy, unstructured content into something that can be understood, licensed and ultimately valued. They have approached data not as something to be scraped, but as an asset class that deserves structure, transparency and respect.\nAccess to high-quality data can lead to better technical performance. One of Human Native’s customers, a prominent UK video AI company, threw away their existing training data after achieving superior results with data sourced through Human Native. Going forward they are only training on fully licensed, reputably sourced, high-quality content.\nThis gives a preview of what the economic model of the Internet can be in the age of generative AI: better AI built on better data, with fair control, compensation and credit for creators.\nThe Internet needs new economic models\nFor the last 30 years, the open Internet has been based on a fundamental value exchange: creators create content, aggregators (such as search engines or social media) send traffic. Creators can monetize that traffic through advertisements, subscriptions or direct support. This is the economic loop that has powered the explosive growth of the Internet.\nBut it’s under real strain.\nCrawl-to-referral ratios are skyrocketing, with 10s of thousands of AI and bot crawls per real human visitor, and it’s unclear how multipurpose crawlers are using the content they access.\nThe community of creators who publish on the Internet is a diverse group: news publishers, content creators, financial professionals, technology companies, aggregators and more. But they have one thing in common: They want to decide how their content is used by AI systems.\nCloudflare’s work in building AI Crawl Control and Pay Per Crawl is predicated on a simple philosophy: Content owners should get to decide how and when their content is accessed by others. Many of our customers want to optimize their brand and content to make sure it is in every training data set and shows up in every new search; others want to have more control and only allow access if there is direct compensation.\nOur tools like AI Search, AI Crawl Control and Pay Per Crawl can help, wherever you land in that equation. The important thing is that the content owner gets to decide.\nNew tools for AI developers\nWith the Human Native team joining Cloudflare, we are accelerating our work in helping customers transform their content to be easily accessed and understood by AI bots and agents in addition to their traditional human audiences.\nCrawling is complex, expensive in terms of engineering and compute to process the content, and has no guarantees of quality control. A crawled index can contain duplicates, spam, illegal material and many more headaches. Developers are left with messy, unstructured data.\nWe recently announced our work in building the AI Index, a powerful new way for both foundation model companies and agents to access content at scale.\nInstead of sending crawlers blindly and repeatedly across the open Internet, AI developers will be able to connect via a pub/sub model: participating websites will expose structured updates whenever their content changes, and developers will be able to subscribe to receive those updates in real time. \nThis opens up new avenues for content creators to experiment with new business models. \nBuilding the foundation for these new business models\nCloudflare is investing heavily in creating the foundations for these new business models, starting with x402.\nWe recently announced that we are creating the x402 Foundation, in partnership with Coinbase, to enable machine-to-machine transactions for digital resources.\nPayments on the web have historically been designed for humans. We browse a merchant’s website, show intent by adding items to a cart, and confirm our intent to purchase by putting in our credit card information and clicking “Pay.” But what if you want to enable direct transactions between automated systems? We need protocols to allow machine-to-machine transactions. \nTogether, Human Native and Cloudflare will accelerate our work in building the basis of these new economic models for the Internet. \nWhat’s next\nThe Internet works best when it is open, fair, and independently sustainable. We’re excited to welcome the Human Native team to Cloudflare, and even more excited about what we will build together to improve the foundations of the Internet in the age of AI.\nOnwards.","dc:creator":"Will Allen","content":" Cloudflare acquires Human Native, an AI data marketplace specialising in transforming content into searchable and useful data, to accelerate work building new economic models for the Internet. ","contentSnippet":"Cloudflare acquires Human Native, an AI data marketplace specialising in transforming content into searchable and useful data, to accelerate work building new economic models for the Internet.","guid":"Szd19ssv1kbKxjxNZhUmR","categories":["AI","Generative AI","Data","Acquisitions","Developer Platform"],"isoDate":"2026-01-15T14:00:00.000Z"},{"creator":"Sebastiaan Neuteboom","title":"What came first: the CNAME or the A record?","link":"https://blog.cloudflare.com/cname-a-record-order-dns-standards/","pubDate":"Wed, 14 Jan 2026 00:00:00 GMT","content:encoded":" <p>On January 8, 2026, a routine update to 1.1.1.1 aimed at reducing memory usage accidentally triggered a wave of DNS resolution failures for users across the Internet. The root cause wasn't an attack or an outage, but a subtle shift in the order of records within our DNS responses.  </p><p>While most modern software treats the order of records in DNS responses as irrelevant, we discovered that some implementations expect CNAME records to appear before everything else. When that order changed, resolution started failing. This post explores the code change that caused the shift, why it broke specific DNS clients, and the 40-year-old protocol ambiguity that makes the \"correct\" order of a DNS response difficult to define.</p>\n    <div>\n      <h2>Timeline</h2>\n      <a href=\"#timeline\">\n        \n      </a>\n    </div>\n    <p><i>All timestamps referenced are in Coordinated Universal Time (UTC).</i></p><table><tr><th><p><b>Time</b></p></th><th><p><b>Description</b></p></th></tr><tr><td><p>2025-12-02</p></td><td><p>The record reordering is introduced to the 1.1.1.1 codebase</p></td></tr><tr><td><p>2025-12-10</p></td><td><p>The change is released to our testing environment</p></td></tr><tr><td><p>2026-01-07 23:48</p></td><td><p>A global release containing the change starts</p></td></tr><tr><td><p>2026-01-08 17:40</p></td><td><p>The release reaches 90% of servers</p></td></tr><tr><td><p>2026-01-08 18:19</p></td><td><p>Incident is declared</p></td></tr><tr><td><p>2026-01-08 18:27</p></td><td><p>The release is reverted</p></td></tr><tr><td><p>2026-01-08 19:55</p></td><td><p>Revert is completed. Impact ends</p></td></tr></table>\n    <div>\n      <h2>What happened?</h2>\n      <a href=\"#what-happened\">\n        \n      </a>\n    </div>\n    <p>While making some improvements to lower the memory usage of our cache implementation, we introduced a subtle change to CNAME record ordering. The change was introduced on December 2, 2025, released to our testing environment on December 10, and began deployment on January 7, 2026.</p>\n    <div>\n      <h3>How DNS CNAME chains work</h3>\n      <a href=\"#how-dns-cname-chains-work\">\n        \n      </a>\n    </div>\n    <p>When you query for a domain like <code>www.example.com</code>, you might get a <a href=\"https://www.cloudflare.com/learning/dns/dns-records/dns-cname-record/\"><u>CNAME (Canonical Name)</u></a> record that indicates one name is an alias for another name. It’s the job of public resolvers, such as <a href=\"https://www.cloudflare.com/learning/dns/what-is-1.1.1.1/\"><u>1.1.1.1</u></a>, to follow this chain of aliases until it reaches a final response:</p><p><code>www.example.com → cdn.example.com → server.cdn-provider.com → 198.51.100.1</code></p><p>As 1.1.1.1 traverses this chain, it caches every intermediate record. Each record in the chain has its own <a href=\"https://www.cloudflare.com/learning/cdn/glossary/time-to-live-ttl/\"><u>TTL (Time-To-Live)</u></a>, indicating how long we can cache it. Not all the TTLs in a CNAME chain need to be the same:</p><p><code>www.example.com → cdn.example.com (TTL: 3600 seconds) # Still cached\ncdn.example.com → 198.51.100.1    (TTL: 300 seconds)  # Expired</code></p><p>When one or more records in a CNAME chain expire, it’s considered partially expired. Fortunately, since parts of the chain are still in our cache, we don’t have to resolve the entire CNAME chain again — only the part that has expired. In our example above, we would take the still valid <code>www.example.com → cdn.example.com</code> chain, and only resolve the expired <code>cdn.example.com</code> <a href=\"https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/\"><u>A record</u></a>. Once that’s done, we combine the existing CNAME chain and the newly resolved records into a single response.</p>\n    <div>\n      <h3>The logic change</h3>\n      <a href=\"#the-logic-change\">\n        \n      </a>\n    </div>\n    <p>The code that merges these two chains is where the change occurred. Previously, the code would create a new list, insert the existing CNAME chain, and then append the new records:</p>\n            <pre><code>impl PartialChain {\n    /// Merges records to the cache entry to make the cached records complete.\n    pub fn fill_cache(&amp;self, entry: &amp;mut CacheEntry) {\n        let mut answer_rrs = Vec::with_capacity(entry.answer.len() + self.records.len());\n        answer_rrs.extend_from_slice(&amp;self.records); // CNAMEs first\n        answer_rrs.extend_from_slice(&amp;entry.answer); // Then A/AAAA records\n        entry.answer = answer_rrs;\n    }\n}\n</code></pre>\n            <p>However, to save some memory allocations and copies, the code was changed to instead append the CNAMEs to the existing answer list:</p>\n            <pre><code>impl PartialChain {\n    /// Merges records to the cache entry to make the cached records complete.\n    pub fn fill_cache(&amp;self, entry: &amp;mut CacheEntry) {\n        entry.answer.extend(self.records); // CNAMEs last\n    }\n}\n</code></pre>\n            <p>As a result, the responses that 1.1.1.1 returned now sometimes had the CNAME records appearing at the bottom, after the final resolved answer.</p>\n    <div>\n      <h3>Why this caused impact</h3>\n      <a href=\"#why-this-caused-impact\">\n        \n      </a>\n    </div>\n    <p>When DNS clients receive a response with a CNAME chain in the answer section, they also need to follow this chain to find out that <code>www.example.com</code> points to <code>198.51.100.1</code>. Some DNS client implementations handle this by keeping track of the expected name for the records as they’re iterated sequentially. When a CNAME is encountered, the expected name is updated:</p>\n            <pre><code>;; QUESTION SECTION:\n;; www.example.com.        IN    A\n\n;; ANSWER SECTION:\nwww.example.com.    3600   IN    CNAME  cdn.example.com.\ncdn.example.com.    300    IN    A      198.51.100.1\n</code></pre>\n            <p></p><ol><li><p>Find records for <code>www.example.com</code></p></li><li><p>Encounter <code>www.example.com. CNAME cdn.example.com</code></p></li><li><p>Find records for <code>cdn.example.com</code></p></li><li><p>Encounter <code>cdn.example.com. A 198.51.100.1</code></p></li></ol><p>When the CNAME suddenly appears at the bottom, this no longer works:</p>\n            <pre><code>;; QUESTION SECTION:\n;; www.example.com.\t       IN    A\n\n;; ANSWER SECTION:\ncdn.example.com.    300    IN    A      198.51.100.1\nwww.example.com.    3600   IN    CNAME  cdn.example.com.\n</code></pre>\n            <p></p><ol><li><p>Find records for <code>www.example.com</code></p></li><li><p>Ignore <code>cdn.example.com. A 198.51.100.1</code> as it doesn’t match the expected name</p></li><li><p>Encounter <code>www.example.com. CNAME cdn.example.com</code></p></li><li><p>Find records for <code>cdn.example.com</code></p></li><li><p>No more records are present, so the response is considered empty</p></li></ol><p>One such implementation that broke is the <a href=\"https://man7.org/linux/man-pages/man3/getaddrinfo.3.html\"><code><u>getaddrinfo</u></code></a> function in glibc, which is commonly used on Linux for DNS resolution. When looking at its <code>getanswer_r</code> implementation, we can indeed see it expects to find the CNAME records before any answers:</p>\n            <pre><code>for (; ancount &gt; 0; --ancount)\n  {\n    // ... parsing DNS records ...\n    \n    if (rr.rtype == T_CNAME)\n      {\n        /* Record the CNAME target as the new expected name. */\n        int n = __ns_name_unpack (c.begin, c.end, rr.rdata,\n                                  name_buffer, sizeof (name_buffer));\n        expected_name = name_buffer;  // Update what we're looking for\n      }\n    else if (rr.rtype == qtype\n             &amp;&amp; __ns_samebinaryname (rr.rname, expected_name)  // Must match!\n             &amp;&amp; rr.rdlength == rrtype_to_rdata_length (type:qtype))\n      {\n        /* Address record matches - store it */\n        ptrlist_add (list:addresses, item:(char *) alloc_buffer_next (abuf, uint32_t));\n        alloc_buffer_copy_bytes (buf:abuf, src:rr.rdata, size:rr.rdlength);\n      }\n  }\n</code></pre>\n            <p>Another notable affected implementation was the DNSC process in three models of Cisco ethernet switches. In the case where switches had been configured to use 1.1.1.1 these switches experienced spontaneous reboot loops when they received a response containing the reordered CNAMEs. <a href=\"https://www.cisco.com/c/en/us/support/docs/smb/switches/Catalyst-switches/kmgmt3846-cbs-reboot-with-fatal-error-from-dnsc-process.html\"><u>Cisco has published a service document describing the issue</u></a>.</p>\n    <div>\n      <h3>Not all implementations break</h3>\n      <a href=\"#not-all-implementations-break\">\n        \n      </a>\n    </div>\n    <p>Most DNS clients don’t have this issue. For example, <a href=\"https://www.freedesktop.org/software/systemd/man/latest/systemd-resolved.service.html\"><u>systemd-resolved</u></a> first parses the records into an ordered set:</p>\n            <pre><code>typedef struct DnsAnswerItem {\n        DnsResourceRecord *rr; // The actual record\n        DnsAnswerFlags flags;  // Which section it came from\n        // ... other metadata\n} DnsAnswerItem;\n\n\ntypedef struct DnsAnswer {\n        unsigned n_ref;\n        OrderedSet *items;\n} DnsAnswer;\n</code></pre>\n            <p>When following a CNAME chain it can then search the entire answer set, even if the CNAME records don’t appear at the top.</p>\n    <div>\n      <h2>What the RFC says</h2>\n      <a href=\"#what-the-rfc-says\">\n        \n      </a>\n    </div>\n    <p><a href=\"https://datatracker.ietf.org/doc/html/rfc1034\"><u>RFC 1034</u></a>, published in 1987, defines much of the behavior of the DNS protocol, and should give us an answer on whether the order of CNAME records matters. <a href=\"https://datatracker.ietf.org/doc/html/rfc1034#section-4.3.1\"><u>Section 4.3.1</u></a> contains the following text:</p><blockquote><p>If recursive service is requested and available, the recursive response to a query will be one of the following:</p><p>- The answer to the query, possibly preface by one or more CNAME RRs that specify aliases encountered on the way to an answer.</p></blockquote><p>While \"possibly preface\" can be interpreted as a requirement for CNAME records to appear before everything else, it does not use normative key words, such as <a href=\"https://datatracker.ietf.org/doc/html/rfc2119\"><u>MUST and SHOULD</u></a> that modern RFCs use to express requirements. This isn’t a flaw in RFC 1034, but simply a result of its age. <a href=\"https://datatracker.ietf.org/doc/html/rfc2119\"><u>RFC 2119</u></a>, which standardized these key words, was published in 1997, 10 years <i>after</i> RFC 1034.</p><p>In our case, we did originally implement the specification so that CNAMEs appear first. However, we did not have any tests asserting the behavior remains consistent due to the ambiguous language in the RFC.</p>\n    <div>\n      <h3>The subtle distinction: RRsets vs RRs in message sections</h3>\n      <a href=\"#the-subtle-distinction-rrsets-vs-rrs-in-message-sections\">\n        \n      </a>\n    </div>\n    <p>To understand why this ambiguity exists, we need to understand a subtle but important distinction in DNS terminology.</p><p>RFC 1034 <a href=\"https://datatracker.ietf.org/doc/html/rfc1034#section-3.6\"><u>section 3.6</u></a> defines Resource Record Sets (RRsets) as collections of records with the same name, type, and class. For RRsets, the specification is clear about ordering:</p><blockquote><p>The order of RRs in a set is not significant, and need not be preserved by name servers, resolvers, or other parts of the DNS.</p></blockquote><p>However, RFC 1034 doesn’t clearly specify how message sections relate to RRsets. While modern DNS specifications have shown that message sections can indeed contain multiple RRsets (consider <a href=\"https://www.cloudflare.com/learning/dns/dnssec/how-dnssec-works/\">DNSSEC</a> responses with signatures), RFC 1034 doesn’t describe message sections in those terms. Instead, it treats message sections as containing individual Resource Records (RRs).</p><p>The problem is that the RFC primarily discusses ordering in the context of RRsets but doesn't specify the ordering of different RRsets relative to each other within a message section. This is where the ambiguity lives.</p><p>RFC 1034 <a href=\"https://datatracker.ietf.org/doc/html/rfc1034#section-6.2.1\"><u>section 6.2.1</u></a> includes an example that demonstrates this ambiguity further. It mentions that the order of Resource Records (RRs) is not significant either:</p><blockquote><p>The difference in ordering of the RRs in the answer section is not significant.</p></blockquote><p>However, this example only shows two A records for the same name within the same RRset. It doesn't address whether this applies to different record types like CNAMEs and A records.</p>\n    <div>\n      <h2>CNAME chain ordering</h2>\n      <a href=\"#cname-chain-ordering\">\n        \n      </a>\n    </div>\n    <p>It turns out that this issue extends beyond putting CNAME records before other record types. Even when CNAMEs appear before other records, sequential parsing can still break if the CNAME chain itself is out of order. Consider the following response:</p>\n            <pre><code>;; QUESTION SECTION:\n;; www.example.com.              IN    A\n\n;; ANSWER SECTION:\ncdn.example.com.           3600  IN    CNAME  server.cdn-provider.com.\nwww.example.com.           3600  IN    CNAME  cdn.example.com.\nserver.cdn-provider.com.   300   IN    A      198.51.100.1\n</code></pre>\n            <p>Each CNAME belongs to a different RRset, as they have different owners, so the statement about RRset order being insignificant doesn’t apply here.</p><p>However, RFC 1034 doesn't specify that CNAME chains must appear in any particular order. There's no requirement that <code>www.example.com. CNAME cdn.example.com.</code> must appear before <code>cdn.example.com. CNAME server.cdn-provider.com.</code>. With sequential parsing, the same issue occurs:</p><ol><li><p>Find records for <code>www.example.com</code></p></li><li><p>Ignore <code>cdn.example.com. CNAME server.cdn-provider.com</code>. as it doesn’t match the expected name</p></li><li><p>Encounter <code>www.example.com. CNAME cdn.example.com</code></p></li><li><p>Find records for <code>cdn.example.com</code></p></li><li><p>Ignore <code>server.cdn-provider.com. A 198.51.100.1</code> as it doesn’t match the expected name</p></li></ol>\n    <div>\n      <h2>What should resolvers do?</h2>\n      <a href=\"#what-should-resolvers-do\">\n        \n      </a>\n    </div>\n    <p>RFC 1034 section 5 describes resolver behavior. <a href=\"https://datatracker.ietf.org/doc/html/rfc1034#section-5.2.2\"><u>Section 5.2.2</u></a> specifically addresses how resolvers should handle aliases (CNAMEs): </p><blockquote><p>In most cases a resolver simply restarts the query at the new name when it encounters a CNAME.</p></blockquote><p>This suggests that resolvers should restart the query upon finding a CNAME, regardless of where it appears in the response. However, it's important to distinguish between different types of resolvers:</p><ul><li><p>Recursive resolvers, like 1.1.1.1, are full DNS resolvers that perform recursive resolution by querying authoritative nameservers</p></li><li><p>Stub resolvers, like glibc’s getaddrinfo, are simplified local interfaces that forward queries to recursive resolvers and process the responses</p></li></ul><p>The RFC sections on resolver behavior were primarily written with full resolvers in mind, not the simplified stub resolvers that most applications actually use. Some stub resolvers evidently don’t implement certain parts of the spec, such as the CNAME-restart logic described in the RFC. </p>\n    <div>\n      <h2>The DNSSEC specifications provide contrast</h2>\n      <a href=\"#the-dnssec-specifications-provide-contrast\">\n        \n      </a>\n    </div>\n    <p>Later DNS specifications demonstrate a different approach to defining record ordering. <a href=\"https://datatracker.ietf.org/doc/html/rfc4035\"><u>RFC 4035</u></a>, which defines protocol modifications for <a href=\"https://www.cloudflare.com/learning/dns/dnssec/how-dnssec-works/\"><u>DNSSEC</u></a>, uses more explicit language:</p><blockquote><p>When placing a signed RRset in the Answer section, the name server MUST also place its RRSIG RRs in the Answer section. The RRSIG RRs have a higher priority for inclusion than any other RRsets that may have to be included.</p></blockquote><p>The specification uses \"MUST\" and explicitly defines \"higher priority\" for <a href=\"https://www.cloudflare.com/learning/dns/dnssec/how-dnssec-works/\"><u>RRSIG</u></a> records. However, \"higher priority for inclusion\" refers to whether RRSIGs should be included in the response, not where they should appear. This provides unambiguous guidance to implementers about record inclusion in DNSSEC contexts, while not mandating any particular behavior around record ordering.</p><p>For unsigned zones, however, the ambiguity from RFC 1034 remains. The word \"preface\" has guided implementation behavior for nearly four decades, but it has never been formally specified as a requirement.</p>\n    <div>\n      <h2>Do CNAME records come first?</h2>\n      <a href=\"#do-cname-records-come-first\">\n        \n      </a>\n    </div>\n    <p>While in our interpretation the RFCs do not require CNAMEs to appear in any particular order, it’s clear that at least some widely-deployed DNS clients rely on it. As some systems using these clients might be updated infrequently, or never updated at all, we believe it’s best to require CNAME records to appear in-order before any other records.</p><p>Based on what we have learned during this incident, we have reverted the CNAME re-ordering and do not intend to change the order in the future.</p><p>To prevent any future incidents or confusion, we have written a proposal in the form of an <a href=\"https://www.ietf.org/participate/ids/\"><u>Internet-Draft</u></a> to be discussed at the IETF. If consensus is reached on the clarified behavior, this would become an RFC that explicitly defines how to correctly handle CNAMEs in DNS responses, helping us and the wider DNS community navigate the protocol. The proposal can be found at <a href=\"https://datatracker.ietf.org/doc/draft-jabley-dnsop-ordered-answer-section/\">https://datatracker.ietf.org/doc/draft-jabley-dnsop-ordered-answer-section</a>. If you have suggestions or feedback we would love to hear your opinions, most usefully via the <a href=\"https://datatracker.ietf.org/wg/dnsop/about/\"><u>DNSOP working group</u></a> at the IETF.</p> ","content:encodedSnippet":"On January 8, 2026, a routine update to 1.1.1.1 aimed at reducing memory usage accidentally triggered a wave of DNS resolution failures for users across the Internet. The root cause wasn't an attack or an outage, but a subtle shift in the order of records within our DNS responses.  \nWhile most modern software treats the order of records in DNS responses as irrelevant, we discovered that some implementations expect CNAME records to appear before everything else. When that order changed, resolution started failing. This post explores the code change that caused the shift, why it broke specific DNS clients, and the 40-year-old protocol ambiguity that makes the \"correct\" order of a DNS response difficult to define.\nTimeline\nAll timestamps referenced are in Coordinated Universal Time (UTC).\n\n\nTime\n\nDescription\n\n\n2025-12-02\n\nThe record reordering is introduced to the 1.1.1.1 codebase\n\n\n2025-12-10\n\nThe change is released to our testing environment\n\n\n2026-01-07 23:48\n\nA global release containing the change starts\n\n\n2026-01-08 17:40\n\nThe release reaches 90% of servers\n\n\n2026-01-08 18:19\n\nIncident is declared\n\n\n2026-01-08 18:27\n\nThe release is reverted\n\n\n2026-01-08 19:55\n\nRevert is completed. Impact ends\n\n\n    \nWhat happened?\nWhile making some improvements to lower the memory usage of our cache implementation, we introduced a subtle change to CNAME record ordering. The change was introduced on December 2, 2025, released to our testing environment on December 10, and began deployment on January 7, 2026.\nHow DNS CNAME chains work\nWhen you query for a domain like www.example.com, you might get a CNAME (Canonical Name) record that indicates one name is an alias for another name. It’s the job of public resolvers, such as 1.1.1.1, to follow this chain of aliases until it reaches a final response:\nwww.example.com → cdn.example.com → server.cdn-provider.com → 198.51.100.1\nAs 1.1.1.1 traverses this chain, it caches every intermediate record. Each record in the chain has its own TTL (Time-To-Live), indicating how long we can cache it. Not all the TTLs in a CNAME chain need to be the same:\nwww.example.com → cdn.example.com (TTL: 3600 seconds) # Still cached\ncdn.example.com → 198.51.100.1    (TTL: 300 seconds)  # Expired\nWhen one or more records in a CNAME chain expire, it’s considered partially expired. Fortunately, since parts of the chain are still in our cache, we don’t have to resolve the entire CNAME chain again — only the part that has expired. In our example above, we would take the still valid www.example.com → cdn.example.com chain, and only resolve the expired cdn.example.com A record. Once that’s done, we combine the existing CNAME chain and the newly resolved records into a single response.\nThe logic change\nThe code that merges these two chains is where the change occurred. Previously, the code would create a new list, insert the existing CNAME chain, and then append the new records:\nimpl PartialChain {\n    /// Merges records to the cache entry to make the cached records complete.\n    pub fn fill_cache(&self, entry: &mut CacheEntry) {\n        let mut answer_rrs = Vec::with_capacity(entry.answer.len() + self.records.len());\n        answer_rrs.extend_from_slice(&self.records); // CNAMEs first\n        answer_rrs.extend_from_slice(&entry.answer); // Then A/AAAA records\n        entry.answer = answer_rrs;\n    }\n}\n\nHowever, to save some memory allocations and copies, the code was changed to instead append the CNAMEs to the existing answer list:\nimpl PartialChain {\n    /// Merges records to the cache entry to make the cached records complete.\n    pub fn fill_cache(&self, entry: &mut CacheEntry) {\n        entry.answer.extend(self.records); // CNAMEs last\n    }\n}\n\nAs a result, the responses that 1.1.1.1 returned now sometimes had the CNAME records appearing at the bottom, after the final resolved answer.\nWhy this caused impact\nWhen DNS clients receive a response with a CNAME chain in the answer section, they also need to follow this chain to find out that www.example.com points to 198.51.100.1. Some DNS client implementations handle this by keeping track of the expected name for the records as they’re iterated sequentially. When a CNAME is encountered, the expected name is updated:\n;; QUESTION SECTION:\n;; www.example.com.        IN    A\n\n;; ANSWER SECTION:\nwww.example.com.    3600   IN    CNAME  cdn.example.com.\ncdn.example.com.    300    IN    A      198.51.100.1\n\n\n\nFind records for www.example.com\n\nEncounter www.example.com. CNAME cdn.example.com\n\nFind records for cdn.example.com\n\nEncounter cdn.example.com. A 198.51.100.1\n\nWhen the CNAME suddenly appears at the bottom, this no longer works:\n;; QUESTION SECTION:\n;; www.example.com.\t       IN    A\n\n;; ANSWER SECTION:\ncdn.example.com.    300    IN    A      198.51.100.1\nwww.example.com.    3600   IN    CNAME  cdn.example.com.\n\n\n\nFind records for www.example.com\n\nIgnore cdn.example.com. A 198.51.100.1 as it doesn’t match the expected name\n\nEncounter www.example.com. CNAME cdn.example.com\n\nFind records for cdn.example.com\n\nNo more records are present, so the response is considered empty\n\nOne such implementation that broke is the getaddrinfo function in glibc, which is commonly used on Linux for DNS resolution. When looking at its getanswer_r implementation, we can indeed see it expects to find the CNAME records before any answers:\nfor (; ancount > 0; --ancount)\n  {\n    // ... parsing DNS records ...\n    \n    if (rr.rtype == T_CNAME)\n      {\n        /* Record the CNAME target as the new expected name. */\n        int n = __ns_name_unpack (c.begin, c.end, rr.rdata,\n                                  name_buffer, sizeof (name_buffer));\n        expected_name = name_buffer;  // Update what we're looking for\n      }\n    else if (rr.rtype == qtype\n             && __ns_samebinaryname (rr.rname, expected_name)  // Must match!\n             && rr.rdlength == rrtype_to_rdata_length (type:qtype))\n      {\n        /* Address record matches - store it */\n        ptrlist_add (list:addresses, item:(char *) alloc_buffer_next (abuf, uint32_t));\n        alloc_buffer_copy_bytes (buf:abuf, src:rr.rdata, size:rr.rdlength);\n      }\n  }\n\nAnother notable affected implementation was the DNSC process in three models of Cisco ethernet switches. In the case where switches had been configured to use 1.1.1.1 these switches experienced spontaneous reboot loops when they received a response containing the reordered CNAMEs. Cisco has published a service document describing the issue.\nNot all implementations break\nMost DNS clients don’t have this issue. For example, systemd-resolved first parses the records into an ordered set:\ntypedef struct DnsAnswerItem {\n        DnsResourceRecord *rr; // The actual record\n        DnsAnswerFlags flags;  // Which section it came from\n        // ... other metadata\n} DnsAnswerItem;\n\n\ntypedef struct DnsAnswer {\n        unsigned n_ref;\n        OrderedSet *items;\n} DnsAnswer;\n\nWhen following a CNAME chain it can then search the entire answer set, even if the CNAME records don’t appear at the top.\nWhat the RFC says\nRFC 1034, published in 1987, defines much of the behavior of the DNS protocol, and should give us an answer on whether the order of CNAME records matters. Section 4.3.1 contains the following text:\n\nIf recursive service is requested and available, the recursive response to a query will be one of the following:\n- The answer to the query, possibly preface by one or more CNAME RRs that specify aliases encountered on the way to an answer.\n\nWhile \"possibly preface\" can be interpreted as a requirement for CNAME records to appear before everything else, it does not use normative key words, such as MUST and SHOULD that modern RFCs use to express requirements. This isn’t a flaw in RFC 1034, but simply a result of its age. RFC 2119, which standardized these key words, was published in 1997, 10 years after RFC 1034.\nIn our case, we did originally implement the specification so that CNAMEs appear first. However, we did not have any tests asserting the behavior remains consistent due to the ambiguous language in the RFC.\nThe subtle distinction: RRsets vs RRs in message sections\nTo understand why this ambiguity exists, we need to understand a subtle but important distinction in DNS terminology.\nRFC 1034 section 3.6 defines Resource Record Sets (RRsets) as collections of records with the same name, type, and class. For RRsets, the specification is clear about ordering:\n\nThe order of RRs in a set is not significant, and need not be preserved by name servers, resolvers, or other parts of the DNS.\n\nHowever, RFC 1034 doesn’t clearly specify how message sections relate to RRsets. While modern DNS specifications have shown that message sections can indeed contain multiple RRsets (consider DNSSEC responses with signatures), RFC 1034 doesn’t describe message sections in those terms. Instead, it treats message sections as containing individual Resource Records (RRs).\nThe problem is that the RFC primarily discusses ordering in the context of RRsets but doesn't specify the ordering of different RRsets relative to each other within a message section. This is where the ambiguity lives.\nRFC 1034 section 6.2.1 includes an example that demonstrates this ambiguity further. It mentions that the order of Resource Records (RRs) is not significant either:\n\nThe difference in ordering of the RRs in the answer section is not significant.\n\nHowever, this example only shows two A records for the same name within the same RRset. It doesn't address whether this applies to different record types like CNAMEs and A records.\nCNAME chain ordering\nIt turns out that this issue extends beyond putting CNAME records before other record types. Even when CNAMEs appear before other records, sequential parsing can still break if the CNAME chain itself is out of order. Consider the following response:\n;; QUESTION SECTION:\n;; www.example.com.              IN    A\n\n;; ANSWER SECTION:\ncdn.example.com.           3600  IN    CNAME  server.cdn-provider.com.\nwww.example.com.           3600  IN    CNAME  cdn.example.com.\nserver.cdn-provider.com.   300   IN    A      198.51.100.1\n\nEach CNAME belongs to a different RRset, as they have different owners, so the statement about RRset order being insignificant doesn’t apply here.\nHowever, RFC 1034 doesn't specify that CNAME chains must appear in any particular order. There's no requirement that www.example.com. CNAME cdn.example.com. must appear before cdn.example.com. CNAME server.cdn-provider.com.. With sequential parsing, the same issue occurs:\n\nFind records for www.example.com\n\nIgnore cdn.example.com. CNAME server.cdn-provider.com. as it doesn’t match the expected name\n\nEncounter www.example.com. CNAME cdn.example.com\n\nFind records for cdn.example.com\n\nIgnore server.cdn-provider.com. A 198.51.100.1 as it doesn’t match the expected name\n\nWhat should resolvers do?\nRFC 1034 section 5 describes resolver behavior. Section 5.2.2 specifically addresses how resolvers should handle aliases (CNAMEs): \n\nIn most cases a resolver simply restarts the query at the new name when it encounters a CNAME.\n\nThis suggests that resolvers should restart the query upon finding a CNAME, regardless of where it appears in the response. However, it's important to distinguish between different types of resolvers:\n\nRecursive resolvers, like 1.1.1.1, are full DNS resolvers that perform recursive resolution by querying authoritative nameservers\n\nStub resolvers, like glibc’s getaddrinfo, are simplified local interfaces that forward queries to recursive resolvers and process the responses\n\nThe RFC sections on resolver behavior were primarily written with full resolvers in mind, not the simplified stub resolvers that most applications actually use. Some stub resolvers evidently don’t implement certain parts of the spec, such as the CNAME-restart logic described in the RFC. \nThe DNSSEC specifications provide contrast\nLater DNS specifications demonstrate a different approach to defining record ordering. RFC 4035, which defines protocol modifications for DNSSEC, uses more explicit language:\n\nWhen placing a signed RRset in the Answer section, the name server MUST also place its RRSIG RRs in the Answer section. The RRSIG RRs have a higher priority for inclusion than any other RRsets that may have to be included.\n\nThe specification uses \"MUST\" and explicitly defines \"higher priority\" for RRSIG records. However, \"higher priority for inclusion\" refers to whether RRSIGs should be included in the response, not where they should appear. This provides unambiguous guidance to implementers about record inclusion in DNSSEC contexts, while not mandating any particular behavior around record ordering.\nFor unsigned zones, however, the ambiguity from RFC 1034 remains. The word \"preface\" has guided implementation behavior for nearly four decades, but it has never been formally specified as a requirement.\nDo CNAME records come first?\nWhile in our interpretation the RFCs do not require CNAMEs to appear in any particular order, it’s clear that at least some widely-deployed DNS clients rely on it. As some systems using these clients might be updated infrequently, or never updated at all, we believe it’s best to require CNAME records to appear in-order before any other records.\nBased on what we have learned during this incident, we have reverted the CNAME re-ordering and do not intend to change the order in the future.\nTo prevent any future incidents or confusion, we have written a proposal in the form of an Internet-Draft to be discussed at the IETF. If consensus is reached on the clarified behavior, this would become an RFC that explicitly defines how to correctly handle CNAMEs in DNS responses, helping us and the wider DNS community navigate the protocol. The proposal can be found at https://datatracker.ietf.org/doc/draft-jabley-dnsop-ordered-answer-section. If you have suggestions or feedback we would love to hear your opinions, most usefully via the DNSOP working group at the IETF.","dc:creator":"Sebastiaan Neuteboom","content":" A recent change to 1.1.1.1 accidentally altered the order of CNAME records in DNS responses, breaking resolution for some clients. This post explores the technical root cause, examines the source code of affected resolvers, and dives into the inherent ambiguities of the DNS RFCs.   ","contentSnippet":"A recent change to 1.1.1.1 accidentally altered the order of CNAME records in DNS responses, breaking resolution for some clients. This post explores the technical root cause, examines the source code of affected resolvers, and dives into the inherent ambiguities of the DNS RFCs.","guid":"3fP84BsxwSxKr7ffpmVO6s","categories":["1.1.1.1","Post Mortem","DNS","Resolver","Standards","Bugs","Consumer Services"],"isoDate":"2026-01-14T00:00:00.000Z"},{"creator":"David Belson","title":"What we know about Iran’s Internet shutdown","link":"https://blog.cloudflare.com/iran-protests-internet-shutdown/","pubDate":"Tue, 13 Jan 2026 00:00:00 GMT","content:encoded":" <p>In late December 2025, <a href=\"https://apnews.com/article/iran-protests-us-israel-war-economy-d5da3b5f56449dd3871c9438c07f069f\"><u>wide-scale protests erupted across multiple cities in Iran</u></a>. While these protests were initially fueled by frustration over inflation, food prices, and currency depreciation, they have grown into demonstrations <a href=\"https://www.theguardian.com/world/2025/dec/31/we-want-the-mullahs-gone-economic-crisis-sparks-biggest-protests-in-iran-since-2022\"><u>demanding a change</u></a> in the country’s leadership regime. </p><p>In the last few days, Internet traffic from Iran has effectively dropped to zero. This is evident in the data available in Cloudflare Radar, as we’ll describe in this post.  </p>\n    <div>\n      <h2>Background</h2>\n      <a href=\"#background\">\n        \n      </a>\n    </div>\n    <p>The Iranian government has a history of cutting off Internet connectivity when such protests take place. In November 2019, protests erupted following the announcement of a significant increase in fuel prices. In response, the Iranian government implemented an <a href=\"https://iran-shutdown.amnesty.org/\"><u>Internet shutdown</u></a> for more than five days. In September 2022, <a href=\"https://www.cnn.com/2022/09/21/middleeast/iran-mahsa-amini-death-widespread-protests-intl-hnk/index.html\"><u>protests and demonstrations erupted across Iran</u></a> in response to the death <a href=\"https://www.aljazeera.com/news/2022/9/16/iranian-woman-dies-after-moral-polices-detention-reports\"><u>in police custody</u></a> of Mahsa/Zhina Amini, a 22-year-old woman from the Kurdistan Province of Iran. Internet services were <a href=\"https://blog.cloudflare.com/protests-internet-disruption-ir/\"><u>disrupted across multiple network providers</u></a> in the following days.</p><p>Amid the current protests, lower traffic volumes were already <a href=\"https://x.com/nima/status/2007830078093250904\"><u>observed</u></a> at the start of the year, indicating potential connectivity issues leading into the more dramatic shutdown that has followed. </p>\n    <div>\n      <h2>Internet connectivity in Iran plummeted on January 8</h2>\n      <a href=\"#internet-connectivity-in-iran-plummeted-on-january-8\">\n        \n      </a>\n    </div>\n    <p>Some traffic anomalies were seen in the first few days of 2026 (described in further detail below), though peak traffic levels recovered by January 5, and exceeded expected levels during the following days.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2cBTGRkiBSJTfJd1QMuXYV/195ffd12571ef25f35c753d8f2594d38/unnamed.png\" />\n          </figure><p>However, this strong recovery proved to be short-lived. IPv6-related shifts observed on January 8 provided the first indication of the changes to come. At 11:50 UTC (15:20 local time), <a href=\"https://x.com/CloudflareRadar/status/2009266152355041543\"><u>the amount of IPv6 address space announced by Iranian networks dropped by 98.5%</u></a>, falling from over 48 million /48s (blocks of 2^80 IPv6 addresses) to just over 737,000 /48s. A drop in announced IP address space (whether IPv6 or IPv4) means that the announcing networks are no longer telling the world how to reach those addresses. A major drop like this one can signal an intentional disruption to Internet connectivity, as there is no longer a path to the clients or servers using those IP addresses.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2lQVvoLc77jO11pGGPRG5N/44eabb313e139af6f93ee976c6cd70e7/BLOG-3110_2.png\" />\n          </figure><p>This drop in announced IPv6 address space served to reduce IPv6’s share of human-generated traffic from around 12% to around 2%.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5GOdWNEvSKh4HapwJEYsOW/878b4cdfcacf00a416629b7aae464f20/BLOG-3110_3.png\" />\n          </figure><p>As seen in the graph below, this drop in IPv6 traffic stayed at a relatively consistent level for approximately 100 minutes, before falling further just before 13:30 UTC (17:00 local time). This second drop resulted in IPv6 traffic from Iran all but disappearing.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5oYBhaT8U8NKRMYTMfSWY2/21a3aee3066274b11c7b4c059a789bbc/BLOG-3110_4.jpg\" />\n          </figure><p>Several hours later, we observed <a href=\"https://x.com/CloudflareRadar/status/2009312093749801350?s=20\"><u>overall traffic levels from the country begin to decline rapidly</u></a>. Between 16:30 - 17:00 UTC (20:00 - 20:30 local time), traffic volumes fell nearly 90%, fueled by a loss of traffic from the major Iranian network providers, including <a href=\"https://radar.cloudflare.com/as197207\"><u>MCCI (AS197207)</u></a>, <a href=\"https://radar.cloudflare.com/as44244\"><u>IranCell (AS44244)</u></a>, and <a href=\"https://radar.cloudflare.com/as58224\"><u>TCI (AS58224)</u></a>.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/17V0uq3ZHwSClk1NbMsfXj/6d02a99d67224ebe72d03c2f9aa537c8/BLOG-3110_5.png\" />\n          </figure>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5FZOw6Vf0zyZcFReGTDgbJ/9fbd98a696d895b10ce76e3831a258ff/BLOG-3110_6.png\" />\n          </figure><p>Around 18:45 UTC, Internet traffic from Iran <a href=\"https://x.com/CloudflareRadar/status/2009354299302908143\"><u>dropped to effectively zero</u></a>, signaling a complete shutdown in the country and disconnection from the global Internet.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4h1G0brqaDqQP4LroOpmu8/6a74b8803bbd251375a6d65732f1be56/BLOG-3110_7.png\" />\n          </figure>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5RY1SofUechQRgKU81yEt9/6ce900d5165a848701306cc34c964817/BLOG-3110_8.png\" />\n          </figure>\n    <div>\n      <h2>Brief windows of connectivity on January 9 — but they don’t last</h2>\n      <a href=\"#brief-windows-of-connectivity-on-january-9-but-they-dont-last\">\n        \n      </a>\n    </div>\n    <p>After the shutdown took hold the previous day, internal traffic data showed an <a href=\"https://x.com/CloudflareRadar/status/2009758510909665404\"><u>extremely low volume of traffic from Iran</u></a>, amounting to less than 0.01% of pre-shutdown peaks, starting around 10:00 UTC (13:30 local time) on January 9. It appears that access to Cloudflare’s public DNS resolver, <a href=\"https://1.1.1.1/dns\"><u>1.1.1.1</u></a>, also became available again around 10:00 UTC (13:30 local time), leading request traffic to briefly spike well above the expected range. However, after spiking, only a small amount of request traffic to 1.1.1.1 remained visible.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1q43txqeO1Y5jfDtaHHvIN/0108bd24f598bbf8b89b4485d9ec58b6/BLOG-3110_9.png\" />\n          </figure><p>Several Iranian universities also saw <a href=\"https://x.com/CloudflareRadar/status/2009630454383870417?s=20\"><u>connectivity briefly restored</u></a>, starting around 11:30 UTC (15:00 local time). These included <a href=\"https://radar.cloudflare.com/as29068\"><u>University of Tehran Informatics Center (AS29068)</u></a>, <a href=\"https://radar.cloudflare.com/as12660\"><u>Sharif University of Technology (AS12660)</u></a>, <a href=\"https://radar.cloudflare.com/as43965\"><u>Tehran University of Medical Science (AS43965)</u></a>, and <a href=\"https://radar.cloudflare.com/as57745\"><u>Tarbita Modares University (AS57745)</u></a>. It is unclear whether this restoration was intentional, but traffic from these networks was once again non-existent after 15:00 UTC (18:30 local time).</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/pcP3V6y2nhRnH7swgSnZb/bd1e5601cb208046324607f792c51d2a/BLOG-3110_10.png\" />\n          </figure>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6XEwTXqeAu6O1StFLmeGLe/20ccec04c3441eb10117535c92424c30/BLOG-3110_11.png\" />\n          </figure>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5ACIe46mGSsNGrcKDRxfQr/8deb5247b24fa9a5b12891dcacae7e27/BLOG-3110_12.png\" />\n          </figure>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3vZpp0VGcx4He4oNyCLOSK/423f26e3fcdd309dae8a02acad990406/BLOG-3110_13.png\" />\n          </figure>\n    <div>\n      <h2>Changes in HTTP traffic preceded the Internet shutdown</h2>\n      <a href=\"#changes-in-http-traffic-preceded-the-internet-shutdown\">\n        \n      </a>\n    </div>\n    <p>Alongside the lower traffic levels observed at the start of the year, as discussed above, a clear shift in HTTP version usage from human-generated traffic was also observed across leading network providers, as seen in the graphs below. Prior to that point, as much as 40% of HTTP requests on <a href=\"https://radar.cloudflare.com/as44244\"><u>IranCell (AS44244)</u></a> used HTTP/3, but that figure fell to just 5% at 20:00 UTC (23:30 local time) on December 31, and continued to decline over the following days. Usage of QUIC from the network followed a similar pattern, as it relies on HTTP/3. </p><p>On <a href=\"https://radar.cloudflare.com/as58224\"><u>TCI (AS58224)</u></a>, HTTP/3 also accounted for as much as 40% of requests at peak, but gradually declined starting on January 1 before falling below 5% starting around 07:00 UTC (10:30 local time) on January 3. QUIC usage on this network followed a similar pattern as well. MahsaNet, an organization that fights against Internet censorship in Iran, <a href=\"https://x.com/mahsanet/status/2007491214405140716?s=20\"><u>suggested</u></a> that these shifts could indicate that “Severe filtering and layered, upgraded whitelisting are clearly evident and being implemented” (translation via X). </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2aF6ntL7YtxVJDEibF0tPh/184f99765daf533480f1b9791774a59f/BLOG-3110_15.png\" />\n          </figure>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5LsVTrJf0kJfhUR0kgu114/9d71caca08bf4aa39979e4388760d158/BLOG-3110_16.png\" />\n          </figure>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4XyJjijsAwE8M7c9xwqI0Y/88b30b6d306e03d168710b21f90076c6/BLOG-3110_17.png\" />\n          </figure>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5Wq1neIAWfcz5P7GP6u5VF/ceeb800a75a1419f2c6de2aa3d9ae1ce/BLOG-3110_18.png\" />\n          </figure>\n    <div>\n      <h2>The shutdown continues</h2>\n      <a href=\"#the-shutdown-continues\">\n        \n      </a>\n    </div>\n    <p>As we noted in social media posts (<a href=\"https://x.com/cloudflareradar\"><u>X</u></a>, <a href=\"https://noc.social/@cloudflareradar\"><u>Mastodon</u></a>, <a href=\"https://bsky.app/profile/radar.cloudflare.com\"><u>Bluesky</u></a>), no significant changes have been observed in Iran’s Internet traffic since January 10. <b>The country remains almost entirely cut off from the global Internet, with internal data showing traffic volumes remaining at a fraction of a percent of previous levels.</b></p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6EY7TALUsSytiMgsei2y2h/0aaf1473dfc1a0be908beee6f0b14e72/BLOG-3110_19.png\" />\n          </figure>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3JXW8ND7fQCbbPj5vexBJH/f194dfd409c343691915dc214917d402/BLOG-3110_20.png\" />\n          </figure><p>We will continue to monitor the state of Internet connectivity in Iran, and will continue to post updates on our social media accounts. Use Cloudflare Radar’s <a href=\"https://radar.cloudflare.com/traffic/ir\"><u>Traffic</u></a> and <a href=\"https://radar.cloudflare.com/routing/ir\"><u>Routing</u></a> pages for Iran and the <a href=\"https://radar.cloudflare.com/traffic/ir#autonomous-systems\"><u>top networks</u></a> within the country for near-real time insights into these metrics.</p> ","content:encodedSnippet":"In late December 2025, wide-scale protests erupted across multiple cities in Iran. While these protests were initially fueled by frustration over inflation, food prices, and currency depreciation, they have grown into demonstrations demanding a change in the country’s leadership regime. \nIn the last few days, Internet traffic from Iran has effectively dropped to zero. This is evident in the data available in Cloudflare Radar, as we’ll describe in this post.  \nBackground\nThe Iranian government has a history of cutting off Internet connectivity when such protests take place. In November 2019, protests erupted following the announcement of a significant increase in fuel prices. In response, the Iranian government implemented an Internet shutdown for more than five days. In September 2022, protests and demonstrations erupted across Iran in response to the death in police custody of Mahsa/Zhina Amini, a 22-year-old woman from the Kurdistan Province of Iran. Internet services were disrupted across multiple network providers in the following days.\nAmid the current protests, lower traffic volumes were already observed at the start of the year, indicating potential connectivity issues leading into the more dramatic shutdown that has followed. \nInternet connectivity in Iran plummeted on January 8\nSome traffic anomalies were seen in the first few days of 2026 (described in further detail below), though peak traffic levels recovered by January 5, and exceeded expected levels during the following days.\nHowever, this strong recovery proved to be short-lived. IPv6-related shifts observed on January 8 provided the first indication of the changes to come. At 11:50 UTC (15:20 local time), the amount of IPv6 address space announced by Iranian networks dropped by 98.5%, falling from over 48 million /48s (blocks of 2^80 IPv6 addresses) to just over 737,000 /48s. A drop in announced IP address space (whether IPv6 or IPv4) means that the announcing networks are no longer telling the world how to reach those addresses. A major drop like this one can signal an intentional disruption to Internet connectivity, as there is no longer a path to the clients or servers using those IP addresses.\nThis drop in announced IPv6 address space served to reduce IPv6’s share of human-generated traffic from around 12% to around 2%.\nAs seen in the graph below, this drop in IPv6 traffic stayed at a relatively consistent level for approximately 100 minutes, before falling further just before 13:30 UTC (17:00 local time). This second drop resulted in IPv6 traffic from Iran all but disappearing.\nSeveral hours later, we observed overall traffic levels from the country begin to decline rapidly. Between 16:30 - 17:00 UTC (20:00 - 20:30 local time), traffic volumes fell nearly 90%, fueled by a loss of traffic from the major Iranian network providers, including MCCI (AS197207), IranCell (AS44244), and TCI (AS58224).\nAround 18:45 UTC, Internet traffic from Iran dropped to effectively zero, signaling a complete shutdown in the country and disconnection from the global Internet.\nBrief windows of connectivity on January 9 — but they don’t last\nAfter the shutdown took hold the previous day, internal traffic data showed an extremely low volume of traffic from Iran, amounting to less than 0.01% of pre-shutdown peaks, starting around 10:00 UTC (13:30 local time) on January 9. It appears that access to Cloudflare’s public DNS resolver, 1.1.1.1, also became available again around 10:00 UTC (13:30 local time), leading request traffic to briefly spike well above the expected range. However, after spiking, only a small amount of request traffic to 1.1.1.1 remained visible.\nSeveral Iranian universities also saw connectivity briefly restored, starting around 11:30 UTC (15:00 local time). These included University of Tehran Informatics Center (AS29068), Sharif University of Technology (AS12660), Tehran University of Medical Science (AS43965), and Tarbita Modares University (AS57745). It is unclear whether this restoration was intentional, but traffic from these networks was once again non-existent after 15:00 UTC (18:30 local time).\nChanges in HTTP traffic preceded the Internet shutdown\nAlongside the lower traffic levels observed at the start of the year, as discussed above, a clear shift in HTTP version usage from human-generated traffic was also observed across leading network providers, as seen in the graphs below. Prior to that point, as much as 40% of HTTP requests on IranCell (AS44244) used HTTP/3, but that figure fell to just 5% at 20:00 UTC (23:30 local time) on December 31, and continued to decline over the following days. Usage of QUIC from the network followed a similar pattern, as it relies on HTTP/3. \nOn TCI (AS58224), HTTP/3 also accounted for as much as 40% of requests at peak, but gradually declined starting on January 1 before falling below 5% starting around 07:00 UTC (10:30 local time) on January 3. QUIC usage on this network followed a similar pattern as well. MahsaNet, an organization that fights against Internet censorship in Iran, suggested that these shifts could indicate that “Severe filtering and layered, upgraded whitelisting are clearly evident and being implemented” (translation via X). \nThe shutdown continues\nAs we noted in social media posts (X, Mastodon, Bluesky), no significant changes have been observed in Iran’s Internet traffic since January 10. The country remains almost entirely cut off from the global Internet, with internal data showing traffic volumes remaining at a fraction of a percent of previous levels.\nWe will continue to monitor the state of Internet connectivity in Iran, and will continue to post updates on our social media accounts. Use Cloudflare Radar’s Traffic and Routing pages for Iran and the top networks within the country for near-real time insights into these metrics.","dc:creator":"David Belson","content":" Cloudflare Radar data shows Internet traffic from Iran has effectively dropped to zero since January 8, signaling a complete shutdown in the country and disconnection from the global Internet.  \n ","contentSnippet":"Cloudflare Radar data shows Internet traffic from Iran has effectively dropped to zero since January 8, signaling a complete shutdown in the country and disconnection from the global Internet.","guid":"5VetKk31z2RaNY9CI4lsye","categories":["Internet Shutdown","Radar","Internet Traffic","1.1.1.1"],"isoDate":"2026-01-13T00:00:00.000Z"},{"creator":"Bryton Herdes","title":"A closer look at a BGP anomaly in Venezuela","link":"https://blog.cloudflare.com/bgp-route-leak-venezuela/","pubDate":"Tue, 06 Jan 2026 08:00:00 GMT","content:encoded":" <p>As news unfolds surrounding the U.S. capture and arrest of Venezuelan leader Nicolás Maduro, a <a href=\"https://loworbitsecurity.com/radar/radar16/?cf_target_id=8EBD08FC8E3F122A23413E8273CF4AF3\"><u>cybersecurity newsletter</u></a> examined <a href=\"https://radar.cloudflare.com/\"><u>Cloudflare Radar</u></a> data and took note of a routing leak in Venezuela on January 2.</p><p>We dug into the data. Since the beginning of December there have been eleven route leak events, impacting multiple prefixes, where AS8048 is the leaker. Although it is impossible to determine definitively what happened on the day of the event, this pattern of route leaks suggests that the CANTV (AS8048) network, a popular Internet Service Provider (ISP) in Venezuela, has insufficient routing export and import policies. In other words, the BGP anomalies observed by the researcher could be tied to poor technical practices by the ISP rather than malfeasance.</p><p>In this post, we’ll briefly discuss Border Gateway Protocol (BGP) and BGP route leaks, and then dig into the anomaly observed and what may have happened to cause it. </p>\n    <div>\n      <h3>Background: BGP route leaks</h3>\n      <a href=\"#background-bgp-route-leaks\">\n        \n      </a>\n    </div>\n    <p>First, let’s revisit what a <a href=\"https://blog.cloudflare.com/route-leak-detection-with-cloudflare-radar/\"><u>BGP route leak</u></a> is. BGP route leaks cause behavior similar to taking the wrong exit off of a highway. While you may still make it to your destination, the path may be slower and come with delays you wouldn’t otherwise have traveling on a more direct route.</p><p>Route leaks were given a formal definition in <a href=\"https://datatracker.ietf.org/doc/html/rfc7908\"><u>RFC7908</u></a> as “the propagation of routing announcement(s) beyond their intended scope.” Intended scope is defined using <a href=\"https://en.wikipedia.org/wiki/Pairwise\"><u>pairwise</u></a> business relationships between networks. The relationships between networks, which in BGP we represent using <a href=\"https://en.wikipedia.org/wiki/Autonomous_system_(Internet)\"><u>Autonomous Systems (ASes)</u></a>, can be one of the following: </p><ul><li><p>customer-provider: A customer pays a provider network to connect them and their own downstream customers to the rest of the Internet</p></li><li><p>peer-peer: Two networks decide to exchange traffic between one another, to each others’ customers, settlement-free (without payment)</p></li></ul><p>In a customer-provider relationship, the provider will announce <i>all routes to the customer</i>. The customer, on the other hand, will advertise <i>only the routes</i> from their own customers and originating from their network directly.</p><p>In a peer-peer relationship, each peer will advertise to one another <i>only their own routes and the routes of their downstream customers</i>. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/16jXNbH5R5Q4evGm4oRY5p/08d0474000111923f37a7e53b809b5c2/BLOG-3107_2.png\" />\n          </figure><p>These advertisements help direct traffic in expected ways: from customers upstream to provider networks, potentially across a single peering link, and then potentially back down to customers on the far end of the path from their providers. </p><p>A valid path would look like the following that abides by the <a href=\"https://ieeexplore.ieee.org/document/6363987\"><u>valley-free routing</u></a> rule: </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3qKtxTWTrGcpMm8u3nAjYT/dd19181418076c0e12b6035154639e75/BLOG-3107_3.png\" />\n          </figure><p>A <b>route leak</b> is a violation of valley-free routing where an AS takes routes from a provider or peer and redistributes them to another provider or peer. For example, a BGP path should never go through a “valley” where traffic goes up to a provider, and back down to a customer, and then up to a provider again. There are different types of route leaks defined in RFC7908, but a simple one is the Type 1: Hairpin route leak between two provider networks by a customer. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4XdHugzuQAVhucuninsUfB/43912258386debc0500e3ceb7c8abab2/BLOG-3107_4.png\" />\n          </figure><p>In the figure above, AS64505 takes routes from one of its providers and redistributes them to their other provider. This is unexpected, since we know providers should not use their customer as an intermediate IP transit network. AS64505 would become overwhelmed with traffic, as a smaller network with a smaller set of backbone and network links than its providers. This can become very impactful quickly. </p>\n    <div>\n      <h3>Route leak by AS8048 (CANTV)</h3>\n      <a href=\"#route-leak-by-as8048-cantv\">\n        \n      </a>\n    </div>\n    <p>Now that we have reminded ourselves what a route leak is in BGP, let’s examine what was hypothesized  in <a href=\"https://loworbitsecurity.com/radar/radar16/?cf_history_state=%7B%22guid%22%3A%22C255D9FF78CD46CDA4F76812EA68C350%22%2C%22historyId%22%3A106%2C%22targetId%22%3A%2251107FD345D9B86C319316904C23F966%22%7D\"><u>the newsletter post</u></a>. The post called attention to a <a href=\"https://radar.cloudflare.com/routing/as8048#bgp-route-leaks\"><u>few route leak anomalies</u></a> on Cloudflare Radar involving AS8048. On the <a href=\"https://radar.cloudflare.com/routing/anomalies/leak-462460\"><u>Radar page</u></a> for this leak, we see this information:</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7d1gGdOtSEvPxciyfNswhw/619965acdc1a7a4d3eafbd99b0ccb9f3/BLOG-3107_5.png\" />\n          </figure><p>We see the leaker AS, which is AS8048 — CANTV, Venezuela’s state-run telephone and Internet Service Provider. We observe that routes were taken from one of their providers AS6762 (Sparkle, an Italian telecom company) and then redistributed to AS52320 (V.tal GlobeNet, a Colombian network service provider). This is definitely a route leak. </p><p>The newsletter suggests “BGP shenanigans” and posits that such a leak could be exploited to collect intelligence useful to government entities. </p><p>While we can’t say with certainty what caused this route leak, our data suggests that its likely cause was more mundane. That’s in part because BGP route leaks happen all of the time, and they have always been part of the Internet — most often for reasons that aren’t malicious.</p><p>To understand more, let’s look closer at the impacted prefixes and networks. The prefixes involved in the leak were all originated by AS21980 (Dayco Telecom, a Venezuelan company):</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/42cmmWdskKqGw7bByd3tGs/c3841ffa205b241798593b91194d25b1/BLOG-3107_6.png\" />\n          </figure><p>The prefixes are also all members of the same 200.74.224.0/20 <a href=\"https://www.cloudflare.com/learning/network-layer/what-is-a-subnet/\"><u>subnet</u></a>, as noted by the newsletter author. Much more intriguing than this, though, is the relationship between the originating network AS21980 and the leaking network AS8048: AS8048 is a <i>provider</i> of AS21980. </p><p>The customer-provider relationship between AS8048 and AS21980 is visible in both <a href=\"https://radar.cloudflare.com/routing/as21980#connectivity\"><u>Cloudflare Radar</u></a> and <a href=\"https://bgp.tools/as/21980#upstreams\"><u>bgp.tools</u></a> AS relationship interference data. We can also get a confidence score of the AS relationship using the monocle tool from <a href=\"https://bgpkit.com/\"><u>BGPKIT</u></a>, as you see here: </p><p><code>➜  ~ monocle as2rel 8048 21980\nExplanation:\n- connected: % of 1813 peers that see this AS relationship\n- peer: % where the relationship is peer-to-peer\n- as1_upstream: % where ASN1 is the upstream (provider)\n- as2_upstream: % where ASN2 is the upstream (provider)</code></p><p><code>Data source: https://data.bgpkit.com/as2rel/as2rel-latest.json.bz2</code></p><p><code>╭──────┬───────┬───────────┬──────┬──────────────┬──────────────╮\n│ asn1 │ asn2  │ connected │ peer │ as1_upstream │ as2_upstream │\n├──────┼───────┼───────────┼──────┼──────────────┼──────────────┤\n│ 8048 │ 21980 │    9.9%   │ 0.6% │     9.4%     │ 0.0%         │\n╰──────┴───────┴───────────┴──────┴──────────────┴──────────────╯</code></p><p>While only 9.9% of route collectors see these two ASes as adjacent, almost all of the paths containing them reflect AS8048 as an upstream provider for AS21980, meaning confidence is high in the provider-customer relationship between the two.</p><p>Many of the leaked routes were also heavily prepended with AS8048, meaning it would have been <a href=\"https://blog.cloudflare.com/prepends-considered-harmful/\"><u>potentially</u></a> <i>less</i> attractive for routing when received by other networks. <b>Prepending</b> is the padding of an AS more than one time in an outbound advertisement by a customer or peer, to attempt to switch traffic away from a particular circuit to another. For example, many of the paths during the leak by AS8048 looked like this: “52320,8048,8048,8048,8048,8048,8048,8048,8048,8048,23520,1299,269832,21980”. </p><p>You can see that AS8048 has sent their AS multiple times in an advertisement to AS52320, because by means of BGP loop prevention the path would never actually travel in and out of AS8048 multiple times in a row. A non-prepended path would look like this: “52320,8048,23520,1299,269832,21980”. </p><p>If AS8048 was intentionally trying to become a <a href=\"https://en.wikipedia.org/wiki/Man-in-the-middle_attack\"><u>man-in-the-middle (MITM)</u></a> for traffic, why would they make the BGP advertisement less attractive instead of <i>more </i>attractive? Also, why leak prefixes to try and MITM traffic when you’re <i>already</i> a provider for the downstream AS anyway? That wouldn’t make much sense. </p><p>The leaks from AS8048 also surfaced in multiple separate announcements, each around an hour apart on January 2, 2026 between 15:30 and 17:45 UTC, suggesting they may have been having network issues that surfaced in a routing policy issue or a convergence-based mishap. </p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4LrT6YC3j7V9p6ab0OYEmA/cebeba76857f7976d8dd4912371c1c43/BLOG-3107_7.png\" />\n          </figure><p>It is also noteworthy that these leak events begin over twelve hours prior to the <a href=\"https://www.nytimes.com/2026/01/03/world/americas/venezuela-maduro-capture-trump.html\"><u>U.S. military strikes in Venezuela</u></a>. Leaks that impact South American networks <a href=\"https://radar.cloudflare.com/routing/br#routing-anomalies\"><u>are common</u></a>, and we have no reason to believe, based on timing or the other factors I have discussed, that the leak is related to the capture of Maduro several hours later.</p><p>In fact, looking back the past two months, we can see plenty of leaks by AS8048 that are just like this one, meaning this is not a new BGP anomaly:</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2Md8BdHtP1GIDkyNT8PTbD/3915068dc6f47046665410b665e29853/BLOG-3107_8.png\" />\n          </figure><p>You can see above in the history of Cloudflare Radar’s route leak alerting pipeline that AS8048 is no stranger to Type 1 hairpin route leaks. Since the beginning of December alone there have been <b>eleven route leak events</b> where AS8048 is the leaker.</p><p>From this we can draw a more innocent possible explanation about the route leak: AS8048 may have configured too loose of export policies facing at least one of their providers, AS52320. And because of that, redistributed routes belong to their customer even when the direct customer BGP routes were missing. If their export policy toward AS52320 only matched on <a href=\"https://blog.cloudflare.com/monitoring-as-sets-and-why-they-matter/\"><u>IRR-generated</u></a> prefix list and not a <i>customer</i> BGP <a href=\"https://datatracker.ietf.org/doc/html/rfc1997\"><u>community</u></a> tag, for example, it would make sense why an indirect path toward AS6762 was leaked back upstream by AS8048. </p><p>These types of policy errors are something <a href=\"https://datatracker.ietf.org/doc/rfc9234/\"><u>RFC9234</u></a> and the Only-to-Customer (OTC) attribute would help with considerably, by coupling BGP more tightly to customer-provider and peer-peer roles, when supported <a href=\"https://blog.apnic.net/2025/09/05/preventing-route-leaks-made-simple-bgp-roleplay-with-junos-rfc-9234/\"><u>by all routing vendors</u></a>. I will save the more technical details on RFC9234 for a follow-up blog post.</p>\n    <div>\n      <h3>The difference between origin and path validation</h3>\n      <a href=\"#the-difference-between-origin-and-path-validation\">\n        \n      </a>\n    </div>\n    <p>The newsletter also calls out as “notable” that Sparkle (AS6762) does not implement <a href=\"https://rpki.cloudflare.com/\"><u>RPKI (Resource Public Key Infrastructure)</u></a> Route Origin Validation (ROV). While it is true that AS6762 appears to have an <a href=\"https://stats.labs.apnic.net/rpki/AS6762?a=6762&amp;c=IT&amp;ll=1&amp;ss=0&amp;mm=1&amp;vv=1&amp;w=7&amp;v=0\"><u>incomplete deployment</u></a> of ROV and is flagged as “unsafe” on <a href=\"http://isbgpsafeyet.com\"><u>isbgpsafeyet.com</u></a> <a href=\"https://isbgpsafeyet.com/#faq\"><u>because of it</u></a>, origin validation would not have prevented this BGP anomaly in Venezuela. </p><p>It is important to separate BGP anomalies into two categories: route misoriginations, and path-based anomalies. Knowing the difference between the two helps to understand the solution for each. Route misoriginations, often called BGP hijacks, are meant to be fixed by RPKI Route Origin Validation (ROV) by making sure the originator of a prefix is who rightfully owns it. In the case of the BGP anomaly described in this post, the origin AS was correct as AS21980 and <b>only</b> the path was anomalous. This means ROV wouldn’t help here.</p><p>Knowing that, we need path-based validation. This is what <a href=\"https://datatracker.ietf.org/doc/draft-ietf-sidrops-aspa-verification/\"><u>Autonomous System Provider Authorization (ASPA)</u></a>, an upcoming draft standard in the IETF, is going to provide. The idea is similar to RPKI Route Origin Authorizations (ROAs) and ROV: create an ASPA object that defines a list of authorized providers (upstreams) for our AS, and everyone will use this to invalidate route leaks on the Internet at various vantage points. Using a concrete example, AS6762 is a <a href=\"https://en.wikipedia.org/wiki/Tier_1_network\"><u>Tier-1</u></a> transit-free network, and they would use the special reserved “AS0” member in their ASPA signed object to communicate to the world that they have no upstream providers, only lateral peers and customers. Then, AS52320, the other provider of AS8048, would see routes from their customer with “6762” in the path and reject them by performing an ASPA verification process.</p><p>ASPA is based on RPKI and is exactly what would help prevent route leaks similar to the one we observed in Venezuela.</p>\n    <div>\n      <h3>A safer BGP, built together </h3>\n      <a href=\"#a-safer-bgp-built-together\">\n        \n      </a>\n    </div>\n    <p>We felt it was important to offer an alternative explanation for the BGP route leak by AS8048 in Venezuela that was observed on Cloudflare Radar. It is helpful to understand that route leaks are an expected side effect of BGP historically being based entirely on trust and carefully-executed business relationship-driven intent. </p><p>While route leaks could be done with malicious intent, the data suggests this event may have been an accident caused by a lack of routing export and import policies that would prevent it. This is why to have a safer BGP and Internet we need to work together and drive adoption of RPKI-based ASPA, for which <a href=\"https://www.ripe.net/manage-ips-and-asns/resource-management/rpki/aspa/\"><u>RIPE recently released object creation</u></a>, on the wide Internet. It will be a collaborative effort, just like RPKI has been for origin validation, but it will be worth it and prevent BGP incidents such as the one in Venezuela. </p><p>In addition to ASPA, we can all implement simpler mechanisms such as <a href=\"https://github.com/job/peerlock\"><u>Peerlock</u></a> and <a href=\"https://archive.nanog.org/sites/default/files/Snijders_Everyday_Practical_Bgp.pdf\"><u>Peerlock-lite</u></a> as operators, which sanity-checks received paths for obvious leaks. One especially promising initiative is the adoption of <a href=\"https://datatracker.ietf.org/doc/rfc9234/\"><u>RFC9234</u></a>, which should be used in addition to ASPA for preventing route leaks with the establishing of BGP roles and a new Only-To-Customer (OTC) attribute. If you haven’t already asked your routing vendors for an implementation of RFC9234 to be on their roadmap: <i>please</i> <i>do</i>. You can help make a big difference.</p><p><i>Update: Sparkle (AS6762) finished RPKI ROV deployment and </i><a href=\"https://github.com/cloudflare/isbgpsafeyet.com/pull/829\"><i><u>was marked safe</u></i></a><i> on February 3, 2026.</i></p> ","content:encodedSnippet":"As news unfolds surrounding the U.S. capture and arrest of Venezuelan leader Nicolás Maduro, a cybersecurity newsletter examined Cloudflare Radar data and took note of a routing leak in Venezuela on January 2.\nWe dug into the data. Since the beginning of December there have been eleven route leak events, impacting multiple prefixes, where AS8048 is the leaker. Although it is impossible to determine definitively what happened on the day of the event, this pattern of route leaks suggests that the CANTV (AS8048) network, a popular Internet Service Provider (ISP) in Venezuela, has insufficient routing export and import policies. In other words, the BGP anomalies observed by the researcher could be tied to poor technical practices by the ISP rather than malfeasance.\nIn this post, we’ll briefly discuss Border Gateway Protocol (BGP) and BGP route leaks, and then dig into the anomaly observed and what may have happened to cause it. \nBackground: BGP route leaks\nFirst, let’s revisit what a BGP route leak is. BGP route leaks cause behavior similar to taking the wrong exit off of a highway. While you may still make it to your destination, the path may be slower and come with delays you wouldn’t otherwise have traveling on a more direct route.\nRoute leaks were given a formal definition in RFC7908 as “the propagation of routing announcement(s) beyond their intended scope.” Intended scope is defined using pairwise business relationships between networks. The relationships between networks, which in BGP we represent using Autonomous Systems (ASes), can be one of the following: \n\ncustomer-provider: A customer pays a provider network to connect them and their own downstream customers to the rest of the Internet\n\npeer-peer: Two networks decide to exchange traffic between one another, to each others’ customers, settlement-free (without payment)\n\nIn a customer-provider relationship, the provider will announce all routes to the customer. The customer, on the other hand, will advertise only the routes from their own customers and originating from their network directly.\nIn a peer-peer relationship, each peer will advertise to one another only their own routes and the routes of their downstream customers. \nThese advertisements help direct traffic in expected ways: from customers upstream to provider networks, potentially across a single peering link, and then potentially back down to customers on the far end of the path from their providers. \nA valid path would look like the following that abides by the valley-free routing rule: \nA route leak is a violation of valley-free routing where an AS takes routes from a provider or peer and redistributes them to another provider or peer. For example, a BGP path should never go through a “valley” where traffic goes up to a provider, and back down to a customer, and then up to a provider again. There are different types of route leaks defined in RFC7908, but a simple one is the Type 1: Hairpin route leak between two provider networks by a customer. \nIn the figure above, AS64505 takes routes from one of its providers and redistributes them to their other provider. This is unexpected, since we know providers should not use their customer as an intermediate IP transit network. AS64505 would become overwhelmed with traffic, as a smaller network with a smaller set of backbone and network links than its providers. This can become very impactful quickly. \nRoute leak by AS8048 (CANTV)\nNow that we have reminded ourselves what a route leak is in BGP, let’s examine what was hypothesized  in the newsletter post. The post called attention to a few route leak anomalies on Cloudflare Radar involving AS8048. On the Radar page for this leak, we see this information:\nWe see the leaker AS, which is AS8048 — CANTV, Venezuela’s state-run telephone and Internet Service Provider. We observe that routes were taken from one of their providers AS6762 (Sparkle, an Italian telecom company) and then redistributed to AS52320 (V.tal GlobeNet, a Colombian network service provider). This is definitely a route leak. \nThe newsletter suggests “BGP shenanigans” and posits that such a leak could be exploited to collect intelligence useful to government entities. \nWhile we can’t say with certainty what caused this route leak, our data suggests that its likely cause was more mundane. That’s in part because BGP route leaks happen all of the time, and they have always been part of the Internet — most often for reasons that aren’t malicious.\nTo understand more, let’s look closer at the impacted prefixes and networks. The prefixes involved in the leak were all originated by AS21980 (Dayco Telecom, a Venezuelan company):\nThe prefixes are also all members of the same 200.74.224.0/20 subnet, as noted by the newsletter author. Much more intriguing than this, though, is the relationship between the originating network AS21980 and the leaking network AS8048: AS8048 is a provider of AS21980. \nThe customer-provider relationship between AS8048 and AS21980 is visible in both Cloudflare Radar and bgp.tools AS relationship interference data. We can also get a confidence score of the AS relationship using the monocle tool from BGPKIT, as you see here: \n➜  ~ monocle as2rel 8048 21980\nExplanation:\n- connected: % of 1813 peers that see this AS relationship\n- peer: % where the relationship is peer-to-peer\n- as1_upstream: % where ASN1 is the upstream (provider)\n- as2_upstream: % where ASN2 is the upstream (provider)\nData source: https://data.bgpkit.com/as2rel/as2rel-latest.json.bz2\n╭──────┬───────┬───────────┬──────┬──────────────┬──────────────╮\n│ asn1 │ asn2  │ connected │ peer │ as1_upstream │ as2_upstream │\n├──────┼───────┼───────────┼──────┼──────────────┼──────────────┤\n│ 8048 │ 21980 │    9.9%   │ 0.6% │     9.4%     │ 0.0%         │\n╰──────┴───────┴───────────┴──────┴──────────────┴──────────────╯\nWhile only 9.9% of route collectors see these two ASes as adjacent, almost all of the paths containing them reflect AS8048 as an upstream provider for AS21980, meaning confidence is high in the provider-customer relationship between the two.\nMany of the leaked routes were also heavily prepended with AS8048, meaning it would have been potentially less attractive for routing when received by other networks. Prepending is the padding of an AS more than one time in an outbound advertisement by a customer or peer, to attempt to switch traffic away from a particular circuit to another. For example, many of the paths during the leak by AS8048 looked like this: “52320,8048,8048,8048,8048,8048,8048,8048,8048,8048,23520,1299,269832,21980”. \nYou can see that AS8048 has sent their AS multiple times in an advertisement to AS52320, because by means of BGP loop prevention the path would never actually travel in and out of AS8048 multiple times in a row. A non-prepended path would look like this: “52320,8048,23520,1299,269832,21980”. \nIf AS8048 was intentionally trying to become a man-in-the-middle (MITM) for traffic, why would they make the BGP advertisement less attractive instead of more attractive? Also, why leak prefixes to try and MITM traffic when you’re already a provider for the downstream AS anyway? That wouldn’t make much sense. \nThe leaks from AS8048 also surfaced in multiple separate announcements, each around an hour apart on January 2, 2026 between 15:30 and 17:45 UTC, suggesting they may have been having network issues that surfaced in a routing policy issue or a convergence-based mishap. \nIt is also noteworthy that these leak events begin over twelve hours prior to the U.S. military strikes in Venezuela. Leaks that impact South American networks are common, and we have no reason to believe, based on timing or the other factors I have discussed, that the leak is related to the capture of Maduro several hours later.\nIn fact, looking back the past two months, we can see plenty of leaks by AS8048 that are just like this one, meaning this is not a new BGP anomaly:\nYou can see above in the history of Cloudflare Radar’s route leak alerting pipeline that AS8048 is no stranger to Type 1 hairpin route leaks. Since the beginning of December alone there have been eleven route leak events where AS8048 is the leaker.\nFrom this we can draw a more innocent possible explanation about the route leak: AS8048 may have configured too loose of export policies facing at least one of their providers, AS52320. And because of that, redistributed routes belong to their customer even when the direct customer BGP routes were missing. If their export policy toward AS52320 only matched on IRR-generated prefix list and not a customer BGP community tag, for example, it would make sense why an indirect path toward AS6762 was leaked back upstream by AS8048. \nThese types of policy errors are something RFC9234 and the Only-to-Customer (OTC) attribute would help with considerably, by coupling BGP more tightly to customer-provider and peer-peer roles, when supported by all routing vendors. I will save the more technical details on RFC9234 for a follow-up blog post.\nThe difference between origin and path validation\nThe newsletter also calls out as “notable” that Sparkle (AS6762) does not implement RPKI (Resource Public Key Infrastructure) Route Origin Validation (ROV). While it is true that AS6762 appears to have an incomplete deployment of ROV and is flagged as “unsafe” on isbgpsafeyet.com because of it, origin validation would not have prevented this BGP anomaly in Venezuela. \nIt is important to separate BGP anomalies into two categories: route misoriginations, and path-based anomalies. Knowing the difference between the two helps to understand the solution for each. Route misoriginations, often called BGP hijacks, are meant to be fixed by RPKI Route Origin Validation (ROV) by making sure the originator of a prefix is who rightfully owns it. In the case of the BGP anomaly described in this post, the origin AS was correct as AS21980 and only the path was anomalous. This means ROV wouldn’t help here.\nKnowing that, we need path-based validation. This is what Autonomous System Provider Authorization (ASPA), an upcoming draft standard in the IETF, is going to provide. The idea is similar to RPKI Route Origin Authorizations (ROAs) and ROV: create an ASPA object that defines a list of authorized providers (upstreams) for our AS, and everyone will use this to invalidate route leaks on the Internet at various vantage points. Using a concrete example, AS6762 is a Tier-1 transit-free network, and they would use the special reserved “AS0” member in their ASPA signed object to communicate to the world that they have no upstream providers, only lateral peers and customers. Then, AS52320, the other provider of AS8048, would see routes from their customer with “6762” in the path and reject them by performing an ASPA verification process.\nASPA is based on RPKI and is exactly what would help prevent route leaks similar to the one we observed in Venezuela.\nA safer BGP, built together \nWe felt it was important to offer an alternative explanation for the BGP route leak by AS8048 in Venezuela that was observed on Cloudflare Radar. It is helpful to understand that route leaks are an expected side effect of BGP historically being based entirely on trust and carefully-executed business relationship-driven intent. \nWhile route leaks could be done with malicious intent, the data suggests this event may have been an accident caused by a lack of routing export and import policies that would prevent it. This is why to have a safer BGP and Internet we need to work together and drive adoption of RPKI-based ASPA, for which RIPE recently released object creation, on the wide Internet. It will be a collaborative effort, just like RPKI has been for origin validation, but it will be worth it and prevent BGP incidents such as the one in Venezuela. \nIn addition to ASPA, we can all implement simpler mechanisms such as Peerlock and Peerlock-lite as operators, which sanity-checks received paths for obvious leaks. One especially promising initiative is the adoption of RFC9234, which should be used in addition to ASPA for preventing route leaks with the establishing of BGP roles and a new Only-To-Customer (OTC) attribute. If you haven’t already asked your routing vendors for an implementation of RFC9234 to be on their roadmap: please do. You can help make a big difference.\nUpdate: Sparkle (AS6762) finished RPKI ROV deployment and was marked safe on February 3, 2026.","dc:creator":"Bryton Herdes","content":" There has been speculation about the cause of a BGP anomaly observed in Venezuela on January 2. We take a look at BGP route leaks, and dive into what the data suggests caused the anomaly in question. ","contentSnippet":"There has been speculation about the cause of a BGP anomaly observed in Venezuela on January 2. We take a look at BGP route leaks, and dive into what the data suggests caused the anomaly in question.","guid":"4WOdNrtTGvlrQDV7Apw8R1","categories":["BGP","RPKI","Routing","Routing Security","Network Services"],"isoDate":"2026-01-06T08:00:00.000Z"},{"creator":"Kevin Deems","title":"How Workers powers our internal maintenance scheduling pipeline","link":"https://blog.cloudflare.com/building-our-maintenance-scheduler-on-workers/","pubDate":"Mon, 22 Dec 2025 14:00:00 GMT","content:encoded":" <p>Cloudflare has data centers in over <a href=\"https://www.cloudflare.com/network/\"><u>330 cities globally</u></a>, so you might think we could easily disrupt a few at any time without users noticing when we plan data center operations. However, the reality is that <a href=\"https://developers.cloudflare.com/support/disruptive-maintenance/\"><u>disruptive maintenance</u></a> requires careful planning, and as Cloudflare grew, managing these complexities through manual coordination between our infrastructure and network operations specialists became nearly impossible.</p><p>It is no longer feasible for a human to track every overlapping maintenance request or account for every customer-specific routing rule in real time. We reached a point where manual oversight alone couldn't guarantee that a routine hardware update in one part of the world wouldn't inadvertently conflict with a critical path in another.</p><p>We realized we needed a centralized, automated \"brain\" to act as a safeguard — a system that could see the entire state of our network at once. By building this scheduler on <a href=\"https://workers.cloudflare.com/\"><u>Cloudflare Workers</u></a>, we created a way to programmatically enforce safety constraints, ensuring that no matter how fast we move, we never sacrifice the reliability of the services on which our customers depend.</p><p>In this blog post, we’ll explain how we built it, and share the results we’re seeing now.</p>\n    <div>\n      <h2>Building a system to de-risk critical maintenance operations</h2>\n      <a href=\"#building-a-system-to-de-risk-critical-maintenance-operations\">\n        \n      </a>\n    </div>\n    <p>Picture an edge router that acts as one of a small, redundant group of gateways that collectively connect the public Internet to the many Cloudflare data centers operating in a metro area. In a populated city, we need to ensure that the multiple data centers sitting behind this small cluster of routers do not get cut off because the routers were all taken offline simultaneously. </p><p>Another maintenance challenge comes from our Zero Trust product, Dedicated CDN Egress IPs, which allows customers to choose specific data centers from which their user traffic will exit Cloudflare and be sent to their geographically close origin servers for low latency. (For the purpose of brevity in this post, we'll refer to the Dedicated CDN Egress IPs product as \"Aegis,\" which was its former name.) If all the data centers a customer chose are offline at once, they would see higher latency and possibly 5xx errors, which we must avoid. </p><p>Our maintenance scheduler solves problems like these. We can make sure that we always have at least one edge router active in a certain area. And when scheduling maintenance, we can see if the combination of multiple scheduled events would cause all the data centers for a customer’s Aegis pools to be offline at the same time.</p><p>Before we created the scheduler, these simultaneous disruptive events could cause downtime for customers. Now, our scheduler notifies internal operators of potential conflicts, allowing us to propose a new time to avoid overlapping with other related data center maintenance events.</p><p>We define these operational scenarios, such as edge router availability and customer rules, as maintenance constraints which allow us to plan more predictable and safe maintenance.</p>\n    <div>\n      <h2>Maintenance constraints</h2>\n      <a href=\"#maintenance-constraints\">\n        \n      </a>\n    </div>\n    <p>Every constraint starts with a set of proposed maintenance items, such as a network router or list of servers. We then find all the maintenance events in the calendar that overlap with the proposed maintenance time window.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2vHCauxOGRXzhrO6DNDr2S/cf38b93ac9b812e5e064f800e537e549/image4.png\" />\n          </figure><p>Next, we aggregate product APIs, such as a list of Aegis customer IP pools. Aegis returns a set of IP ranges where a customer requested egress out of specific data center IDs, shown below.</p>\n            <pre><code>[\n    {\n      \"cidr\": \"104.28.0.32/32\",\n      \"pool_name\": \"customer-9876\",\n      \"port_slots\": [\n        {\n          \"dc_id\": 21,\n          \"other_colos_enabled\": true,\n        },\n        {\n          \"dc_id\": 45,\n          \"other_colos_enabled\": true,\n        }\n      ],\n      \"modified_at\": \"2023-10-22T13:32:47.213767Z\"\n    },\n]</code></pre>\n            <p>In this scenario, data center 21 and data center 45 relate to each other because we need at least one data center online for the Aegis customer 9876 to receive egress traffic from Cloudflare. If we tried to take data centers 21 and 45 down simultaneously, our coordinator would alert us that there would be unintended consequences for that customer workload.</p><p>We initially had a naive solution to load all data into a single Worker. This included all server relationships, product configurations, and metrics for product and infrastructure health to compute constraints. Even in our proof of concept phase, we ran into problems with “out of memory” errors.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1v4Q6bXsZLBXLbrbRrcW3o/00d291ef3db459e99ae9b620965b6bc7/image2.png\" />\n          </figure><p>We needed to be more cognizant of Workers’ <a href=\"https://developers.cloudflare.com/workers/platform/limits/\"><u>platform limits</u></a>. This required loading only as much data as was absolutely necessary to process the constraint’s business logic. If a maintenance request for a router in Frankfurt, Germany, comes in, we almost certainly do not care what is happening in Australia since there is no overlap across regions. Thus, we should only load data for neighboring data centers in Germany. We needed a more efficient way to process relationships in our dataset.</p>\n    <div>\n      <h2>Graph processing on Workers</h2>\n      <a href=\"#graph-processing-on-workers\">\n        \n      </a>\n    </div>\n    <p>As we looked at our constraints, a pattern emerged where each constraint boiled down to two concepts: objects and associations. In graph theory, these components are known as vertices and edges, respectively. An object could be a network router and an association could be the list of Aegis pools in the data center that requires the router to be online. We took inspiration from Facebook’s <a href=\"https://research.facebook.com/publications/tao-facebooks-distributed-data-store-for-the-social-graph/\"><u>TAO</u></a> research paper to establish a graph interface on top of our product and infrastructure data. The API looks like the following:</p>\n            <pre><code>type ObjectID = string\n\ninterface MainTAOInterface&lt;TObject, TAssoc, TAssocType&gt; {\n  object_get(id: ObjectID): Promise&lt;TObject | undefined&gt;\n\n  assoc_get(id1: ObjectID, atype: TAssocType): AsyncIterable&lt;TAssoc&gt;\n}</code></pre>\n            <p>The core insight is that associations are typed. For example, a constraint would call the graph interface to retrieve Aegis product data.</p>\n            <pre><code>async function constraint(c: AppContext, aegis: TAOAegisClient, datacenters: string[]): Promise&lt;Record&lt;string, PoolAnalysis&gt;&gt; {\n  const datacenterEntries = await Promise.all(\n    datacenters.map(async (dcID) =&gt; {\n      const iter = aegis.assoc_get(c, dcID, AegisAssocType.DATACENTER_INSIDE_AEGIS_POOL)\n      const pools: string[] = []\n      for await (const assoc of iter) {\n        pools.push(assoc.id2)\n      }\n      return [dcID, pools] as const\n    }),\n  )\n\n  const datacenterToPools = new Map&lt;string, string[]&gt;(datacenterEntries)\n  const uniquePools = new Set&lt;string&gt;()\n  for (const pools of datacenterToPools.values()) {\n    for (const pool of pools) uniquePools.add(pool)\n  }\n\n  const poolTotalsEntries = await Promise.all(\n    [...uniquePools].map(async (pool) =&gt; {\n      const total = aegis.assoc_count(c, pool, AegisAssocType.AEGIS_POOL_CONTAINS_DATACENTER)\n      return [pool, total] as const\n    }),\n  )\n\n  const poolTotals = new Map&lt;string, number&gt;(poolTotalsEntries)\n  const poolAnalysis: Record&lt;string, PoolAnalysis&gt; = {}\n  for (const [dcID, pools] of datacenterToPools.entries()) {\n    for (const pool of pools) {\n      poolAnalysis[pool] = {\n        affectedDatacenters: new Set([dcID]),\n        totalDatacenters: poolTotals.get(pool),\n      }\n    }\n  }\n\n  return poolAnalysis\n}</code></pre>\n            <p>We use two association types in the code above:</p><ol><li><p>DATACENTER_INSIDE_AEGIS_POOL, which retrieves the Aegis customer pools that a data center resides in.</p></li><li><p>AEGIS_POOL_CONTAINS_DATACENTER, which retrieves the data centers an Aegis pool needs to serve traffic.</p></li></ol><p>The associations are inverted indices of one another. The access pattern is exactly the same as before, but now the graph implementation has much more control of how much data it queries. Before, we needed to load all Aegis pools into memory and filter inside constraint business logic. Now, we can directly fetch only the data that matters to the application.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4b68YLIHiOPt5EeyTUTeBt/5f624f0d0912e7dfd0e308a3427d194c/unnamed.png\" />\n          </figure><p>The interface is powerful because our graph implementation can improve performance behind the scenes without complicating the business logic. This lets us use the scalability of Workers and Cloudflare’s CDN to fetch data from our internal systems very quickly.</p>\n    <div>\n      <h2>Fetch pipeline</h2>\n      <a href=\"#fetch-pipeline\">\n        \n      </a>\n    </div>\n    <p>We switched to using the new graph implementation, sending more targeted API requests. Response sizes dropped by 100x overnight, switching from loading a few massive requests to many tiny requests.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/71aDOicyippmUbj4ypXKw/73dacdf16ca0ac422efdfec9e86e9dbf/image5.png\" />\n          </figure><p>While this solves the issue of loading too much into memory, we now have a subrequest problem because instead of a few large HTTP requests, we make an order of magnitude more of small requests. Overnight, we started consistently breaching subrequest limits.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/36KjfOU8xIuUkwF7QOlNkK/e2275a50ff1bef497cdb201c2d3a6249/image3.png\" />\n          </figure><p>In order to solve this problem, we built a smart middleware layer between our graph implementation and the <code>fetch</code> API.</p>\n            <pre><code>export const fetchPipeline = new FetchPipeline()\n  .use(requestDeduplicator())\n  .use(lruCacher({\n    maxItems: 100,\n  }))\n  .use(cdnCacher())\n  .use(backoffRetryer({\n    retries: 3,\n    baseMs: 100,\n    jitter: true,\n  }))\n  .handler(terminalFetch);</code></pre>\n            <p>If you’re familiar with Go, you may have seen the <a href=\"https://pkg.go.dev/golang.org/x/sync/singleflight\"><u>singleflight</u></a> package before. We took inspiration from this idea and the first middleware component in the fetch pipeline deduplicates inflight HTTP requests, so they all wait on the same Promise for data instead of producing duplicate requests in the same Worker. Next, we use a lightweight Least Recently Used (LRU) cache to internally cache requests that we have already seen before.</p><p>Once both of those are complete, we use Cloudflare’s <code>caches.default.match</code> function to cache all GET requests in the region that the Worker is running. Since we have multiple data sources with different performance characteristics, we choose time to live (TTL) values carefully. For example, real-time data is only cached for 1 minute. Relatively static infrastructure data could be cached for 1–24 hours depending on the type of data. Power management data might be changed manually and infrequently, so we can cache it for longer at the edge.</p><p>In addition to those layers, we have the standard exponential backoff, retries and jitter. This helps reduce wasted <code>fetch</code> calls where a downstream resource might be unavailable temporarily. By backing off slightly, we increase the chance that we fetch the next request successfully. Conversely, if the Worker sends requests constantly without backoff, it will easily breach the subrequest limit when the origin starts returning 5xx errors.</p><p>Putting it all together, we saw ~99% cache hit rate. <a href=\"https://www.cloudflare.com/learning/cdn/what-is-a-cache-hit-ratio/\"><u>Cache hit rate</u></a> is the percentage of HTTP requests served from Cloudflare’s fast cache memory (a \"hit\") versus slower requests to data sources running in our control plane (a \"miss\"), calculated as (hits / (hits + misses)). A high rate means better HTTP request performance and lower costs because querying data from cache in our Worker is an order of magnitude faster than fetching from an origin server in a different region. After tuning settings, for our in memory and CDN caches, hit rates have increased dramatically. Since much of our workload is real-time, we will never have a 100% hit rate as we must request fresh data at least once per minute.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1jifI33QpBkQPd7tE5Tapi/186a74b922faac3abe091b79f03d640b/image1.png\" />\n          </figure><p>We have talked about improving the fetching layer, but not about how we made origin HTTP requests faster. Our maintenance coordinator needs to react in real-time to network degradation and failure of machines in data centers. We use our distributed <a href=\"https://blog.cloudflare.com/how-cloudflare-runs-prometheus-at-scale/\"><u>Prometheus</u></a> query engine, Thanos, to deliver performant metrics from the edge into the coordinator.</p>\n    <div>\n      <h2>Thanos in real-time</h2>\n      <a href=\"#thanos-in-real-time\">\n        \n      </a>\n    </div>\n    <p>To explain how our choice in using the graph processing interface affected our real-time queries, let’s walk through an example. In order to analyze the health of edge routers, we could send the following query:</p>\n            <pre><code>sum by (instance) (network_snmp_interface_admin_status{instance=~\"edge.*\"})</code></pre>\n            <p>Originally, we asked our Thanos service, which stores Prometheus metrics, for a list of each edge router’s current health status and would manually filter for routers relevant to the maintenance inside the Worker. This is suboptimal for many reasons. For example, Thanos returned multi-MB responses which it needed to decode and encode. The Worker also needed to cache and decode these large HTTP responses only to filter out the majority of the data while processing a specific maintenance request. Since TypeScript is single-threaded and parsing JSON data is CPU-bound, sending two large HTTP requests means that one is blocked waiting for the other to finish parsing.</p><p>Instead, we simply use the graph to find targeted relationships such as the interface links between edge and spine routers, denoted as <code>EDGE_ROUTER_NETWORK_CONNECTS_TO_SPINE</code>.</p>\n            <pre><code>sum by (lldp_name) (network_snmp_interface_admin_status{instance=~\"edge01.fra03\", lldp_name=~\"spine.*\"})</code></pre>\n            <p>The result is 1 Kb on average instead of multiple MBs, or approximately 1000x smaller. This also massively reduces the amount of CPU required inside the Worker because we offload most of the deserialization to Thanos. As we explained before, this means we need to make a higher number of these smaller fetch requests, but load balancers in front of Thanos can spread the requests evenly to increase throughput for this use case. </p><p>Our graph implementation and fetch pipeline successfully tamed the 'thundering herd' of thousands of tiny real-time requests. However, historical analysis presents a different I/O challenge. Instead of fetching small, specific relationships, we need to scan months of data to find conflicting maintenance windows. In the past, Thanos would issue a massive amount of random reads to our object store, R2. To solve this massive bandwidth penalty without losing performance, we adopted a new approach the Observability team developed internally this year.</p>\n    <div>\n      <h2>Historical data analysis</h2>\n      <a href=\"#historical-data-analysis\">\n        \n      </a>\n    </div>\n    <p>There are enough maintenance use cases that we must rely on historical data to tell us if our solution is accurate and will scale with the growth of Cloudflare’s network. We do not want to cause incidents, and we also want to avoid blocking proposed physical maintenance unnecessarily. In order to balance these two priorities, we can use time series data about maintenance events that happened two months or even a year ago to tell us how often a maintenance event is violating one of our constraints, e.g. edge router availability or Aegis. We blogged earlier this year about using Thanos to <a href=\"https://blog.cloudflare.com/safe-change-at-any-scale/\"><u>automatically release and revert software</u></a> to the edge.</p><p>Thanos primarily fans out to Prometheus, but when Prometheus' retention is not enough to answer the query it has to download data from object storage — R2 in our case. Prometheus TSDB blocks were originally designed for local SSDs, relying on random access patterns that become a bottleneck when moved to object storage. When our scheduler needs to analyze months of historical maintenance data to identify conflicting constraints, random reads from object storage incur a massive I/O penalty. To solve this, we implemented a conversion layer that transforms these blocks into <a href=\"https://parquet.apache.org/\"><u>Apache Parquet</u></a> files. Parquet is a columnar format native to big data analytics that organizes data by column rather than row, which — together with rich statistics — allows us to only fetch what we need.</p><p>Furthermore, since we are rewriting TSDB blocks into Parquet files, we can also store the data in a way that allows us to read the data in just a few big sequential chunks.</p>\n            <pre><code>sum by (instance) (hmd:release_scopes:enabled{dc_id=\"45\"})</code></pre>\n            <p>In the example above we would choose the tuple “(__name__, dc_id)” as a primary sorting key so that metrics with the name “hmd:release_scopes:enabled” and the same value for “dc_id” get sorted close together.</p><p>Our Parquet gateway now issues precise R2 range requests to fetch only the specific columns relevant to the query. This reduces the payload from megabytes to kilobytes. Furthermore, because these file segments are immutable, we can aggressively cache them on the Cloudflare CDN.</p><p>This turns R2 into a low-latency query engine, allowing us to backtest complex maintenance scenarios against long-term trends instantly, avoiding the timeouts and high tail latency we saw with the original TSDB format. The graph below shows a recent load test, where Parquet reached up to 15x the P90 performance compared to the old system for the same query pattern.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6lVj6W4W4MMUy6cEsDpk5G/21614b7ac003a86cb5162a2ba75f4c42/image8.png\" />\n          </figure><p>To get a deeper understanding of how the Parquet implementation works, you can watch this talk at PromCon EU 2025, <a href=\"https://www.youtube.com/watch?v=wDN2w2xN6bA&amp;list=PLoz-W_CUquUlHOg314_YttjHL0iGTdE3O&amp;index=16\"><u>Beyond TSDB: Unlocking Prometheus with Parquet for Modern Scale</u></a>.</p>\n    <div>\n      <h2>Building for scale</h2>\n      <a href=\"#building-for-scale\">\n        \n      </a>\n    </div>\n    <p>By leveraging Cloudflare Workers, we moved from a system that ran out of memory to one that intelligently caches data and uses efficient observability tooling to analyze product and infrastructure data in real time. We built a maintenance scheduler that balances network growth with product performance.</p><p>But “balance” is a moving target.</p><p>Every day, we add more hardware around the world, and the logic required to maintain it without disrupting customer traffic gets exponentially harder with more products and types of maintenance operations. We’ve worked through the first set of challenges, but now we’re staring down more subtle, complex ones that only appear at this massive scale.</p><p>We need engineers who aren't afraid of hard problems. Join our <a href=\"https://www.cloudflare.com/careers/jobs/?department=Infrastructure\"><u>Infrastructure team</u></a> and come build with us.</p> ","content:encodedSnippet":"Cloudflare has data centers in over 330 cities globally, so you might think we could easily disrupt a few at any time without users noticing when we plan data center operations. However, the reality is that disruptive maintenance requires careful planning, and as Cloudflare grew, managing these complexities through manual coordination between our infrastructure and network operations specialists became nearly impossible.\nIt is no longer feasible for a human to track every overlapping maintenance request or account for every customer-specific routing rule in real time. We reached a point where manual oversight alone couldn't guarantee that a routine hardware update in one part of the world wouldn't inadvertently conflict with a critical path in another.\nWe realized we needed a centralized, automated \"brain\" to act as a safeguard — a system that could see the entire state of our network at once. By building this scheduler on Cloudflare Workers, we created a way to programmatically enforce safety constraints, ensuring that no matter how fast we move, we never sacrifice the reliability of the services on which our customers depend.\nIn this blog post, we’ll explain how we built it, and share the results we’re seeing now.\nBuilding a system to de-risk critical maintenance operations\nPicture an edge router that acts as one of a small, redundant group of gateways that collectively connect the public Internet to the many Cloudflare data centers operating in a metro area. In a populated city, we need to ensure that the multiple data centers sitting behind this small cluster of routers do not get cut off because the routers were all taken offline simultaneously. \nAnother maintenance challenge comes from our Zero Trust product, Dedicated CDN Egress IPs, which allows customers to choose specific data centers from which their user traffic will exit Cloudflare and be sent to their geographically close origin servers for low latency. (For the purpose of brevity in this post, we'll refer to the Dedicated CDN Egress IPs product as \"Aegis,\" which was its former name.) If all the data centers a customer chose are offline at once, they would see higher latency and possibly 5xx errors, which we must avoid. \nOur maintenance scheduler solves problems like these. We can make sure that we always have at least one edge router active in a certain area. And when scheduling maintenance, we can see if the combination of multiple scheduled events would cause all the data centers for a customer’s Aegis pools to be offline at the same time.\nBefore we created the scheduler, these simultaneous disruptive events could cause downtime for customers. Now, our scheduler notifies internal operators of potential conflicts, allowing us to propose a new time to avoid overlapping with other related data center maintenance events.\nWe define these operational scenarios, such as edge router availability and customer rules, as maintenance constraints which allow us to plan more predictable and safe maintenance.\nMaintenance constraints\nEvery constraint starts with a set of proposed maintenance items, such as a network router or list of servers. We then find all the maintenance events in the calendar that overlap with the proposed maintenance time window.\nNext, we aggregate product APIs, such as a list of Aegis customer IP pools. Aegis returns a set of IP ranges where a customer requested egress out of specific data center IDs, shown below.\n[\n    {\n      \"cidr\": \"104.28.0.32/32\",\n      \"pool_name\": \"customer-9876\",\n      \"port_slots\": [\n        {\n          \"dc_id\": 21,\n          \"other_colos_enabled\": true,\n        },\n        {\n          \"dc_id\": 45,\n          \"other_colos_enabled\": true,\n        }\n      ],\n      \"modified_at\": \"2023-10-22T13:32:47.213767Z\"\n    },\n]\nIn this scenario, data center 21 and data center 45 relate to each other because we need at least one data center online for the Aegis customer 9876 to receive egress traffic from Cloudflare. If we tried to take data centers 21 and 45 down simultaneously, our coordinator would alert us that there would be unintended consequences for that customer workload.\nWe initially had a naive solution to load all data into a single Worker. This included all server relationships, product configurations, and metrics for product and infrastructure health to compute constraints. Even in our proof of concept phase, we ran into problems with “out of memory” errors.\nWe needed to be more cognizant of Workers’ platform limits. This required loading only as much data as was absolutely necessary to process the constraint’s business logic. If a maintenance request for a router in Frankfurt, Germany, comes in, we almost certainly do not care what is happening in Australia since there is no overlap across regions. Thus, we should only load data for neighboring data centers in Germany. We needed a more efficient way to process relationships in our dataset.\nGraph processing on Workers\nAs we looked at our constraints, a pattern emerged where each constraint boiled down to two concepts: objects and associations. In graph theory, these components are known as vertices and edges, respectively. An object could be a network router and an association could be the list of Aegis pools in the data center that requires the router to be online. We took inspiration from Facebook’s TAO research paper to establish a graph interface on top of our product and infrastructure data. The API looks like the following:\ntype ObjectID = string\n\ninterface MainTAOInterface<TObject, TAssoc, TAssocType> {\n  object_get(id: ObjectID): Promise<TObject | undefined>\n\n  assoc_get(id1: ObjectID, atype: TAssocType): AsyncIterable<TAssoc>\n}\nThe core insight is that associations are typed. For example, a constraint would call the graph interface to retrieve Aegis product data.\nasync function constraint(c: AppContext, aegis: TAOAegisClient, datacenters: string[]): Promise<Record<string, PoolAnalysis>> {\n  const datacenterEntries = await Promise.all(\n    datacenters.map(async (dcID) => {\n      const iter = aegis.assoc_get(c, dcID, AegisAssocType.DATACENTER_INSIDE_AEGIS_POOL)\n      const pools: string[] = []\n      for await (const assoc of iter) {\n        pools.push(assoc.id2)\n      }\n      return [dcID, pools] as const\n    }),\n  )\n\n  const datacenterToPools = new Map<string, string[]>(datacenterEntries)\n  const uniquePools = new Set<string>()\n  for (const pools of datacenterToPools.values()) {\n    for (const pool of pools) uniquePools.add(pool)\n  }\n\n  const poolTotalsEntries = await Promise.all(\n    [...uniquePools].map(async (pool) => {\n      const total = aegis.assoc_count(c, pool, AegisAssocType.AEGIS_POOL_CONTAINS_DATACENTER)\n      return [pool, total] as const\n    }),\n  )\n\n  const poolTotals = new Map<string, number>(poolTotalsEntries)\n  const poolAnalysis: Record<string, PoolAnalysis> = {}\n  for (const [dcID, pools] of datacenterToPools.entries()) {\n    for (const pool of pools) {\n      poolAnalysis[pool] = {\n        affectedDatacenters: new Set([dcID]),\n        totalDatacenters: poolTotals.get(pool),\n      }\n    }\n  }\n\n  return poolAnalysis\n}\nWe use two association types in the code above:\n\nDATACENTER_INSIDE_AEGIS_POOL, which retrieves the Aegis customer pools that a data center resides in.\n\nAEGIS_POOL_CONTAINS_DATACENTER, which retrieves the data centers an Aegis pool needs to serve traffic.\n\nThe associations are inverted indices of one another. The access pattern is exactly the same as before, but now the graph implementation has much more control of how much data it queries. Before, we needed to load all Aegis pools into memory and filter inside constraint business logic. Now, we can directly fetch only the data that matters to the application.\nThe interface is powerful because our graph implementation can improve performance behind the scenes without complicating the business logic. This lets us use the scalability of Workers and Cloudflare’s CDN to fetch data from our internal systems very quickly.\nFetch pipeline\nWe switched to using the new graph implementation, sending more targeted API requests. Response sizes dropped by 100x overnight, switching from loading a few massive requests to many tiny requests.\nWhile this solves the issue of loading too much into memory, we now have a subrequest problem because instead of a few large HTTP requests, we make an order of magnitude more of small requests. Overnight, we started consistently breaching subrequest limits.\nIn order to solve this problem, we built a smart middleware layer between our graph implementation and the fetch API.\nexport const fetchPipeline = new FetchPipeline()\n  .use(requestDeduplicator())\n  .use(lruCacher({\n    maxItems: 100,\n  }))\n  .use(cdnCacher())\n  .use(backoffRetryer({\n    retries: 3,\n    baseMs: 100,\n    jitter: true,\n  }))\n  .handler(terminalFetch);\nIf you’re familiar with Go, you may have seen the singleflight package before. We took inspiration from this idea and the first middleware component in the fetch pipeline deduplicates inflight HTTP requests, so they all wait on the same Promise for data instead of producing duplicate requests in the same Worker. Next, we use a lightweight Least Recently Used (LRU) cache to internally cache requests that we have already seen before.\nOnce both of those are complete, we use Cloudflare’s caches.default.match function to cache all GET requests in the region that the Worker is running. Since we have multiple data sources with different performance characteristics, we choose time to live (TTL) values carefully. For example, real-time data is only cached for 1 minute. Relatively static infrastructure data could be cached for 1–24 hours depending on the type of data. Power management data might be changed manually and infrequently, so we can cache it for longer at the edge.\nIn addition to those layers, we have the standard exponential backoff, retries and jitter. This helps reduce wasted fetch calls where a downstream resource might be unavailable temporarily. By backing off slightly, we increase the chance that we fetch the next request successfully. Conversely, if the Worker sends requests constantly without backoff, it will easily breach the subrequest limit when the origin starts returning 5xx errors.\nPutting it all together, we saw ~99% cache hit rate. Cache hit rate is the percentage of HTTP requests served from Cloudflare’s fast cache memory (a \"hit\") versus slower requests to data sources running in our control plane (a \"miss\"), calculated as (hits / (hits + misses)). A high rate means better HTTP request performance and lower costs because querying data from cache in our Worker is an order of magnitude faster than fetching from an origin server in a different region. After tuning settings, for our in memory and CDN caches, hit rates have increased dramatically. Since much of our workload is real-time, we will never have a 100% hit rate as we must request fresh data at least once per minute.\nWe have talked about improving the fetching layer, but not about how we made origin HTTP requests faster. Our maintenance coordinator needs to react in real-time to network degradation and failure of machines in data centers. We use our distributed Prometheus query engine, Thanos, to deliver performant metrics from the edge into the coordinator.\nThanos in real-time\nTo explain how our choice in using the graph processing interface affected our real-time queries, let’s walk through an example. In order to analyze the health of edge routers, we could send the following query:\nsum by (instance) (network_snmp_interface_admin_status{instance=~\"edge.*\"})\nOriginally, we asked our Thanos service, which stores Prometheus metrics, for a list of each edge router’s current health status and would manually filter for routers relevant to the maintenance inside the Worker. This is suboptimal for many reasons. For example, Thanos returned multi-MB responses which it needed to decode and encode. The Worker also needed to cache and decode these large HTTP responses only to filter out the majority of the data while processing a specific maintenance request. Since TypeScript is single-threaded and parsing JSON data is CPU-bound, sending two large HTTP requests means that one is blocked waiting for the other to finish parsing.\nInstead, we simply use the graph to find targeted relationships such as the interface links between edge and spine routers, denoted as EDGE_ROUTER_NETWORK_CONNECTS_TO_SPINE.\nsum by (lldp_name) (network_snmp_interface_admin_status{instance=~\"edge01.fra03\", lldp_name=~\"spine.*\"})\nThe result is 1 Kb on average instead of multiple MBs, or approximately 1000x smaller. This also massively reduces the amount of CPU required inside the Worker because we offload most of the deserialization to Thanos. As we explained before, this means we need to make a higher number of these smaller fetch requests, but load balancers in front of Thanos can spread the requests evenly to increase throughput for this use case. \nOur graph implementation and fetch pipeline successfully tamed the 'thundering herd' of thousands of tiny real-time requests. However, historical analysis presents a different I/O challenge. Instead of fetching small, specific relationships, we need to scan months of data to find conflicting maintenance windows. In the past, Thanos would issue a massive amount of random reads to our object store, R2. To solve this massive bandwidth penalty without losing performance, we adopted a new approach the Observability team developed internally this year.\nHistorical data analysis\nThere are enough maintenance use cases that we must rely on historical data to tell us if our solution is accurate and will scale with the growth of Cloudflare’s network. We do not want to cause incidents, and we also want to avoid blocking proposed physical maintenance unnecessarily. In order to balance these two priorities, we can use time series data about maintenance events that happened two months or even a year ago to tell us how often a maintenance event is violating one of our constraints, e.g. edge router availability or Aegis. We blogged earlier this year about using Thanos to automatically release and revert software to the edge.\nThanos primarily fans out to Prometheus, but when Prometheus' retention is not enough to answer the query it has to download data from object storage — R2 in our case. Prometheus TSDB blocks were originally designed for local SSDs, relying on random access patterns that become a bottleneck when moved to object storage. When our scheduler needs to analyze months of historical maintenance data to identify conflicting constraints, random reads from object storage incur a massive I/O penalty. To solve this, we implemented a conversion layer that transforms these blocks into Apache Parquet files. Parquet is a columnar format native to big data analytics that organizes data by column rather than row, which — together with rich statistics — allows us to only fetch what we need.\nFurthermore, since we are rewriting TSDB blocks into Parquet files, we can also store the data in a way that allows us to read the data in just a few big sequential chunks.\nsum by (instance) (hmd:release_scopes:enabled{dc_id=\"45\"})\nIn the example above we would choose the tuple “(__name__, dc_id)” as a primary sorting key so that metrics with the name “hmd:release_scopes:enabled” and the same value for “dc_id” get sorted close together.\nOur Parquet gateway now issues precise R2 range requests to fetch only the specific columns relevant to the query. This reduces the payload from megabytes to kilobytes. Furthermore, because these file segments are immutable, we can aggressively cache them on the Cloudflare CDN.\nThis turns R2 into a low-latency query engine, allowing us to backtest complex maintenance scenarios against long-term trends instantly, avoiding the timeouts and high tail latency we saw with the original TSDB format. The graph below shows a recent load test, where Parquet reached up to 15x the P90 performance compared to the old system for the same query pattern.\nTo get a deeper understanding of how the Parquet implementation works, you can watch this talk at PromCon EU 2025, Beyond TSDB: Unlocking Prometheus with Parquet for Modern Scale.\nBuilding for scale\nBy leveraging Cloudflare Workers, we moved from a system that ran out of memory to one that intelligently caches data and uses efficient observability tooling to analyze product and infrastructure data in real time. We built a maintenance scheduler that balances network growth with product performance.\nBut “balance” is a moving target.\nEvery day, we add more hardware around the world, and the logic required to maintain it without disrupting customer traffic gets exponentially harder with more products and types of maintenance operations. We’ve worked through the first set of challenges, but now we’re staring down more subtle, complex ones that only appear at this massive scale.\nWe need engineers who aren't afraid of hard problems. Join our Infrastructure team and come build with us.","dc:creator":"Kevin Deems","content":" Physical data center maintenance is risky on a global network. We built a maintenance scheduler on Workers to safely plan disruptive operations, while solving scaling challenges by viewing the state of our infrastructure through a graph interface on top of multiple data sources and metrics pipelines. ","contentSnippet":"Physical data center maintenance is risky on a global network. We built a maintenance scheduler on Workers to safely plan disruptive operations, while solving scaling challenges by viewing the state of our infrastructure through a graph interface on top of multiple data sources and metrics pipelines.","guid":"5pdspiP2m71MeIoVL8wv1i","categories":["Cloudflare Workers","Reliability","Prometheus","Infrastructure"],"isoDate":"2025-12-22T14:00:00.000Z"},{"creator":"Dane Knecht","title":"Code Orange: Fail Small — our resilience plan following recent incidents","link":"https://blog.cloudflare.com/fail-small-resilience-plan/","pubDate":"Fri, 19 Dec 2025 22:35:30 GMT","content:encoded":" <p></p><p>On <a href=\"https://blog.cloudflare.com/18-november-2025-outage/\"><u>November 18, 2025</u></a>, Cloudflare’s network experienced significant failures to deliver network traffic for approximately two hours and ten minutes. Nearly three weeks later, on <a href=\"https://blog.cloudflare.com/5-december-2025-outage/\"><u>December 5, 2025</u></a>, our network again failed to serve traffic for 28% of applications behind our network for about 25 minutes.</p><p>We published detailed post-mortem blog posts following both incidents, but we know that we have more to do to earn back your trust. Today we are sharing details about the work underway at Cloudflare to prevent outages like these from happening again.</p><p>We are calling the plan “<b>Code Orange: Fail Small</b>”, which reflects our goal of making our network more resilient to errors or mistakes that could lead to a major outage. A “Code Orange” means the work on this project is prioritized above all else. For context, we declared a “Code Orange” at Cloudflare <a href=\"https://blog.cloudflare.com/major-data-center-power-failure-again-cloudflare-code-orange-tested/\"><u>once before</u></a>, following another major incident that required top priority from everyone across the company. We feel the recent events require the same focus.  Code Orange is our way to enable that to happen, allowing teams to work cross-functionally as necessary to get the job done while pausing any other work.</p><p>The Code Orange work is organized into three main areas:</p><ul><li><p>Require controlled rollouts for any configuration change that is propagated to the network, just like we do today for software binary releases.</p></li><li><p>Review, improve, and test failure modes of all systems handling network traffic to ensure they exhibit well-defined behavior under all conditions, including unexpected error states.</p></li><li><p>Change our internal “break glass”* procedures, and remove any circular dependencies so that we, and our customers, can act fast and access all systems without issue during an incident.</p></li></ul><p>These projects will deliver iterative improvements as they proceed, rather than one “big bang” change at their conclusion. Every individual update will contribute to more resiliency at Cloudflare. By the end, we expect Cloudflare’s network to be much more resilient, including for issues such as those that triggered the global incidents we experienced in the last two months.</p><p>We understand that these incidents are painful for our customers and the Internet as a whole. We’re deeply embarrassed by them, which is why this work is the first priority for everyone here at Cloudflare.</p><p><sup><b><i>*</i></b></sup><sup><i> Break glass procedures at Cloudflare allow certain individuals to elevate their privilege under certain circumstances to perform urgent actions to resolve high severity scenarios.</i></sup></p>\n    <div>\n      <h2>What went wrong?</h2>\n      <a href=\"#what-went-wrong\">\n        \n      </a>\n    </div>\n    <p>In the first incident, users visiting a customer site on Cloudflare saw error pages that indicated Cloudflare could not deliver a response to their request. In the second, they saw blank pages.</p><p>Both outages followed a similar pattern. In the moments leading up to each incident we instantaneously deployed a configuration change in our data centers in hundreds of cities around the world.</p><p>The November change was an automatic update to our Bot Management classifier. We run various artificial intelligence models that learn from the traffic flowing through our network to build detections that identify bots. We constantly update those systems to stay ahead of bad actors trying to evade our security protection to reach customer sites.</p><p>During the December incident, while trying to protect our customers from a vulnerability in the popular open source framework React, we deployed a change to a security tool used by our security analysts to improve our signatures. Similar to the urgency of new bot management updates, we needed to get ahead of the attackers who wanted to exploit the vulnerability. That change triggered the start of the incident.</p><p>This pattern exposed a serious gap in how we deploy configuration changes at Cloudflare, versus how we release software updates. When we release software version updates, we do so in a controlled and monitored fashion. For each new binary release, the deployment must successfully complete multiple gates before it can serve worldwide traffic. We deploy first to employee traffic, before carefully rolling out the change to increasing percentages of customers worldwide, starting with free users. If we detect an anomaly at any stage, we can revert the release without any human intervention.</p><p>We have not applied that methodology to configuration changes. Unlike releasing the core software that powers our network, when we make configuration changes, we are modifying the values of how that software behaves and we can do so instantly. We give this power to our customers too: If you make a change to a setting in Cloudflare, it will propagate globally in seconds.</p><p>While that speed has advantages, it also comes with risks that we need to address. The past two incidents have demonstrated that we need to treat any change that is applied to how we serve traffic in our network with the same level of tested caution that we apply to changes to the software itself.</p>\n    <div>\n      <h2>We will change how we deploy configuration updates at Cloudflare</h2>\n      <a href=\"#we-will-change-how-we-deploy-configuration-updates-at-cloudflare\">\n        \n      </a>\n    </div>\n    <p>Our ability to deploy configuration changes globally within seconds was the core commonality across the two incidents. In both events, a wrong configuration took down our network in seconds.</p><p>Introducing controlled rollouts of our configuration, just as we <b><i>already do</i></b> for software releases, is the most important workstream of our Code Orange plan.</p><p>Configuration changes at Cloudflare propagate to the network very quickly. When a user creates a new DNS record, or creates a new security rule, it reaches 90% of servers on the network within seconds. This is powered by a software component that we internally call Quicksilver.</p><p>Quicksilver is also used for any configuration change required by our own teams. The speed is a feature: we can react and globally update our network behavior very quickly. However, in both incidents this caused a breaking change to propagate to the entire network in seconds rather than passing through gates to test it.</p><p>While the ability to deploy changes to our network on a near-instant basis is useful in many cases, it is rarely necessary. Work is underway to treat configuration the same way that we treat code by introducing controlled deployments within Quicksilver to any configuration change.</p><p>We release software updates to our network multiple times per day through what we call our Health Mediated Deployment (HMD) system. In this framework, every team at Cloudflare that owns a service (a piece of software deployed into our network) must define the metrics that indicate a deployment has succeeded or failed, the rollout plan, and the steps to take if it does not succeed.</p><p>Different services will have slightly different variables. Some might need longer wait times before proceeding to more data centers, while others might have lower tolerances for error rates even if it causes false positive signals.</p><p>Once deployed, our HMD toolkit begins to carefully progress against that plan while monitoring each step before proceeding. If any step fails, the rollback will automatically begin and the team can be paged if needed.</p><p>By the end of Code Orange, configuration updates will follow this same process. We expect this to allow us to quickly catch the kinds of issues that occurred in these past two incidents long before they become widespread problems.</p>\n    <div>\n      <h2>How will we address failure modes between services?</h2>\n      <a href=\"#how-will-we-address-failure-modes-between-services\">\n        \n      </a>\n    </div>\n    <p>While we are optimistic that better control over configuration changes will catch more problems before they become incidents, we know that mistakes can and will occur. During both incidents, errors in one part of our network became problems in most of our technology stack, including the control plane that customers rely on to configure how they use Cloudflare.</p><p>We need to think about careful, graduated rollouts not just in terms of geographic progression (spreading to more of our data centers) or in terms of population progression (spreading to employees and customer types). We also need to plan for safer deployments that contain failures from service progression (spreading from one product like our Bot Management service to an unrelated one like our dashboard).</p><p>To that end, we are in the process of reviewing the interface contracts between every critical product and service that comprise our network to ensure that we a) <b>assume failure will occur</b> between each interface and b) handle that failure in the absolute <b>most reasonable way possible</b>. </p><p>To go back to our Bot Management service failure, there were at least two key interfaces where, if we had assumed failure was going to happen, we could have handled it gracefully to the point that it was unlikely any customer would have been impacted. The first was in the interface that read the corrupted config file. Instead of panicking, there should have been a sane set of validated defaults which would have allowed traffic to pass through our network, while we would have, at worst, lost the realtime fine-tuning that feeds into our bot detection machine-learning models.\n\nThe second interface was between the core software that runs our network and the Bot Management module itself. In the event that our bot management module failed (as it did), we should not have dropped traffic by default. Instead, we could have come up with, yet again, a more sane default of allowing the traffic to pass with a passable classification.</p>\n    <div>\n      <h2>How will we solve emergencies faster?</h2>\n      <a href=\"#how-will-we-solve-emergencies-faster\">\n        \n      </a>\n    </div>\n    <p>During the incidents, it took us too long to resolve the problem. In both cases, this was worsened by our security systems preventing team members from accessing the tools they needed to fix the problem, and in some cases, circular dependencies slowed us down as some internal systems also became unavailable.</p><p>As a security company, all our tools are behind authentication layers with fine-grained access controls to ensure customer data is safe and to prevent unauthorized access. This is the right thing to do, but at the same time, our current processes and systems slowed us down when speed was a top priority.</p><p>Circular dependencies also affected our customer experience. For example, during the November 18 incident, Turnstile, our no CAPTCHA bot solution, became unavailable. As we use Turnstile on the login flow to the Cloudflare dashboard, customers who did not have active sessions, or API service tokens, were not able to log in to Cloudflare in the moment of most need to make critical changes.</p><p>Our team will be reviewing and improving all of the break glass procedures and technology to ensure that, when necessary, we can access the right tools as fast as possible while maintaining our security requirements. This includes reviewing and removing circular dependencies, or being able to “bypass” them quickly in the event there is an incident.<b> </b>We will also increase the frequency of our training exercises, so that processes are well understood by all teams prior to any potential disaster scenario in the future. </p>\n    <div>\n      <h2>When will we be done?</h2>\n      <a href=\"#when-will-we-be-done\">\n        \n      </a>\n    </div>\n    <p>While we haven’t captured in this post all the work being undertaken internally, the workstreams detailed above describe the top priorities the teams are being asked to focus on. Each of these workstreams maps to a detailed plan touching nearly every product and engineering team at Cloudflare. We have a lot of work to do.</p><p>By the end of Q1, and largely before then, we will:</p><ul><li><p>Ensure all production systems are covered by Health Mediated Deployments (HMD) for configuration management.</p></li><li><p>Update our systems to adhere to proper failure modes as appropriate for each product set.</p></li><li><p>Ensure we have processes in place so the right people have the right access to provide proper remediation during an emergency.</p></li></ul><p>Some of these goals will be evergreen. We will always need to better handle circular dependencies as we launch new software and our break glass procedures will need to update to reflect how our security technology changes over time.</p><p>We failed our users and the Internet as a whole in these past two incidents. We have work to do to make it right. We plan to share updates as this work proceeds and appreciate the questions and feedback we have received from our customers and partners.</p> ","content:encodedSnippet":"On November 18, 2025, Cloudflare’s network experienced significant failures to deliver network traffic for approximately two hours and ten minutes. Nearly three weeks later, on December 5, 2025, our network again failed to serve traffic for 28% of applications behind our network for about 25 minutes.\nWe published detailed post-mortem blog posts following both incidents, but we know that we have more to do to earn back your trust. Today we are sharing details about the work underway at Cloudflare to prevent outages like these from happening again.\nWe are calling the plan “Code Orange: Fail Small”, which reflects our goal of making our network more resilient to errors or mistakes that could lead to a major outage. A “Code Orange” means the work on this project is prioritized above all else. For context, we declared a “Code Orange” at Cloudflare once before, following another major incident that required top priority from everyone across the company. We feel the recent events require the same focus.  Code Orange is our way to enable that to happen, allowing teams to work cross-functionally as necessary to get the job done while pausing any other work.\nThe Code Orange work is organized into three main areas:\n\nRequire controlled rollouts for any configuration change that is propagated to the network, just like we do today for software binary releases.\n\nReview, improve, and test failure modes of all systems handling network traffic to ensure they exhibit well-defined behavior under all conditions, including unexpected error states.\n\nChange our internal “break glass”* procedures, and remove any circular dependencies so that we, and our customers, can act fast and access all systems without issue during an incident.\n\nThese projects will deliver iterative improvements as they proceed, rather than one “big bang” change at their conclusion. Every individual update will contribute to more resiliency at Cloudflare. By the end, we expect Cloudflare’s network to be much more resilient, including for issues such as those that triggered the global incidents we experienced in the last two months.\nWe understand that these incidents are painful for our customers and the Internet as a whole. We’re deeply embarrassed by them, which is why this work is the first priority for everyone here at Cloudflare.\n* Break glass procedures at Cloudflare allow certain individuals to elevate their privilege under certain circumstances to perform urgent actions to resolve high severity scenarios.\nWhat went wrong?\nIn the first incident, users visiting a customer site on Cloudflare saw error pages that indicated Cloudflare could not deliver a response to their request. In the second, they saw blank pages.\nBoth outages followed a similar pattern. In the moments leading up to each incident we instantaneously deployed a configuration change in our data centers in hundreds of cities around the world.\nThe November change was an automatic update to our Bot Management classifier. We run various artificial intelligence models that learn from the traffic flowing through our network to build detections that identify bots. We constantly update those systems to stay ahead of bad actors trying to evade our security protection to reach customer sites.\nDuring the December incident, while trying to protect our customers from a vulnerability in the popular open source framework React, we deployed a change to a security tool used by our security analysts to improve our signatures. Similar to the urgency of new bot management updates, we needed to get ahead of the attackers who wanted to exploit the vulnerability. That change triggered the start of the incident.\nThis pattern exposed a serious gap in how we deploy configuration changes at Cloudflare, versus how we release software updates. When we release software version updates, we do so in a controlled and monitored fashion. For each new binary release, the deployment must successfully complete multiple gates before it can serve worldwide traffic. We deploy first to employee traffic, before carefully rolling out the change to increasing percentages of customers worldwide, starting with free users. If we detect an anomaly at any stage, we can revert the release without any human intervention.\nWe have not applied that methodology to configuration changes. Unlike releasing the core software that powers our network, when we make configuration changes, we are modifying the values of how that software behaves and we can do so instantly. We give this power to our customers too: If you make a change to a setting in Cloudflare, it will propagate globally in seconds.\nWhile that speed has advantages, it also comes with risks that we need to address. The past two incidents have demonstrated that we need to treat any change that is applied to how we serve traffic in our network with the same level of tested caution that we apply to changes to the software itself.\nWe will change how we deploy configuration updates at Cloudflare\nOur ability to deploy configuration changes globally within seconds was the core commonality across the two incidents. In both events, a wrong configuration took down our network in seconds.\nIntroducing controlled rollouts of our configuration, just as we already do for software releases, is the most important workstream of our Code Orange plan.\nConfiguration changes at Cloudflare propagate to the network very quickly. When a user creates a new DNS record, or creates a new security rule, it reaches 90% of servers on the network within seconds. This is powered by a software component that we internally call Quicksilver.\nQuicksilver is also used for any configuration change required by our own teams. The speed is a feature: we can react and globally update our network behavior very quickly. However, in both incidents this caused a breaking change to propagate to the entire network in seconds rather than passing through gates to test it.\nWhile the ability to deploy changes to our network on a near-instant basis is useful in many cases, it is rarely necessary. Work is underway to treat configuration the same way that we treat code by introducing controlled deployments within Quicksilver to any configuration change.\nWe release software updates to our network multiple times per day through what we call our Health Mediated Deployment (HMD) system. In this framework, every team at Cloudflare that owns a service (a piece of software deployed into our network) must define the metrics that indicate a deployment has succeeded or failed, the rollout plan, and the steps to take if it does not succeed.\nDifferent services will have slightly different variables. Some might need longer wait times before proceeding to more data centers, while others might have lower tolerances for error rates even if it causes false positive signals.\nOnce deployed, our HMD toolkit begins to carefully progress against that plan while monitoring each step before proceeding. If any step fails, the rollback will automatically begin and the team can be paged if needed.\nBy the end of Code Orange, configuration updates will follow this same process. We expect this to allow us to quickly catch the kinds of issues that occurred in these past two incidents long before they become widespread problems.\nHow will we address failure modes between services?\nWhile we are optimistic that better control over configuration changes will catch more problems before they become incidents, we know that mistakes can and will occur. During both incidents, errors in one part of our network became problems in most of our technology stack, including the control plane that customers rely on to configure how they use Cloudflare.\nWe need to think about careful, graduated rollouts not just in terms of geographic progression (spreading to more of our data centers) or in terms of population progression (spreading to employees and customer types). We also need to plan for safer deployments that contain failures from service progression (spreading from one product like our Bot Management service to an unrelated one like our dashboard).\nTo that end, we are in the process of reviewing the interface contracts between every critical product and service that comprise our network to ensure that we a) assume failure will occur between each interface and b) handle that failure in the absolute most reasonable way possible. \nTo go back to our Bot Management service failure, there were at least two key interfaces where, if we had assumed failure was going to happen, we could have handled it gracefully to the point that it was unlikely any customer would have been impacted. The first was in the interface that read the corrupted config file. Instead of panicking, there should have been a sane set of validated defaults which would have allowed traffic to pass through our network, while we would have, at worst, lost the realtime fine-tuning that feeds into our bot detection machine-learning models.\n\nThe second interface was between the core software that runs our network and the Bot Management module itself. In the event that our bot management module failed (as it did), we should not have dropped traffic by default. Instead, we could have come up with, yet again, a more sane default of allowing the traffic to pass with a passable classification.\nHow will we solve emergencies faster?\nDuring the incidents, it took us too long to resolve the problem. In both cases, this was worsened by our security systems preventing team members from accessing the tools they needed to fix the problem, and in some cases, circular dependencies slowed us down as some internal systems also became unavailable.\nAs a security company, all our tools are behind authentication layers with fine-grained access controls to ensure customer data is safe and to prevent unauthorized access. This is the right thing to do, but at the same time, our current processes and systems slowed us down when speed was a top priority.\nCircular dependencies also affected our customer experience. For example, during the November 18 incident, Turnstile, our no CAPTCHA bot solution, became unavailable. As we use Turnstile on the login flow to the Cloudflare dashboard, customers who did not have active sessions, or API service tokens, were not able to log in to Cloudflare in the moment of most need to make critical changes.\nOur team will be reviewing and improving all of the break glass procedures and technology to ensure that, when necessary, we can access the right tools as fast as possible while maintaining our security requirements. This includes reviewing and removing circular dependencies, or being able to “bypass” them quickly in the event there is an incident. We will also increase the frequency of our training exercises, so that processes are well understood by all teams prior to any potential disaster scenario in the future. \nWhen will we be done?\nWhile we haven’t captured in this post all the work being undertaken internally, the workstreams detailed above describe the top priorities the teams are being asked to focus on. Each of these workstreams maps to a detailed plan touching nearly every product and engineering team at Cloudflare. We have a lot of work to do.\nBy the end of Q1, and largely before then, we will:\n\nEnsure all production systems are covered by Health Mediated Deployments (HMD) for configuration management.\n\nUpdate our systems to adhere to proper failure modes as appropriate for each product set.\n\nEnsure we have processes in place so the right people have the right access to provide proper remediation during an emergency.\n\nSome of these goals will be evergreen. We will always need to better handle circular dependencies as we launch new software and our break glass procedures will need to update to reflect how our security technology changes over time.\nWe failed our users and the Internet as a whole in these past two incidents. We have work to do to make it right. We plan to share updates as this work proceeds and appreciate the questions and feedback we have received from our customers and partners.","dc:creator":"Dane Knecht","content":" We have declared “Code Orange: Fail Small” to focus everyone at Cloudflare on a set of high-priority workstreams with one simple goal: ensure that the cause of our last two global outages never happens again. ","contentSnippet":"We have declared “Code Orange: Fail Small” to focus everyone at Cloudflare on a set of high-priority workstreams with one simple goal: ensure that the cause of our last two global outages never happens again.","guid":"DMVZ2E5NT13VbQvP1hUNj","categories":["Outage","Post Mortem","Code Orange"],"isoDate":"2025-12-19T22:35:30.000Z"},{"creator":"Justin Paine","title":"Innovating to address streaming abuse — and our latest transparency report","link":"https://blog.cloudflare.com/h1-2025-transparency-report/","pubDate":"Fri, 19 Dec 2025 14:00:00 GMT","content:encoded":" <p>Cloudflare's latest <a href=\"https://www.cloudflare.com/transparency/\"><u>transparency report</u></a> — covering the first half of 2025 — is now live. As part of our commitment to transparency, Cloudflare publishes such reports twice a year, describing how we handle legal requests for customer information and reports of abuse of our services. Although we’ve been publishing these reports for over 10 years, we’ve continued to adapt our transparency reporting and our commitments to reflect Cloudflare’s growth and changes as a company. Most recently, we made <a href=\"https://blog.cloudflare.com/cloudflare-2024-transparency-reports-now-live-with-new-data-and-a-new-format/\"><u>changes</u></a> to the format of our reports to make them even more comprehensive and understandable.</p><p>In general, we try to provide updates on our approach or the requests that we receive in the transparency report itself. To that end, we have some notable updates for the first half of 2025. But our transparency report can only go so far in explaining the numbers. </p><p>In this blog post, we’ll do a deeper dive on one topic: Cloudflare’s approach to streaming and claims of copyright violations. Given increased access to AI tools and other systems for abuse, bad actors have become increasingly sophisticated in the way they attempt to abuse systems to stream copyrighted content, often incorporating steps to hide their behavior. We’ve responded by experimenting with new ways to address allegations of streaming and copyright infringement, working closely with rightsholders to better identify domains and accounts that might be streaming to speed up our processes to respond in real time and to better identify possible risks. </p><p>This effort aligns with the interests of policymakers, rightsholders, and online service providers in preventing pirated streaming of sporting and other events over the Internet. Indeed, the same actors who infringe legitimate intellectual property rights with unauthorized streaming may seek to misuse Cloudflare’s services, impacting performance, costs, and reliability for other users. This shared interest in identifying and responding to unauthorized streaming has led to opportunities for partnerships and better information sharing. Preventing unauthorized streaming is a hard problem that requires those partnerships, with streamers constantly finding new ways to evade detection and preventive actions.</p>\n    <div>\n      <h3>Innovating to address abuse and identify new threats </h3>\n      <a href=\"#innovating-to-address-abuse-and-identify-new-threats\">\n        \n      </a>\n    </div>\n    <p>With approximately 20% of the web behind Cloudflare’s network, building smart and scalable abuse processes has never been optional. Even as a much smaller company with more limited services, we <a href=\"https://blog.cloudflare.com/out-of-the-clouds-and-into-the-weeds-cloudflares-approach-to-abuse-in-new-products/\"><u>recognized</u></a> the importance of creating a system that efficiently got abuse reporting to those best positioned to action the reports, typically the website owner or hosting provider. Our view was that we could play an important role in ensuring that allegations of abuse reported to us went to those entities without compromising their security.</p><p>As we have developed new services, we have applied a service-specific <a href=\"https://www.cloudflare.com/trust-hub/abuse-approach/\"><u>approach to abuse</u></a>, reflecting the nature of the services provided, legal requirements, and human rights considerations. This approach means that we treat hosted content differently than content on websites that use our security and CDN services, an approach reflected throughout our transparency report. </p><p>Beyond Cloudflare’s response to individual abuse reports, we also recognize the value of systems that learn from the abuse reports we receive. Not only do efforts to identify abuse patterns improve our ability to detect and mitigate abuse on our network, they enable us to better protect our customers from a wide range of cyber threats.</p><p>Rapid developments in AI and constantly improving technologies create new challenges and new opportunities. Bad actors have learned how to use AI to quickly stand up sophisticated phishing campaigns, or shift and divide unauthorized streaming traffic to evade detection. LLMs also enable misuse of abuse reporting systems, facilitating the creation of large volumes of low quality or even malicious abuse reports.</p><p>At the same time, the ability to apply machine learning and AI to the reams of traffic and information behind Cloudflare’s network has enabled the development of new tools to detect and mitigate abusive conduct. Cloudflare has created automated systems that can keep up with the scale of the issue, all while more accurately identifying genuine abuse. In 2024, as reflected in the temporary surge in phishing actions reported in our <a href=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/7vust2n7oACblNR2Jk7jZx/5b84afdbb6fbdcc751d6a7ba9a7f938b/H2_2024_AbuseProcessesTransparencyReport_AQFinal.pdf?_gl=1*3escw2*_gcl_au*MjgzODYzMTA4LjE3NTY4NDEzMjg.*_ga*MmIwZjcyYmUtY2EzYi00ZDdlLWJhZWEtOTM5NDQ2MjFhZGEz*_ga_SQCRB0TXZW*czE3NjIxMTg3OTkkbzIxMiRnMSR0MTc2MjEyMTM3OCRqNTkkbDAkaDAkZHdnZlU5UHM2VU5YUUlhRVVlUkNKb1g0ck1kM3ZiR2xZM0E.\"><u>abuse transparency report</u></a>, Cloudflare expanded the use of automated systems to respond to reports of technical abuse like phishing. Behind the scenes, Cloudflare has taken similar steps to identify new patterns of abusive behavior, to help prevent bad actors from using our services in the first place.</p><p>Knowing that bad actors aren’t likely to give up, Cloudflare has continued innovating in 2025. We’re exploring new ways to learn about and respond to abuse, with the goal of identifying and pursuing the strategies with the most promise for long-term impact.</p>\n    <div>\n      <h3>Technical responses to streaming abuse</h3>\n      <a href=\"#technical-responses-to-streaming-abuse\">\n        \n      </a>\n    </div>\n    <p>Cloudflare has always believed that, regardless of their size, websites deserve a secure, fast, reliable web presence. And because we didn’t think you should have to pay for coming under cyberattack, we’ve offered a <a href=\"https://www.cloudflare.com/plans/free/\"><u>free plan</u></a> for websites since Cloudflare launched in 2010. That system — which protects websites around the world from cyberattack for free — works because websites do not consume much bandwidth.</p><p>Streaming is different. Every second of a typical video requires as much bandwidth as loading a full webpage. To ensure that we can continue to provide free services, we’ve always restricted use of our free services to deliver streaming video. Although most of our customers respect these limitations and understand the role they play in enabling our ability to provide these services for free, we sometimes have users attempt to misconfigure our service to stream video.</p><p>In the first half of 2025, Cloudflare worked with several large rightsholders on efforts to address unauthorized streaming. This included providing rightsholders with an API for streamlined reporting, giving feedback on the quality of reports to ensure rightsholders are giving us actionable information, and, after verifying reports against our own internal metrics, taking steps to respond to streaming reports at scale.</p><p>Those efforts bore results, helping us better identify and action unauthorized streaming. The engagement resulted in a significant increase in DMCA reports that Cloudflare received for websites using our hosted services, from approximately 11,000 in the second half of 2024 to approximately 125,000 in the first half of 2025. It also enabled us to speed up our notice and takedown process as we took action in response to 54,000 reports, compared to 1,000 reports in the second half of 2024. Using information from these reports, we identified additional signs of abusive behavior, leading us to terminate hosting services to another 21,000 accounts.</p><p>Cloudflare also relied on information provided by rightsholders to bolster our technical tools for preventing unauthorized streaming over Cloudflare’s network by websites using our non-hosted services. To maintain the ability to provide free and low-cost services to static websites, we may take action on websites using those services if they appear to be streaming, regardless of whether that content infringes on copyright. Over the years, we have built a variety of tools to identify and restrict this type of streaming. While rightsholders’ streaming reports are focused on infringement, we can use these reports as signals to help inform our technical tools and improve our response. Working closely with rightsholders has improved our response time on their specific abuse reports and has also helped us prevent thousands of similar websites attempting to stream in an unauthorized manner over our network before they have ever been identified as infringing.</p><p>The information about streamer tactics and techniques gleaned from these efforts are useful in our broader cybersecurity efforts. Earlier this year, for example, we used information from our streaming program to help a smaller customer whose services were being abused to host streaming content without their knowledge. Understanding how illegal streamers were accessing and abusing their services enabled us to provide them guidance and tools to prevent the behavior.</p><p>While we have made significant progress on this issue, we fully expect that streamers will adjust their behavior in response to the steps we’ve taken. Cloudflare’s work is not done, and we will continue to look for innovative ways to prevent and address this type of abuse. </p>\n    <div>\n      <h3>Addressing blocking demands</h3>\n      <a href=\"#addressing-blocking-demands\">\n        \n      </a>\n    </div>\n    <p>As Cloudflare has been collaborating with rightsholders on technical solutions to streaming that address the issue in real time, many regulators and rightsholders have taken a clunkier approach: pursuing legally-mandated blocking of the Internet. Lack of technical expertise or sheer indifference can lead to significant overblocking of innocent websites, often without transparency or accountability for those responsible. We share the view of civil society groups like the <a href=\"https://www.internetsociety.org/resources/policybriefs/2025/perspectives-on-internet-content-blocking/\"><u>Internet Society</u></a> that the best and most effective approach remains removing illegal content at the source.</p><p>One of the most notorious examples of overblocking has been actions by Spanish football league LaLiga. Working through ISPs in Spain, they have engaged in widespread blocking of IP addresses shared by many thousands of websites during matches, without any government oversight. This has caused severe Internet outages across Spain during the time of matches. The disproportionate effect of IP address blocking is <a href=\"https://blog.cloudflare.com/consequences-of-ip-blocking/\"><u>well known</u></a>. LaLiga has nonetheless been unapologetic about causing the blocking of countless unrelated websites, suggesting that their commercial interests should trump the rights of Spanish Internet users to access the broader Internet during match times. Although this approach ignores well-established legal principles requiring that any blocking be proportionate to the problem, the Spanish government has not acted to protect the rights of Spanish Internet users. Balanced against these clear harms and lack of government willingness to provide sufficient oversight, we have seen no concrete evidence that such blunt force blocking efforts meaningfully solve the issue.</p><p>Cloudflare believes that regulators and rightsholders have a responsibility to seek out proportionate ways to prevent online infringement, and that working collaboratively with service providers offers the best way to effectively address abuse without fundamentally damaging the Internet. For reasons illustrated by the LaLiga example, blocking at the infrastructure layer is often overbroad, non-transparent, and ineffective.</p><p>Although we have real concerns about blocking, and particularly the way blocking has been co-opted by rightsholders to further their commercial interests over the rights of ordinary Internet users to access lawful content, Cloudflare has examined ways that blocking might be applied as a more targeted or proportionate response. In general, Cloudflare has found that blocking is of limited effectiveness, as determined users will find ways to circumvent restrictions. Nonetheless, Cloudflare has taken steps to comply with valid orders related to our CDN services that satisfy human rights principles relating to proportionality, due process, free expression, and transparency. In countries with laws that provide for blocking access to online content and provide appropriate oversight, Cloudflare may geoblock websites to limit access in the relevant jurisdiction to those websites through Cloudflare’s CDN services.</p><p>Cloudflare has never blocked through our public DNS resolver. As we have previously <a href=\"https://blog.cloudflare.com/latest-copyright-decision-in-germany-rejects-blocking-through-global-dns-resolvers/\"><u>described</u></a>, we believe demands to block through public DNS are at odds with the desire for an open Internet and would require the creation of new tools that are contrary to the design of our resolver. We continue to litigate against efforts to require us to build such capabilities. Cloudflare has sometimes taken action to geoblock access to websites through Cloudflare’s CDN and security services, in response to DNS blocking orders.</p><p>In the first half of 2025, Cloudflare saw a marked increase in the number of blocking orders it received in Europe. Private rightsholders obtained multiple orders directing Cloudflare to block access to websites in Belgium, France, and Italy. While Cloudflare has challenged aspects of those orders, we have taken steps to comply with them by geoblocking access to the websites at issue in the relevant countries through Cloudflare’s CDN and security services. </p><p>Cloudflare also began giving effect to UK court orders directing other service providers to block websites identified as being dedicated to copyright infringement. Based on a voluntary agreement with rightsholders, Cloudflare is geoblocking websites subject to these orders through our pass-through CDN and security services. When we take action on domains pursuant to these orders, we post an interstitial page that returns a <a href=\"https://developers.cloudflare.com/support/troubleshooting/http-status-codes/4xx-client-error/error-451/\"><u>451 status code</u></a> that directs the visitor to the specific order, which includes a process for affected parties to contest the blocking action.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/74JGtoTseoNdxz0xLIW4AK/55545650ab85002692ca9bb07ba6a2a9/image3.png\" />\n          </figure><p><sup>Example of a 451 error page in the UK.</sup></p><p>Our efforts in the UK to block content based on a finding of infringement in an order directed to a third party reflect our desire to experiment with more targeted approaches than the overblocking we have seen in other countries in Europe, as well as our understanding that the UK’s regime includes important protections around proportionality, due process, and transparency, including an opportunity for affected parties to seek redress. We are currently monitoring the impact of this approach, and have taken these steps with the understanding that we can change course if we see the system being abused. </p><p>Finally, in the first half of 2025, we have seen an expansion of areas for which blocking has been demanded. We received official government notices in France and Belgium that websites using our hosted services were offering gambling services illegally in those jurisdictions. In both cases, we were able to share the notice with our customer, and they took action themselves to address it. This illustrates the benefit of connecting our customer directly with the government regulator so that they can address issues with their websites, rather than proceeding directly to a blocking demand. </p>\n    <div>\n      <h3>Looking forward</h3>\n      <a href=\"#looking-forward\">\n        \n      </a>\n    </div>\n    <p>Cloudflare will continue to look for ways to work with rightsholders and regulators to find effective and proportionate ways to address online abuse. As a company that values transparency, we use our biannual transparency reports to describe the principles we apply in doing this work, and in responding to abuse reports or requests for customer information more generally. We invite you to dive into the numbers and <a href=\"https://www.cloudflare.com/transparency/\"><u>learn more here</u></a>.</p> ","content:encodedSnippet":"Cloudflare's latest transparency report — covering the first half of 2025 — is now live. As part of our commitment to transparency, Cloudflare publishes such reports twice a year, describing how we handle legal requests for customer information and reports of abuse of our services. Although we’ve been publishing these reports for over 10 years, we’ve continued to adapt our transparency reporting and our commitments to reflect Cloudflare’s growth and changes as a company. Most recently, we made changes to the format of our reports to make them even more comprehensive and understandable.\nIn general, we try to provide updates on our approach or the requests that we receive in the transparency report itself. To that end, we have some notable updates for the first half of 2025. But our transparency report can only go so far in explaining the numbers. \nIn this blog post, we’ll do a deeper dive on one topic: Cloudflare’s approach to streaming and claims of copyright violations. Given increased access to AI tools and other systems for abuse, bad actors have become increasingly sophisticated in the way they attempt to abuse systems to stream copyrighted content, often incorporating steps to hide their behavior. We’ve responded by experimenting with new ways to address allegations of streaming and copyright infringement, working closely with rightsholders to better identify domains and accounts that might be streaming to speed up our processes to respond in real time and to better identify possible risks. \nThis effort aligns with the interests of policymakers, rightsholders, and online service providers in preventing pirated streaming of sporting and other events over the Internet. Indeed, the same actors who infringe legitimate intellectual property rights with unauthorized streaming may seek to misuse Cloudflare’s services, impacting performance, costs, and reliability for other users. This shared interest in identifying and responding to unauthorized streaming has led to opportunities for partnerships and better information sharing. Preventing unauthorized streaming is a hard problem that requires those partnerships, with streamers constantly finding new ways to evade detection and preventive actions.\nInnovating to address abuse and identify new threats \nWith approximately 20% of the web behind Cloudflare’s network, building smart and scalable abuse processes has never been optional. Even as a much smaller company with more limited services, we recognized the importance of creating a system that efficiently got abuse reporting to those best positioned to action the reports, typically the website owner or hosting provider. Our view was that we could play an important role in ensuring that allegations of abuse reported to us went to those entities without compromising their security.\nAs we have developed new services, we have applied a service-specific approach to abuse, reflecting the nature of the services provided, legal requirements, and human rights considerations. This approach means that we treat hosted content differently than content on websites that use our security and CDN services, an approach reflected throughout our transparency report. \nBeyond Cloudflare’s response to individual abuse reports, we also recognize the value of systems that learn from the abuse reports we receive. Not only do efforts to identify abuse patterns improve our ability to detect and mitigate abuse on our network, they enable us to better protect our customers from a wide range of cyber threats.\nRapid developments in AI and constantly improving technologies create new challenges and new opportunities. Bad actors have learned how to use AI to quickly stand up sophisticated phishing campaigns, or shift and divide unauthorized streaming traffic to evade detection. LLMs also enable misuse of abuse reporting systems, facilitating the creation of large volumes of low quality or even malicious abuse reports.\nAt the same time, the ability to apply machine learning and AI to the reams of traffic and information behind Cloudflare’s network has enabled the development of new tools to detect and mitigate abusive conduct. Cloudflare has created automated systems that can keep up with the scale of the issue, all while more accurately identifying genuine abuse. In 2024, as reflected in the temporary surge in phishing actions reported in our abuse transparency report, Cloudflare expanded the use of automated systems to respond to reports of technical abuse like phishing. Behind the scenes, Cloudflare has taken similar steps to identify new patterns of abusive behavior, to help prevent bad actors from using our services in the first place.\nKnowing that bad actors aren’t likely to give up, Cloudflare has continued innovating in 2025. We’re exploring new ways to learn about and respond to abuse, with the goal of identifying and pursuing the strategies with the most promise for long-term impact.\nTechnical responses to streaming abuse\nCloudflare has always believed that, regardless of their size, websites deserve a secure, fast, reliable web presence. And because we didn’t think you should have to pay for coming under cyberattack, we’ve offered a free plan for websites since Cloudflare launched in 2010. That system — which protects websites around the world from cyberattack for free — works because websites do not consume much bandwidth.\nStreaming is different. Every second of a typical video requires as much bandwidth as loading a full webpage. To ensure that we can continue to provide free services, we’ve always restricted use of our free services to deliver streaming video. Although most of our customers respect these limitations and understand the role they play in enabling our ability to provide these services for free, we sometimes have users attempt to misconfigure our service to stream video.\nIn the first half of 2025, Cloudflare worked with several large rightsholders on efforts to address unauthorized streaming. This included providing rightsholders with an API for streamlined reporting, giving feedback on the quality of reports to ensure rightsholders are giving us actionable information, and, after verifying reports against our own internal metrics, taking steps to respond to streaming reports at scale.\nThose efforts bore results, helping us better identify and action unauthorized streaming. The engagement resulted in a significant increase in DMCA reports that Cloudflare received for websites using our hosted services, from approximately 11,000 in the second half of 2024 to approximately 125,000 in the first half of 2025. It also enabled us to speed up our notice and takedown process as we took action in response to 54,000 reports, compared to 1,000 reports in the second half of 2024. Using information from these reports, we identified additional signs of abusive behavior, leading us to terminate hosting services to another 21,000 accounts.\nCloudflare also relied on information provided by rightsholders to bolster our technical tools for preventing unauthorized streaming over Cloudflare’s network by websites using our non-hosted services. To maintain the ability to provide free and low-cost services to static websites, we may take action on websites using those services if they appear to be streaming, regardless of whether that content infringes on copyright. Over the years, we have built a variety of tools to identify and restrict this type of streaming. While rightsholders’ streaming reports are focused on infringement, we can use these reports as signals to help inform our technical tools and improve our response. Working closely with rightsholders has improved our response time on their specific abuse reports and has also helped us prevent thousands of similar websites attempting to stream in an unauthorized manner over our network before they have ever been identified as infringing.\nThe information about streamer tactics and techniques gleaned from these efforts are useful in our broader cybersecurity efforts. Earlier this year, for example, we used information from our streaming program to help a smaller customer whose services were being abused to host streaming content without their knowledge. Understanding how illegal streamers were accessing and abusing their services enabled us to provide them guidance and tools to prevent the behavior.\nWhile we have made significant progress on this issue, we fully expect that streamers will adjust their behavior in response to the steps we’ve taken. Cloudflare’s work is not done, and we will continue to look for innovative ways to prevent and address this type of abuse. \nAddressing blocking demands\nAs Cloudflare has been collaborating with rightsholders on technical solutions to streaming that address the issue in real time, many regulators and rightsholders have taken a clunkier approach: pursuing legally-mandated blocking of the Internet. Lack of technical expertise or sheer indifference can lead to significant overblocking of innocent websites, often without transparency or accountability for those responsible. We share the view of civil society groups like the Internet Society that the best and most effective approach remains removing illegal content at the source.\nOne of the most notorious examples of overblocking has been actions by Spanish football league LaLiga. Working through ISPs in Spain, they have engaged in widespread blocking of IP addresses shared by many thousands of websites during matches, without any government oversight. This has caused severe Internet outages across Spain during the time of matches. The disproportionate effect of IP address blocking is well known. LaLiga has nonetheless been unapologetic about causing the blocking of countless unrelated websites, suggesting that their commercial interests should trump the rights of Spanish Internet users to access the broader Internet during match times. Although this approach ignores well-established legal principles requiring that any blocking be proportionate to the problem, the Spanish government has not acted to protect the rights of Spanish Internet users. Balanced against these clear harms and lack of government willingness to provide sufficient oversight, we have seen no concrete evidence that such blunt force blocking efforts meaningfully solve the issue.\nCloudflare believes that regulators and rightsholders have a responsibility to seek out proportionate ways to prevent online infringement, and that working collaboratively with service providers offers the best way to effectively address abuse without fundamentally damaging the Internet. For reasons illustrated by the LaLiga example, blocking at the infrastructure layer is often overbroad, non-transparent, and ineffective.\nAlthough we have real concerns about blocking, and particularly the way blocking has been co-opted by rightsholders to further their commercial interests over the rights of ordinary Internet users to access lawful content, Cloudflare has examined ways that blocking might be applied as a more targeted or proportionate response. In general, Cloudflare has found that blocking is of limited effectiveness, as determined users will find ways to circumvent restrictions. Nonetheless, Cloudflare has taken steps to comply with valid orders related to our CDN services that satisfy human rights principles relating to proportionality, due process, free expression, and transparency. In countries with laws that provide for blocking access to online content and provide appropriate oversight, Cloudflare may geoblock websites to limit access in the relevant jurisdiction to those websites through Cloudflare’s CDN services.\nCloudflare has never blocked through our public DNS resolver. As we have previously described, we believe demands to block through public DNS are at odds with the desire for an open Internet and would require the creation of new tools that are contrary to the design of our resolver. We continue to litigate against efforts to require us to build such capabilities. Cloudflare has sometimes taken action to geoblock access to websites through Cloudflare’s CDN and security services, in response to DNS blocking orders.\nIn the first half of 2025, Cloudflare saw a marked increase in the number of blocking orders it received in Europe. Private rightsholders obtained multiple orders directing Cloudflare to block access to websites in Belgium, France, and Italy. While Cloudflare has challenged aspects of those orders, we have taken steps to comply with them by geoblocking access to the websites at issue in the relevant countries through Cloudflare’s CDN and security services. \nCloudflare also began giving effect to UK court orders directing other service providers to block websites identified as being dedicated to copyright infringement. Based on a voluntary agreement with rightsholders, Cloudflare is geoblocking websites subject to these orders through our pass-through CDN and security services. When we take action on domains pursuant to these orders, we post an interstitial page that returns a 451 status code that directs the visitor to the specific order, which includes a process for affected parties to contest the blocking action.\nExample of a 451 error page in the UK.\nOur efforts in the UK to block content based on a finding of infringement in an order directed to a third party reflect our desire to experiment with more targeted approaches than the overblocking we have seen in other countries in Europe, as well as our understanding that the UK’s regime includes important protections around proportionality, due process, and transparency, including an opportunity for affected parties to seek redress. We are currently monitoring the impact of this approach, and have taken these steps with the understanding that we can change course if we see the system being abused. \nFinally, in the first half of 2025, we have seen an expansion of areas for which blocking has been demanded. We received official government notices in France and Belgium that websites using our hosted services were offering gambling services illegally in those jurisdictions. In both cases, we were able to share the notice with our customer, and they took action themselves to address it. This illustrates the benefit of connecting our customer directly with the government regulator so that they can address issues with their websites, rather than proceeding directly to a blocking demand. \nLooking forward\nCloudflare will continue to look for ways to work with rightsholders and regulators to find effective and proportionate ways to address online abuse. As a company that values transparency, we use our biannual transparency reports to describe the principles we apply in doing this work, and in responding to abuse reports or requests for customer information more generally. We invite you to dive into the numbers and learn more here.","dc:creator":"Justin Paine","content":" Cloudflare's H1 2025 Transparency Report is here. We discuss our principles on content blocking and our innovative approach to combating unauthorized streaming and copyright abuse. ","contentSnippet":"Cloudflare's H1 2025 Transparency Report is here. We discuss our principles on content blocking and our innovative approach to combating unauthorized streaming and copyright abuse.","guid":"5mt8quFYw1l3UpRAh6JsHU","categories":["Transparency"],"isoDate":"2025-12-19T14:00:00.000Z"},{"creator":"Jérôme Schneider","title":"Announcing support for GROUP BY, SUM, and other aggregation queries in R2 SQL","link":"https://blog.cloudflare.com/r2-sql-aggregations/","pubDate":"Thu, 18 Dec 2025 14:00:00 GMT","content:encoded":" <p></p><p>When you’re dealing with large amounts of data, it’s helpful to get a quick overview — which is exactly what aggregations provide in SQL. Aggregations, known as “GROUP BY queries”, provide a bird’s eye view, so you can quickly gain insights from vast volumes of data.</p><p>That’s why we are excited to announce support for aggregations in <a href=\"https://blog.cloudflare.com/r2-sql-deep-dive/\"><u>R2 SQL</u></a>, Cloudflare's serverless, distributed, analytics query engine, which is capable of running SQL queries over data stored in <a href=\"https://developers.cloudflare.com/r2/data-catalog/\"><u>R2 Data Catalog</u></a>. Aggregations will allow users of <a href=\"https://developers.cloudflare.com/r2-sql/\"><u>R2 SQL</u></a> to spot important trends and changes in the data, generate reports and find anomalies in logs.</p><p>This release builds on the already supported filter queries, which are foundational for analytical workloads, and allow users to find needles in haystacks of <a href=\"https://parquet.apache.org/\"><u>Apache Parquet</u></a> files.</p><p>In this post, we’ll unpack the utility and quirks of aggregations, and then dive into how we extended R2 SQL to support running such queries over vast amounts of data stored in R2 Data Catalog.</p>\n    <div>\n      <h2>The importance of aggregations in analytics</h2>\n      <a href=\"#the-importance-of-aggregations-in-analytics\">\n        \n      </a>\n    </div>\n    <p>Aggregations, or “GROUP BY queries”, generate a short summary of the underlying data.</p><p>A common use case for aggregations is generating reports. Consider a table called “sales”, which contains historical data of all sales across various countries and departments of some organisation. One could easily generate a report on the volume of sales by department using this aggregation query:</p>\n            <pre><code>SELECT department, sum(value)\nFROM sales\nGROUP BY department</code></pre>\n            <p>\nThe “GROUP BY” statement allows us to split table rows into buckets. Each bucket has a label corresponding to a particular department. Once the buckets are full, we can then calculate “sum(value)” for all rows in each bucket, giving us the total volume of sales performed by the corresponding department.</p><p>For some reports, we might only be interested in departments that had the largest volume. That’s where an “ORDER BY” statement comes in handy:</p>\n            <pre><code>SELECT department, sum(value)\nFROM sales\nGROUP BY department\nORDER BY sum(value) DESC\nLIMIT 10</code></pre>\n            <p>Here we instruct the query engine to sort all department buckets by their total sales volume in the descending order and only return the top 10 largest.</p><p>Finally, we might be interested in filtering out anomalies. For example, we might want to only include departments that had more than five sales total in our report. We can easily do that with a “HAVING” statement:</p>\n            <pre><code>SELECT department, sum(value), count(*)\nFROM sales\nGROUP BY department\nHAVING count(*) &gt; 5\nORDER BY sum(value) DESC\nLIMIT 10</code></pre>\n            <p>Here we added a new aggregate function to our query — “count(*)” — which calculates how many rows ended up in each bucket. This directly corresponds to the number of sales in each department, so we have also added a predicate in the “HAVING” statement to make sure that we only leave buckets with more than five rows in them.</p>\n    <div>\n      <h2>Two approaches to aggregation: compute sooner or later</h2>\n      <a href=\"#two-approaches-to-aggregation-compute-sooner-or-later\">\n        \n      </a>\n    </div>\n    <p>Aggregation queries have a curious property: they can reference columns that are not stored anywhere. Consider “sum(value)”: this column is computed by the query engine on the fly, unlike the “department” column, which is fetched from Parquet files stored on R2. This subtle difference means that any query that references aggregates like “sum”, “count” and others needs to be split into two phases.</p><p>The first phase is computing new columns. If we are to sort the data by “count(*)” column using “ORDER BY” statement or filter rows based on it using “HAVING” statement, we need to know the values of this column. Once the values of columns like “count(*)” are known, we can proceed with the rest of the query execution.</p><p>Note that if the query does not reference aggregate functions in “HAVING” or “ORDER BY”, but still uses them in “SELECT”, we can make use of a trick. Since we do not need the values of aggregate functions until the very end, we can compute them partially and merge results just before we are about to return them to the user.</p><p>The key difference between the two approaches is when we compute aggregate functions: in advance, to perform some additional computations on them later; or on the fly, to iteratively build results the user needs.</p><p>First, we will dive into building results on the fly — a technique we call “scatter-gather aggregations.” We will then build on top of that to introduce “shuffling aggregations” capable of running extra computations like “HAVING” and “ORDER BY” on top of aggregate functions.</p>\n    <div>\n      <h2>Scatter-gather aggregations</h2>\n      <a href=\"#scatter-gather-aggregations\">\n        \n      </a>\n    </div>\n    <p>Aggregate queries without “HAVING” and “ORDER BY” can be executed in a fashion similar to filter queries. For filter queries, R2 SQL picks one node to be the coordinator in query execution. This node analyzes the query and consults R2 Data Catalog to figure out which Parquet row groups may contain data relevant to the query. Each Parquet row group represents a relatively small piece of work that a single compute node can handle. Coordinator node distributes the work across many worker nodes and collects results to return them to the user.</p><p>In order to execute aggregate queries, we follow all the same steps and distribute small pieces of work between worker nodes. However, this time instead of just filtering rows based on the predicate in the “WHERE” statement, worker nodes also compute <b>pre-aggregates</b>.</p><p>Pre-aggregates represent an intermediary state of an aggregation. This is an incomplete piece of data representing a partially computed aggregate function on a subset of data. Multiple pre-aggregates can be merged together to compute the final value of an aggregate function. Splitting aggregate functions into pre-aggregates allows us to horizontally scale computation of aggregation, making use of vast compute resources available in Cloudflare’s network.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2Vh0x4qHkjOuQTrxSzkVKx/84c05ebf590cb4949b188f5856a4e951/image2.png\" />\n          </figure><p>For example, pre-aggregate for “count(*)” is simply a number representing the count of rows in a subset of data. Computing the final “count(*)” is as easy as adding these numbers together. Pre-aggregate for “avg(value)” consists of two numbers: “sum(value)” and “count(*)”. The value of “avg(value)” can then be computed by adding together all “sum(value)” values, adding together all “count(*)” values and finally dividing one number by the other.</p><p>Once worker nodes have finished computing the pre-aggregates, they stream results to the coordinator node. The coordinator node collects all results, computes final values of aggregate functions from pre-aggregates, and returns the result to the user.</p>\n    <div>\n      <h2>Shuffling, beyond the limits of scatter-gather</h2>\n      <a href=\"#shuffling-beyond-the-limits-of-scatter-gather\">\n        \n      </a>\n    </div>\n    <p>Scatter-gather is highly efficient when the coordinator can compute the final result by merging small, partial states from workers. If you run a query like <code>SELECT sum(sales) FROM orders</code>, the coordinator receives a single number from each worker and adds them up. The memory footprint on the coordinator is negligible regardless of how much data resides in R2.</p><p>However, this approach becomes inefficient when the query requires sorting or filtering based on the <i>result</i> of an aggregation. Consider this query, which finds the top two departments by sales volume:</p>\n            <pre><code>SELECT department, sum(sales)\nFROM sales\nGROUP BY department\nORDER BY sum(sales) DESC\nLIMIT 2</code></pre>\n            <p>Correctly determining the global Top 2 requires knowing the total sales for every department across the entire dataset. Because the data is spread effectively at random across the underlying Parquet files, sales for a specific department are likely split across many different workers. A department might have low sales on every individual worker, excluding it from any local Top 2 list, yet have the highest sales volume globally when summed together.</p><p>The diagram below illustrates how a scatter-gather approach would not work for this query. \"Dept A\" is the global sales leader, but because its sales are evenly spread across workers, it doesn’t make to some local Top 2 lists, and ends up being discarded by the coordinator.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3ZJ6AfXzepKtJhiL6DcjiJ/07f4f523d871b25dcf444ee2ada546bd/image4.png\" />\n          </figure><p>Consequently, when the query orders results by their global aggregation, the coordinator cannot rely on pre-filtered results from workers. It must request the total count for <i>every</i> department from <i>every</i> worker to calculate the global totals before it can sort them. If you are grouping by a high-cardinality column like IP addresses or User IDs, this forces the coordinator to ingest and merge millions of rows, creating a resource bottleneck on a single node.</p><p>To solve this, we need <b>shuffling</b>, a way to colocate data for specific groups before the final aggregation occurs.</p>\n    <div>\n      <h3>Shuffling of aggregation data</h3>\n      <a href=\"#shuffling-of-aggregation-data\">\n        \n      </a>\n    </div>\n    <p>To address the challenges of random data distribution, we introduce a <b>shuffling stage</b>. Instead of sending results to the coordinator, workers exchange data directly with each other to colocate rows based on their grouping key.</p><p>This routing relies on <b>deterministic hash partitioning</b>. When a worker processes a row, it hashes the <code>GROUP BY</code> column to identify the destination worker. Because this hash is deterministic, every worker in the cluster independently agrees on where to send specific data. If \"Engineering\" hashes to Worker 5, every worker knows to route \"Engineering\" rows to Worker 5. No central registry is required.</p><p>The diagram below illustrates this flow. Notice how \"Dept A\" starts on Workers 1, 2 and 3. Because the hash function maps \"Dept A\" to Worker 1, all workers route those rows to that same destination.</p>\n          <figure>\n          <img src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3Mw7FvL7ZJgDZqnh3ygkZM/9cfb493b5889d7efe43e4719d9523c93/image1.png\" />\n          </figure><p>Shuffling aggregates produces the correct results. However, this all-to-all exchange creates a timing dependency. If Worker 1 begins calculating the final total for \"Dept A\" before Worker 3 has finished sending its share of the data, the result will be incomplete.</p><p>To address this, we enforce a strict <b>synchronization barrier</b>. The coordinator tracks the progress of the entire cluster while workers buffer their outgoing data and flush it via <a href=\"https://grpc.io/\"><u>gRPC</u></a> streams to their peers. Only when every worker confirms that it has finished processing its input files and flushing its shuffle buffers does the coordinator issue the command to proceed. This barrier guarantees that when the next stage begins, the dataset on each worker is complete and accurate.</p>\n    <div>\n      <h3>Local finalization</h3>\n      <a href=\"#local-finalization\">\n        \n      </a>\n    </div>\n    <p>Once the synchronization barrier is lifted, every worker holds the complete dataset for its assigned groups. Worker 1 now has 100% of the sales records for \"Dept A\" and can calculate the final total with certainty.</p><p>This allows us to push computational logic like filtering and sorting down to the worker rather than burdening the coordinator. For example, if the query includes <code>HAVING count(*) &gt; 5</code>, the worker can filter out groups that do not meet this criteria immediately after aggregation.</p><p>At the end of this stage, each worker produces a sorted, finalized stream of results for the groups it owns.</p>\n    <div>\n      <h3>The streaming merge</h3>\n      <a href=\"#the-streaming-merge\">\n        \n      </a>\n    </div>\n    <p>The final piece of the puzzle is the coordinator. In the scatter-gather model, the coordinator was responsible for the expensive task of aggregating and sorting the entire dataset. In the shuffling model, its role changes.</p><p>Because the workers have already computed the final aggregates and sorted them locally, the coordinator only needs to perform a <b>k-way merge</b>. It opens a stream to every worker and reads the results row by row. It compares the current row from each worker, picks the \"winner\" based on the sort order, and adds it to the query results that will be sent to the user.</p><p>This approach is particularly powerful for <code>LIMIT</code> queries. If a user asks for the top 10 departments, the coordinator merges the streams until it has found the top 10 items and then immediately stops processing. It does not need to load or merge the millions of remaining rows, allowing for greater scale of operation without over-consumption of compute resources.</p>\n    <div>\n      <h2>A powerful engine for processing massive datasets</h2>\n      <a href=\"#a-powerful-engine-for-processing-massive-datasets\">\n        \n      </a>\n    </div>\n    <p>With the addition of aggregations, <a href=\"https://developers.cloudflare.com/r2-sql/?cf_target_id=84F4CFDF79EFE12291D34EF36907F300\"><u>R2 SQL</u></a> transforms from a tool great for filtering data into a powerful engine capable of data processing on massive datasets. This is made possible by implementing distributed execution strategies like scatter-gather and shuffling, where we are able to push the compute to where the data lives, using the scale of Cloudflare’s global compute and network. </p><p>Whether you are generating reports, monitoring high-volume logs for anomalies, or simply trying to spot trends in your data, you can now easily do it all within Cloudflare’s Developer Platform without the overhead of managing complex OLAP infrastructure or moving data out of R2.</p>\n    <div>\n      <h2>Try it now</h2>\n      <a href=\"#try-it-now\">\n        \n      </a>\n    </div>\n    <p>Support for aggregations in R2 SQL is available today. We are excited to see how you use these new functions with data in R2 Data Catalog.</p><ul><li><p><b>Get Started:</b> Check out our <a href=\"https://developers.cloudflare.com/r2-sql/sql-reference/\"><u>documentation</u></a> for examples and syntax guides on running aggregation queries.</p></li><li><p><b>Join the Conversation:</b> If you have questions, feedback, or want to share what you’re building, join us in the Cloudflare <a href=\"https://discord.com/invite/cloudflaredev\"><u>Developer Discord</u></a>.</p></li></ul><p></p> ","content:encodedSnippet":"When you’re dealing with large amounts of data, it’s helpful to get a quick overview — which is exactly what aggregations provide in SQL. Aggregations, known as “GROUP BY queries”, provide a bird’s eye view, so you can quickly gain insights from vast volumes of data.\nThat’s why we are excited to announce support for aggregations in R2 SQL, Cloudflare's serverless, distributed, analytics query engine, which is capable of running SQL queries over data stored in R2 Data Catalog. Aggregations will allow users of R2 SQL to spot important trends and changes in the data, generate reports and find anomalies in logs.\nThis release builds on the already supported filter queries, which are foundational for analytical workloads, and allow users to find needles in haystacks of Apache Parquet files.\nIn this post, we’ll unpack the utility and quirks of aggregations, and then dive into how we extended R2 SQL to support running such queries over vast amounts of data stored in R2 Data Catalog.\nThe importance of aggregations in analytics\nAggregations, or “GROUP BY queries”, generate a short summary of the underlying data.\nA common use case for aggregations is generating reports. Consider a table called “sales”, which contains historical data of all sales across various countries and departments of some organisation. One could easily generate a report on the volume of sales by department using this aggregation query:\nSELECT department, sum(value)\nFROM sales\nGROUP BY department\nFor some reports, we might only be interested in departments that had the largest volume. That’s where an “ORDER BY” statement comes in handy:\nSELECT department, sum(value)\nFROM sales\nGROUP BY department\nORDER BY sum(value) DESC\nLIMIT 10\nHere we instruct the query engine to sort all department buckets by their total sales volume in the descending order and only return the top 10 largest.\nFinally, we might be interested in filtering out anomalies. For example, we might want to only include departments that had more than five sales total in our report. We can easily do that with a “HAVING” statement:\nSELECT department, sum(value), count(*)\nFROM sales\nGROUP BY department\nHAVING count(*) > 5\nORDER BY sum(value) DESC\nLIMIT 10\nHere we added a new aggregate function to our query — “count(*)” — which calculates how many rows ended up in each bucket. This directly corresponds to the number of sales in each department, so we have also added a predicate in the “HAVING” statement to make sure that we only leave buckets with more than five rows in them.\nTwo approaches to aggregation: compute sooner or later\nAggregation queries have a curious property: they can reference columns that are not stored anywhere. Consider “sum(value)”: this column is computed by the query engine on the fly, unlike the “department” column, which is fetched from Parquet files stored on R2. This subtle difference means that any query that references aggregates like “sum”, “count” and others needs to be split into two phases.\nThe first phase is computing new columns. If we are to sort the data by “count(*)” column using “ORDER BY” statement or filter rows based on it using “HAVING” statement, we need to know the values of this column. Once the values of columns like “count(*)” are known, we can proceed with the rest of the query execution.\nNote that if the query does not reference aggregate functions in “HAVING” or “ORDER BY”, but still uses them in “SELECT”, we can make use of a trick. Since we do not need the values of aggregate functions until the very end, we can compute them partially and merge results just before we are about to return them to the user.\nThe key difference between the two approaches is when we compute aggregate functions: in advance, to perform some additional computations on them later; or on the fly, to iteratively build results the user needs.\nFirst, we will dive into building results on the fly — a technique we call “scatter-gather aggregations.” We will then build on top of that to introduce “shuffling aggregations” capable of running extra computations like “HAVING” and “ORDER BY” on top of aggregate functions.\nScatter-gather aggregations\nAggregate queries without “HAVING” and “ORDER BY” can be executed in a fashion similar to filter queries. For filter queries, R2 SQL picks one node to be the coordinator in query execution. This node analyzes the query and consults R2 Data Catalog to figure out which Parquet row groups may contain data relevant to the query. Each Parquet row group represents a relatively small piece of work that a single compute node can handle. Coordinator node distributes the work across many worker nodes and collects results to return them to the user.\nIn order to execute aggregate queries, we follow all the same steps and distribute small pieces of work between worker nodes. However, this time instead of just filtering rows based on the predicate in the “WHERE” statement, worker nodes also compute pre-aggregates.\nPre-aggregates represent an intermediary state of an aggregation. This is an incomplete piece of data representing a partially computed aggregate function on a subset of data. Multiple pre-aggregates can be merged together to compute the final value of an aggregate function. Splitting aggregate functions into pre-aggregates allows us to horizontally scale computation of aggregation, making use of vast compute resources available in Cloudflare’s network.\nFor example, pre-aggregate for “count(*)” is simply a number representing the count of rows in a subset of data. Computing the final “count(*)” is as easy as adding these numbers together. Pre-aggregate for “avg(value)” consists of two numbers: “sum(value)” and “count(*)”. The value of “avg(value)” can then be computed by adding together all “sum(value)” values, adding together all “count(*)” values and finally dividing one number by the other.\nOnce worker nodes have finished computing the pre-aggregates, they stream results to the coordinator node. The coordinator node collects all results, computes final values of aggregate functions from pre-aggregates, and returns the result to the user.\nShuffling, beyond the limits of scatter-gather\nScatter-gather is highly efficient when the coordinator can compute the final result by merging small, partial states from workers. If you run a query like SELECT sum(sales) FROM orders, the coordinator receives a single number from each worker and adds them up. The memory footprint on the coordinator is negligible regardless of how much data resides in R2.\nHowever, this approach becomes inefficient when the query requires sorting or filtering based on the result of an aggregation. Consider this query, which finds the top two departments by sales volume:\nSELECT department, sum(sales)\nFROM sales\nGROUP BY department\nORDER BY sum(sales) DESC\nLIMIT 2\nCorrectly determining the global Top 2 requires knowing the total sales for every department across the entire dataset. Because the data is spread effectively at random across the underlying Parquet files, sales for a specific department are likely split across many different workers. A department might have low sales on every individual worker, excluding it from any local Top 2 list, yet have the highest sales volume globally when summed together.\nThe diagram below illustrates how a scatter-gather approach would not work for this query. \"Dept A\" is the global sales leader, but because its sales are evenly spread across workers, it doesn’t make to some local Top 2 lists, and ends up being discarded by the coordinator.\nConsequently, when the query orders results by their global aggregation, the coordinator cannot rely on pre-filtered results from workers. It must request the total count for every department from every worker to calculate the global totals before it can sort them. If you are grouping by a high-cardinality column like IP addresses or User IDs, this forces the coordinator to ingest and merge millions of rows, creating a resource bottleneck on a single node.\nTo solve this, we need shuffling, a way to colocate data for specific groups before the final aggregation occurs.\nShuffling of aggregation data\nTo address the challenges of random data distribution, we introduce a shuffling stage. Instead of sending results to the coordinator, workers exchange data directly with each other to colocate rows based on their grouping key.\nThis routing relies on deterministic hash partitioning. When a worker processes a row, it hashes the GROUP BY column to identify the destination worker. Because this hash is deterministic, every worker in the cluster independently agrees on where to send specific data. If \"Engineering\" hashes to Worker 5, every worker knows to route \"Engineering\" rows to Worker 5. No central registry is required.\nThe diagram below illustrates this flow. Notice how \"Dept A\" starts on Workers 1, 2 and 3. Because the hash function maps \"Dept A\" to Worker 1, all workers route those rows to that same destination.\nShuffling aggregates produces the correct results. However, this all-to-all exchange creates a timing dependency. If Worker 1 begins calculating the final total for \"Dept A\" before Worker 3 has finished sending its share of the data, the result will be incomplete.\nTo address this, we enforce a strict synchronization barrier. The coordinator tracks the progress of the entire cluster while workers buffer their outgoing data and flush it via gRPC streams to their peers. Only when every worker confirms that it has finished processing its input files and flushing its shuffle buffers does the coordinator issue the command to proceed. This barrier guarantees that when the next stage begins, the dataset on each worker is complete and accurate.\nLocal finalization\nOnce the synchronization barrier is lifted, every worker holds the complete dataset for its assigned groups. Worker 1 now has 100% of the sales records for \"Dept A\" and can calculate the final total with certainty.\nThis allows us to push computational logic like filtering and sorting down to the worker rather than burdening the coordinator. For example, if the query includes HAVING count(*) > 5, the worker can filter out groups that do not meet this criteria immediately after aggregation.\nAt the end of this stage, each worker produces a sorted, finalized stream of results for the groups it owns.\nThe streaming merge\nThe final piece of the puzzle is the coordinator. In the scatter-gather model, the coordinator was responsible for the expensive task of aggregating and sorting the entire dataset. In the shuffling model, its role changes.\nBecause the workers have already computed the final aggregates and sorted them locally, the coordinator only needs to perform a k-way merge. It opens a stream to every worker and reads the results row by row. It compares the current row from each worker, picks the \"winner\" based on the sort order, and adds it to the query results that will be sent to the user.\nThis approach is particularly powerful for LIMIT queries. If a user asks for the top 10 departments, the coordinator merges the streams until it has found the top 10 items and then immediately stops processing. It does not need to load or merge the millions of remaining rows, allowing for greater scale of operation without over-consumption of compute resources.\nA powerful engine for processing massive datasets\nWith the addition of aggregations, R2 SQL transforms from a tool great for filtering data into a powerful engine capable of data processing on massive datasets. This is made possible by implementing distributed execution strategies like scatter-gather and shuffling, where we are able to push the compute to where the data lives, using the scale of Cloudflare’s global compute and network. \nWhether you are generating reports, monitoring high-volume logs for anomalies, or simply trying to spot trends in your data, you can now easily do it all within Cloudflare’s Developer Platform without the overhead of managing complex OLAP infrastructure or moving data out of R2.\nTry it now\nSupport for aggregations in R2 SQL is available today. We are excited to see how you use these new functions with data in R2 Data Catalog.\n\nGet Started: Check out our documentation for examples and syntax guides on running aggregation queries.\n\nJoin the Conversation: If you have questions, feedback, or want to share what you’re building, join us in the Cloudflare Developer Discord.","dc:creator":"Jérôme Schneider","content":" Cloudflare’s R2 SQL, a distributed query engine, now supports aggregations. Explore how we built distributed GROUP BY execution, using scatter-gather and shuffling strategies to run analytics directly over your R2 Data Catalog. ","contentSnippet":"Cloudflare’s R2 SQL, a distributed query engine, now supports aggregations. Explore how we built distributed GROUP BY execution, using scatter-gather and shuffling strategies to run analytics directly over your R2 Data Catalog.","guid":"1qWQCp4QfhsZAs27s7fEc0","categories":["R2","Data","Edge Computing","Rust","Serverless","SQL"],"isoDate":"2025-12-18T14:00:00.000Z"}],"feedUrl":"https://blog.cloudflare.com/","image":{"link":"https://blog.cloudflare.com","url":"https://blog.cloudflare.com/favicon.png","title":"The Cloudflare Blog"},"paginationLinks":{"self":"https://blog.cloudflare.com/"},"title":" The Cloudflare Blog ","description":" Get the latest news on how products at Cloudflare are built, technologies used, and join the teams helping to build a better Internet. ","link":"https://blog.cloudflare.com","language":"en-us","lastBuildDate":"Tue, 17 Feb 2026 02:53:48 GMT"}}